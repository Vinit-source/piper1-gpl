{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title-header"
      },
      "source": [
        "# Piper TTS Fine-Tuning: Indian English Voice (Google Drive Edition)\n",
        "\n",
        "This notebook provides a complete pipeline for fine-tuning the Piper TTS model\n",
        "using a dataset stored in Google Drive.\n",
        "\n",
        "**Compatible with:** Google Colab\n",
        "\n",
        "## Overview\n",
        "1. Colab Anti-Disconnect\n",
        "2. Check GPU Type\n",
        "3. Mount Google Drive\n",
        "4. Install Software\n",
        "5. Download Dataset from Drive (audio + transcript)\n",
        "6. Training\n",
        "7. Save Model and Config to Drive\n",
        "\n",
        "---\n",
        "\n",
        "**Repository:** https://github.com/Vinit-source/piper1-gpl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "first-steps-header"
      },
      "source": [
        "# ðŸ”§ **1. First Steps** ðŸ”§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "anti-disconnect",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7c5f2a72-307d-4dab-85f4-7d08faefd78d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}\n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown ## **Google Colab Anti-Disconnect.** ðŸ”Œ\n",
        "#@markdown ---\n",
        "#@markdown #### Avoid automatic disconnection. Still, it will disconnect after **6 to 12 hours**.\n",
        "\n",
        "import IPython\n",
        "js_code = '''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "display(IPython.display.Javascript(js_code))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "check-gpu",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "585f2585-f5a0-44b3-83fe-6eea851387d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec  3 18:56:58 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## **Check GPU Type.** ðŸ‘ï¸\n",
        "#@markdown ---\n",
        "#@markdown #### A higher capable GPU can lead to faster training speeds. By default, you will have a **Tesla T4**.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mount-gdrive",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ba08590-9590-49fc-f10d-19069e70e82b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **Mount Google Drive.** ðŸ“‚\n",
        "#@markdown ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-header"
      },
      "source": [
        "# ðŸ“¦ **2. Install Software** ðŸ“¦"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "install-software",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4390ebf3-cb4e-4378-e7ba-1c601df5bd88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package libpcaudio0:amd64.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libpcaudio0_1.1-6build2_amd64.deb ...\n",
            "Unpacking libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../1-libsonic0_0.2.0-11build1_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Selecting previously unselected package espeak-ng-data:amd64.\n",
            "Preparing to unpack .../2-espeak-ng-data_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package libespeak-ng1:amd64.\n",
            "Preparing to unpack .../3-libespeak-ng1_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package espeak-ng.\n",
            "Preparing to unpack .../4-espeak-ng_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package ninja-build.\n",
            "Preparing to unpack .../5-ninja-build_1.10.1-1_amd64.deb ...\n",
            "Unpacking ninja-build (1.10.1-1) ...\n",
            "Setting up ninja-build (1.10.1-1) ...\n",
            "Setting up libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Setting up espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Cloning into '/content/piper1-gpl'...\n",
            "remote: Enumerating objects: 846, done.\u001b[K\n",
            "remote: Counting objects: 100% (320/320), done.\u001b[K\n",
            "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
            "remote: Total 846 (delta 250), reused 220 (delta 212), pack-reused 526 (from 2)\u001b[K\n",
            "Receiving objects: 100% (846/846), 7.34 MiB | 15.53 MiB/s, done.\n",
            "Resolving deltas: 100% (476/476), done.\n",
            "/content/piper1-gpl\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m241.8/241.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m846.0/846.0 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m760.5/760.5 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for piper-tts (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Compiling /content/piper1-gpl/src/piper/train/vits/monotonic_align/core.pyx because it changed.\n",
            "[1/1] Cythonizing /content/piper1-gpl/src/piper/train/vits/monotonic_align/core.pyx\n",
            "/usr/local/lib/python3.12/dist-packages/Cython/Compiler/Main.py:381: FutureWarning: Cython directive 'language_level' not set, using '3str' for now (Py3). This has changed from earlier releases! File: /content/piper1-gpl/src/piper/train/vits/monotonic_align/core.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "performance hint: core.pyx:7:5: Exception check on 'maximum_path_each' will always require the GIL to be acquired.\n",
            "Possible solutions:\n",
            "\t1. Declare 'maximum_path_each' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n",
            "\t2. Use an 'int' return type on 'maximum_path_each' to allow an error code to be returned.\n",
            "performance hint: core.pyx:38:6: Exception check on 'maximum_path_c' will always require the GIL to be acquired.\n",
            "Possible solutions:\n",
            "\t1. Declare 'maximum_path_c' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n",
            "\t2. Use an 'int' return type on 'maximum_path_c' to allow an error code to be returned.\n",
            "performance hint: core.pyx:42:21: Exception check after calling 'maximum_path_each' will always require the GIL to be acquired.\n",
            "Possible solutions:\n",
            "\t1. Declare 'maximum_path_each' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n",
            "\t2. Use an 'int' return type on 'maximum_path_each' to allow an error code to be returned.\n",
            "\n",
            "âœ… Piper TTS installed successfully!\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **Install Piper and Dependencies.** ðŸ“¦\n",
        "#@markdown ---\n",
        "#@markdown This cell installs Piper TTS and all necessary dependencies for training.\n",
        "\n",
        "import os\n",
        "\n",
        "# Install system dependencies\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq espeak-ng build-essential cmake ninja-build\n",
        "\n",
        "# Clone the piper1-gpl repository with updated dependencies\n",
        "PIPER_REPO_URL = \"https://github.com/Vinit-source/piper1-gpl.git\"\n",
        "PIPER_BRANCH = \"copilot/fine-tune-text-to-speech-model\"\n",
        "PIPER_DIR = \"/content/piper1-gpl\"\n",
        "\n",
        "if not os.path.exists(PIPER_DIR):\n",
        "    !git clone -b {PIPER_BRANCH} {PIPER_REPO_URL} {PIPER_DIR}\n",
        "else:\n",
        "    print(f\"Repository already exists at {PIPER_DIR}\")\n",
        "\n",
        "# Install piper with training dependencies\n",
        "%cd {PIPER_DIR}\n",
        "!pip install -e .[train] -q\n",
        "\n",
        "# Build monotonic alignment module (required for VITS)\n",
        "!bash build_monotonic_align.sh\n",
        "\n",
        "print(\"\\nâœ… Piper TTS installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset-header"
      },
      "source": [
        "# ðŸ“¥ **3. Download Dataset from Drive** ðŸ“¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dataset-config",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7223a1-3ae7-429a-fa28-6b1507ae143b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset path: /content/drive/MyDrive/Piper-POC-Training/\n",
            "Output path: /content/drive/MyDrive/Piper-POC-Training/output\n",
            "Local dataset dir: /content/dataset\n",
            "Local output dir: /content/output/en_IN-spicor-medium\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **Configure Dataset Path.** ðŸ“\n",
        "#@markdown ---\n",
        "#@markdown ### Set the path to your dataset in Google Drive.\n",
        "#@markdown\n",
        "#@markdown **Expected folder structure:**\n",
        "#@markdown ```\n",
        "#@markdown your_dataset_folder/\n",
        "#@markdown â”œâ”€â”€ wavs/           # Audio files (WAV format, 22050Hz recommended)\n",
        "#@markdown â”‚   â”œâ”€â”€ 1.wav\n",
        "#@markdown â”‚   â”œâ”€â”€ 2.wav\n",
        "#@markdown â”‚   â””â”€â”€ ...\n",
        "#@markdown â””â”€â”€ metadata.csv    # Transcript file (format: wavs/filename.wav|text)\n",
        "#@markdown ```\n",
        "#@markdown\n",
        "#@markdown **Transcript format (single speaker):**\n",
        "#@markdown ```\n",
        "#@markdown wavs/1.wav|This is the text for audio 1.\n",
        "#@markdown wavs/2.wav|This is the text for audio 2.\n",
        "#@markdown ```\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ### Path to your dataset folder in Google Drive:\n",
        "gdrive_dataset_path = \"/content/drive/MyDrive/Piper-POC-Training/\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Model name (used for output files):\n",
        "model_name = \"en_IN-spicor-medium\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Output path in Google Drive (for saving checkpoints and models):\n",
        "gdrive_output_path = \"/content/drive/MyDrive/Piper-POC-Training/output\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Validate paths\n",
        "gdrive_dataset_path = gdrive_dataset_path.strip()\n",
        "gdrive_output_path = gdrive_output_path.strip()\n",
        "\n",
        "# Create local working directories\n",
        "LOCAL_DATASET_DIR = \"/content/dataset\"\n",
        "LOCAL_WAVS_DIR = f\"{LOCAL_DATASET_DIR}/wavs\"\n",
        "LOCAL_CACHE_DIR = \"/content/audio_cache\"\n",
        "LOCAL_OUTPUT_DIR = f\"/content/output/{model_name}\"\n",
        "\n",
        "os.makedirs(LOCAL_DATASET_DIR, exist_ok=True)\n",
        "os.makedirs(LOCAL_WAVS_DIR, exist_ok=True)\n",
        "os.makedirs(LOCAL_CACHE_DIR, exist_ok=True)\n",
        "os.makedirs(LOCAL_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Create output directory in Google Drive\n",
        "os.makedirs(gdrive_output_path, exist_ok=True)\n",
        "\n",
        "print(f\"Dataset path: {gdrive_dataset_path}\")\n",
        "print(f\"Output path: {gdrive_output_path}\")\n",
        "print(f\"Local dataset dir: {LOCAL_DATASET_DIR}\")\n",
        "print(f\"Local output dir: {LOCAL_OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "extract-dataset",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd4827de-1b6e-4db7-8498-fc1435f76b55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found wavs.zip, extracting...\n",
            "Found transcript file: transcripts.txt\n",
            "\n",
            "âœ… Dataset loaded: 10 audio files, total duration: 0:01:24\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **Extract Dataset.** ðŸ“¥\n",
        "#@markdown ---\n",
        "#@markdown This cell copies your dataset from Google Drive to the local environment.\n",
        "#@markdown\n",
        "#@markdown **Important:** Audio files must be in WAV format (22050Hz, 16-bit, mono recommended).\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "import wave\n",
        "import zipfile\n",
        "import datetime\n",
        "import shutil\n",
        "\n",
        "def get_dataset_duration(wav_path):\n",
        "    \"\"\"Calculate total duration of all WAV files in a directory.\"\"\"\n",
        "    totalduration = 0\n",
        "    totalcount = 0\n",
        "    for file_name in os.listdir(wav_path):\n",
        "        if not file_name.endswith(\".wav\"):\n",
        "            continue\n",
        "        full_path = os.path.join(wav_path, file_name)\n",
        "        try:\n",
        "            with wave.open(full_path, \"rb\") as wave_file:\n",
        "                frames = wave_file.getnframes()\n",
        "                rate = wave_file.getframerate()\n",
        "                duration = frames / float(rate)\n",
        "                totalduration += duration\n",
        "                totalcount += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping bad file {file_name}: {e}\")\n",
        "            continue\n",
        "    duration_str = str(datetime.timedelta(seconds=round(totalduration, 0)))\n",
        "    return totalcount, duration_str\n",
        "\n",
        "def cleanup_ghost_files(dataset_path):\n",
        "    \"\"\"Delete macOS ghost files (._*) from the dataset.\"\"\"\n",
        "    count = 0\n",
        "    for filename in os.listdir(dataset_path):\n",
        "        if filename.startswith(\"._\"):\n",
        "            file_path = os.path.join(dataset_path, filename)\n",
        "            os.remove(file_path)\n",
        "            count += 1\n",
        "    if count > 0:\n",
        "        print(f\"Cleaned up {count} macOS artifact files.\")\n",
        "\n",
        "# Check if dataset exists in Google Drive\n",
        "if not os.path.exists(gdrive_dataset_path):\n",
        "    raise Exception(f\"Dataset folder not found: {gdrive_dataset_path}\")\n",
        "\n",
        "# Check for wavs folder or zip file\n",
        "gdrive_wavs_path = os.path.join(gdrive_dataset_path, \"wavs\")\n",
        "gdrive_wavs_zip = os.path.join(gdrive_dataset_path, \"wavs.zip\")\n",
        "\n",
        "if os.path.exists(gdrive_wavs_zip):\n",
        "    print(\"Found wavs.zip, extracting...\")\n",
        "    with zipfile.ZipFile(gdrive_wavs_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(LOCAL_DATASET_DIR)\n",
        "    # Handle nested wavs folder if present\n",
        "    if os.path.exists(f\"{LOCAL_DATASET_DIR}/wavs/wavs\"):\n",
        "        for f in os.listdir(f\"{LOCAL_DATASET_DIR}/wavs/wavs\"):\n",
        "            shutil.move(f\"{LOCAL_DATASET_DIR}/wavs/wavs/{f}\", f\"{LOCAL_WAVS_DIR}/{f}\")\n",
        "elif os.path.exists(gdrive_wavs_path):\n",
        "    print(\"Found wavs folder, copying...\")\n",
        "    !cp -r \"{gdrive_wavs_path}\"/* \"{LOCAL_WAVS_DIR}/\"\n",
        "else:\n",
        "    raise Exception(f\"No 'wavs' folder or 'wavs.zip' found in {gdrive_dataset_path}\")\n",
        "\n",
        "# Clean up macOS ghost files\n",
        "cleanup_ghost_files(LOCAL_WAVS_DIR)\n",
        "\n",
        "# Copy transcript file\n",
        "transcript_files = [\"metadata.csv\", \"transcripts.txt\", \"transcript.txt\", \"metadata.txt\"]\n",
        "transcript_found = False\n",
        "\n",
        "for tf in transcript_files:\n",
        "    transcript_file_path = os.path.join(LOCAL_WAVS_DIR, tf)\n",
        "    if os.path.exists(transcript_file_path):\n",
        "        print(f\"Found transcript file: {tf}\")\n",
        "        shutil.copy(transcript_file_path, f\"{LOCAL_DATASET_DIR}/metadata.csv\")\n",
        "        transcript_found = True\n",
        "        break\n",
        "\n",
        "if not transcript_found:\n",
        "    raise Exception(f\"No transcript file found in {gdrive_dataset_path}. Expected one of: {transcript_files}\")\n",
        "\n",
        "# Get dataset statistics\n",
        "audio_count, dataset_dur = get_dataset_duration(LOCAL_WAVS_DIR)\n",
        "print(f\"\\nâœ… Dataset loaded: {audio_count} audio files, total duration: {dataset_dur}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training-header"
      },
      "source": [
        "# ðŸ¤– **4. Training** ðŸ¤–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "training-config",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d634e110-56f9-4e60-b7cc-f547b281f727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language: en\n",
            "Sample rate: 22050\n",
            "Batch size: 16\n",
            "Max epochs: 10\n",
            "Validation split: 0.05\n",
            "Use pretrained: True\n",
            "Resume training: False\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **Training Configuration.** âš™ï¸\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ### Language of the dataset:\n",
        "language = \"en\" #@param [\"ar\", \"ca\", \"cs\", \"da\", \"de\", \"el\", \"en\", \"en-us\", \"es\", \"es-419\", \"fi\", \"fr\", \"hu\", \"is\", \"it\", \"ka\", \"kk\", \"lb\", \"ne\", \"nl\", \"nb\", \"pl\", \"pt-br\", \"pt-pt\", \"ro\", \"ru\", \"sr\", \"sv\", \"sw\", \"tr\", \"uk\", \"vi\", \"zh\"]\n",
        "\n",
        "#@markdown ### Sample rate (22050 recommended for medium/high quality):\n",
        "sample_rate = 22050 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Is this a single speaker dataset?\n",
        "single_speaker = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Training Parameters:\n",
        "\n",
        "#@markdown ### Batch size (reduce if out of memory):\n",
        "batch_size = 16 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Maximum epochs:\n",
        "max_epochs = 10 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Validation split (0.0 to disable validation):\n",
        "validation_split = 0.05 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown ### Number of test examples for audio generation:\n",
        "num_test_examples = 3 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Fine-tuning Options:\n",
        "\n",
        "#@markdown ### Download and use a pretrained checkpoint for fine-tuning?\n",
        "use_pretrained = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### Resume from existing checkpoint in output folder?\n",
        "resume_training = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "# Calculate num_speakers\n",
        "num_speakers = 1 if single_speaker else 0  # 0 will be calculated from metadata\n",
        "\n",
        "print(f\"Language: {language}\")\n",
        "print(f\"Sample rate: {sample_rate}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Max epochs: {max_epochs}\")\n",
        "print(f\"Validation split: {validation_split}\")\n",
        "print(f\"Use pretrained: {use_pretrained}\")\n",
        "print(f\"Resume training: {resume_training}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-pretrained"
      },
      "outputs": [],
      "source": [
        "#@markdown # **Download Pretrained Checkpoint.** ðŸ“¥\n",
        "#@markdown ---\n",
        "#@markdown Downloads a pretrained checkpoint from Hugging Face for fine-tuning.\n",
        "\n",
        "import os\n",
        "\n",
        "PRETRAINED_CKPT_PATH = \"/content/pretrained.ckpt\"\n",
        "\n",
        "if use_pretrained and not resume_training:\n",
        "    if not os.path.exists(PRETRAINED_CKPT_PATH):\n",
        "        print(\"Downloading pretrained checkpoint from Hugging Face...\")\n",
        "\n",
        "        # Install huggingface_hub if needed\n",
        "        !pip install -q huggingface_hub\n",
        "\n",
        "        from huggingface_hub import hf_hub_download\n",
        "\n",
        "        # Download the en_US hfc_female medium checkpoint\n",
        "        downloaded_path = hf_hub_download(\n",
        "            repo_id=\"rhasspy/piper-checkpoints\",\n",
        "            filename=\"en/en_US/hfc_female/medium/epoch=2868-step=1575188.ckpt\",\n",
        "            repo_type=\"dataset\",\n",
        "            local_dir=\"/content/checkpoints\",\n",
        "        )\n",
        "\n",
        "        # Also download the config for reference\n",
        "        config_path = hf_hub_download(\n",
        "            repo_id=\"rhasspy/piper-checkpoints\",\n",
        "            filename=\"en/en_US/hfc_female/medium/config.json\",\n",
        "            repo_type=\"dataset\",\n",
        "            local_dir=\"/content/checkpoints\",\n",
        "        )\n",
        "\n",
        "        # Copy to expected location\n",
        "        !cp \"{downloaded_path}\" \"{PRETRAINED_CKPT_PATH}\"\n",
        "\n",
        "        print(f\"\\nâœ… Pretrained checkpoint downloaded to: {PRETRAINED_CKPT_PATH}\")\n",
        "    else:\n",
        "        print(f\"Pretrained checkpoint already exists: {PRETRAINED_CKPT_PATH}\")\n",
        "elif resume_training:\n",
        "    print(\"Resume training mode - will look for existing checkpoint in output folder.\")\n",
        "else:\n",
        "    print(\"Training from scratch (no pretrained checkpoint).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tensorboard"
      },
      "outputs": [],
      "source": [
        "#@markdown # **Launch TensorBoard.** ðŸ“Š\n",
        "#@markdown ---\n",
        "#@markdown TensorBoard allows you to monitor training progress in real-time.\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {LOCAL_OUTPUT_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train-model"
      },
      "outputs": [],
      "source": [
        "#@markdown # **Start Training.** ðŸ‹ï¸â€â™‚ï¸\n",
        "#@markdown ---\n",
        "#@markdown This cell runs the Piper training process.\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "\n",
        "# Change to piper directory\n",
        "%cd /content/piper1-gpl\n",
        "\n",
        "# Build the training command\n",
        "csv_path = f\"{LOCAL_DATASET_DIR}/metadata.csv\"\n",
        "config_path = f\"{LOCAL_OUTPUT_DIR}/{model_name}.json\"\n",
        "cache_dir = LOCAL_CACHE_DIR\n",
        "audio_dir = LOCAL_WAVS_DIR\n",
        "\n",
        "# Determine checkpoint path\n",
        "ckpt_path_arg = \"\"\n",
        "\n",
        "if resume_training:\n",
        "    # Look for existing checkpoint\n",
        "    checkpoints = glob.glob(f\"{LOCAL_OUTPUT_DIR}/lightning_logs/**/checkpoints/last.ckpt\", recursive=True)\n",
        "    if checkpoints:\n",
        "        # Sort by version number to get the latest\n",
        "        latest_ckpt = sorted(checkpoints, key=lambda x: int(re.findall(r'version_(\\d+)', x)[0]) if re.findall(r'version_(\\d+)', x) else 0)[-1]\n",
        "        print(f\"Resuming from checkpoint: {latest_ckpt}\")\n",
        "        ckpt_path_arg = f\"--ckpt_path \\\"{latest_ckpt}\\\"\"\n",
        "    else:\n",
        "        print(\"No existing checkpoint found. Starting fresh.\")\n",
        "        if use_pretrained and os.path.exists(PRETRAINED_CKPT_PATH):\n",
        "            ckpt_path_arg = f\"--ckpt_path \\\"{PRETRAINED_CKPT_PATH}\\\"\"\n",
        "elif use_pretrained and os.path.exists(PRETRAINED_CKPT_PATH):\n",
        "    print(f\"Fine-tuning from pretrained checkpoint: {PRETRAINED_CKPT_PATH}\")\n",
        "    ckpt_path_arg = f\"--ckpt_path \\\"{PRETRAINED_CKPT_PATH}\\\"\"\n",
        "\n",
        "# Build the training command using the piper.train module\n",
        "train_cmd = f\"\"\"\n",
        "python -m piper.train \\\n",
        "    --data.csv_path \"{csv_path}\" \\\n",
        "    --data.cache_dir \"{cache_dir}\" \\\n",
        "    --data.audio_dir \"{audio_dir}\" \\\n",
        "    --data.espeak_voice \"{language}\" \\\n",
        "    --data.config_path \"{config_path}\" \\\n",
        "    --data.voice_name \"{model_name}\" \\\n",
        "    --data.sample_rate {sample_rate} \\\n",
        "    --data.batch_size {batch_size} \\\n",
        "    --data.validation_split {validation_split} \\\n",
        "    --data.num_test_examples {num_test_examples} \\\n",
        "    --model.sample_rate {sample_rate} \\\n",
        "    --model.num_speakers {num_speakers} \\\n",
        "    --trainer.max_epochs {max_epochs} \\\n",
        "    --trainer.accelerator gpu \\\n",
        "    --trainer.devices 1 \\\n",
        "    --trainer.precision 32 \\\n",
        "    --trainer.default_root_dir \"{LOCAL_OUTPUT_DIR}\" \\\n",
        "    --trainer.callbacks+=ModelCheckpoint \\\n",
        "    --trainer.callbacks.dirpath \"{LOCAL_OUTPUT_DIR}/checkpoints\" \\\n",
        "    --trainer.callbacks.filename \"piper-{{epoch:04d}}-{{step:08d}}\" \\\n",
        "    --trainer.callbacks.save_top_k 3 \\\n",
        "    --trainer.callbacks.save_last true \\\n",
        "    --trainer.callbacks.every_n_epochs 1 \\\n",
        "    {ckpt_path_arg}\n",
        "\"\"\"\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"Training command:\\n{train_cmd}\")\n",
        "!{train_cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save-header"
      },
      "source": [
        "# ðŸ’¾ **5. Save Model to Drive** ðŸ’¾"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export-onnx"
      },
      "outputs": [],
      "source": [
        "#@markdown # **Export Model to ONNX.** ðŸ“¦\n",
        "#@markdown ---\n",
        "#@markdown This cell exports the trained model to ONNX format for inference.\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Find the best checkpoint\n",
        "checkpoint_dir = f\"{LOCAL_OUTPUT_DIR}/checkpoints\"\n",
        "last_ckpt = f\"{checkpoint_dir}/last.ckpt\"\n",
        "\n",
        "if os.path.exists(last_ckpt):\n",
        "    best_checkpoint = last_ckpt\n",
        "    print(f\"Using last checkpoint: {best_checkpoint}\")\n",
        "else:\n",
        "    # Find any checkpoint\n",
        "    all_checkpoints = glob.glob(f\"{LOCAL_OUTPUT_DIR}/**/checkpoints/*.ckpt\", recursive=True)\n",
        "    if all_checkpoints:\n",
        "        best_checkpoint = sorted(all_checkpoints)[-1]\n",
        "        print(f\"Using checkpoint: {best_checkpoint}\")\n",
        "    else:\n",
        "        raise Exception(f\"No checkpoints found in {LOCAL_OUTPUT_DIR}\")\n",
        "\n",
        "# Export to ONNX\n",
        "onnx_output_path = f\"{LOCAL_OUTPUT_DIR}/{model_name}.onnx\"\n",
        "\n",
        "%cd /content/piper1-gpl\n",
        "!python -m piper.train.export_onnx \\\n",
        "    --checkpoint \"{best_checkpoint}\" \\\n",
        "    --output-file \"{onnx_output_path}\"\n",
        "\n",
        "print(f\"\\nâœ… Model exported to: {onnx_output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save-to-drive"
      },
      "outputs": [],
      "source": [
        "#@markdown # **Save Model and Config to Google Drive.** ðŸ’¾\n",
        "#@markdown ---\n",
        "#@markdown This cell copies the trained model and config files to Google Drive.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create model-specific output directory in Google Drive\n",
        "gdrive_model_dir = os.path.join(gdrive_output_path, model_name)\n",
        "os.makedirs(gdrive_model_dir, exist_ok=True)\n",
        "\n",
        "# Files to copy\n",
        "files_to_copy = [\n",
        "    (f\"{LOCAL_OUTPUT_DIR}/{model_name}.onnx\", f\"{gdrive_model_dir}/{model_name}.onnx\"),\n",
        "    (f\"{LOCAL_OUTPUT_DIR}/{model_name}.json\", f\"{gdrive_model_dir}/{model_name}.onnx.json\"),\n",
        "]\n",
        "\n",
        "# Also copy the last checkpoint\n",
        "last_ckpt = f\"{LOCAL_OUTPUT_DIR}/checkpoints/last.ckpt\"\n",
        "if os.path.exists(last_ckpt):\n",
        "    files_to_copy.append((last_ckpt, f\"{gdrive_model_dir}/last.ckpt\"))\n",
        "\n",
        "# Copy files\n",
        "for src, dst in files_to_copy:\n",
        "    if os.path.exists(src):\n",
        "        print(f\"Copying {src} -> {dst}\")\n",
        "        shutil.copy(src, dst)\n",
        "    else:\n",
        "        print(f\"Warning: {src} not found, skipping.\")\n",
        "\n",
        "print(f\"\\nâœ… Model saved to Google Drive: {gdrive_model_dir}\")\n",
        "print(\"\\nFiles saved:\")\n",
        "for f in os.listdir(gdrive_model_dir):\n",
        "    fpath = os.path.join(gdrive_model_dir, f)\n",
        "    size_mb = os.path.getsize(fpath) / (1024 * 1024)\n",
        "    print(f\"  - {f} ({size_mb:.2f} MB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test-header"
      },
      "source": [
        "# ðŸŽ§ **6. Test the Model (Optional)** ðŸŽ§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test-model"
      },
      "outputs": [],
      "source": [
        "#@markdown # **Test the Exported Model.** ðŸŽ§\n",
        "#@markdown ---\n",
        "#@markdown This cell tests the exported ONNX model by generating speech.\n",
        "\n",
        "#@markdown ### Text to synthesize:\n",
        "test_text = \"Hello, this is a test of the fine-tuned Piper text to speech model.\" #@param {type:\"string\"}\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "try:\n",
        "    import onnxruntime as ort\n",
        "except ImportError:\n",
        "    !pip install -q onnxruntime\n",
        "    import onnxruntime as ort\n",
        "\n",
        "# Load model and config\n",
        "onnx_path = f\"{LOCAL_OUTPUT_DIR}/{model_name}.onnx\"\n",
        "config_json_path = f\"{LOCAL_OUTPUT_DIR}/{model_name}.json\"\n",
        "\n",
        "if not os.path.exists(onnx_path):\n",
        "    raise Exception(f\"ONNX model not found: {onnx_path}\")\n",
        "\n",
        "if not os.path.exists(config_json_path):\n",
        "    raise Exception(f\"Config file not found: {config_json_path}\")\n",
        "\n",
        "# Load config\n",
        "with open(config_json_path, 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "phoneme_id_map = config.get('phoneme_id_map', {})\n",
        "model_sample_rate = config.get('audio', {}).get('sample_rate', 22050)\n",
        "\n",
        "# Create ONNX session\n",
        "session = ort.InferenceSession(onnx_path)\n",
        "\n",
        "# Simple phoneme-based synthesis (for testing)\n",
        "# In production, use espeak-ng for proper phonemization\n",
        "def text_to_phoneme_ids(text, phoneme_id_map):\n",
        "    \"\"\"Simple text to phoneme ID conversion for testing.\"\"\"\n",
        "    ids = []\n",
        "    for char in text.lower():\n",
        "        if char in phoneme_id_map:\n",
        "            value = phoneme_id_map[char]\n",
        "            # Handle both list and integer values\n",
        "            if isinstance(value, list):\n",
        "                ids.extend(value)\n",
        "            else:\n",
        "                ids.append(value)\n",
        "        elif char == ' ':\n",
        "            if ' ' in phoneme_id_map:\n",
        "                value = phoneme_id_map[' ']\n",
        "                if isinstance(value, list):\n",
        "                    ids.extend(value)\n",
        "                else:\n",
        "                    ids.append(value)\n",
        "    return ids if ids else [1, 2, 3, 4, 5]  # Fallback\n",
        "\n",
        "# Get phoneme IDs\n",
        "phoneme_ids = text_to_phoneme_ids(test_text, phoneme_id_map)\n",
        "\n",
        "# Prepare inputs\n",
        "input_array = np.array([phoneme_ids], dtype=np.int64)\n",
        "input_lengths = np.array([len(phoneme_ids)], dtype=np.int64)\n",
        "scales = np.array([0.667, 1.0, 0.8], dtype=np.float32)  # noise_scale, length_scale, noise_scale_w\n",
        "\n",
        "# Run inference\n",
        "inputs = {\n",
        "    'input': input_array,\n",
        "    'input_lengths': input_lengths,\n",
        "    'scales': scales,\n",
        "}\n",
        "\n",
        "output = session.run(None, inputs)\n",
        "audio = output[0].squeeze()\n",
        "\n",
        "print(f\"Generated audio shape: {audio.shape}\")\n",
        "print(f\"Audio duration: {len(audio) / model_sample_rate:.2f} seconds\")\n",
        "\n",
        "# Play audio\n",
        "display(Audio(audio, rate=model_sample_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary-header"
      },
      "source": [
        "# ðŸ“‹ **Summary**\n",
        "\n",
        "This notebook provides a complete pipeline for fine-tuning Piper TTS with data from Google Drive:\n",
        "\n",
        "1. **Anti-Disconnect** - Keep Colab session alive\n",
        "2. **GPU Check** - Verify GPU availability\n",
        "3. **Mount Drive** - Access your Google Drive\n",
        "4. **Install Software** - Install Piper and dependencies from the piper1-gpl repository\n",
        "5. **Dataset Loading** - Copy dataset from Drive to local storage\n",
        "6. **Training** - Fine-tune the model with configurable parameters\n",
        "7. **Export & Save** - Export to ONNX and save to Google Drive\n",
        "\n",
        "### Output Files\n",
        "\n",
        "After training, you'll find these files in your Google Drive output folder:\n",
        "- `{model_name}.onnx` - The trained model in ONNX format\n",
        "- `{model_name}.onnx.json` - Model configuration file\n",
        "- `last.ckpt` - Latest training checkpoint (for resuming training)\n",
        "\n",
        "### Using the Model\n",
        "\n",
        "To use the trained model with Piper:\n",
        "```bash\n",
        "echo \"Hello world\" | piper --model {model_name}.onnx --output_file output.wav\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}