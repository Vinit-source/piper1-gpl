"""PyTorch Lightning module."""

import ast
import logging
import operator
from functools import reduce
from itertools import chain
from typing import Optional

import lightning as L
import torch
from torch import autocast
from torch.nn import functional as F
from torch.optim.lr_scheduler import (
    CosineAnnealingWarmRestarts,
    ExponentialLR,
    ReduceLROnPlateau,
)

from .commons import slice_segments
from .dataset import Batch
from .losses import discriminator_loss, feature_loss, generator_loss, kl_loss
from .mel_processing import mel_spectrogram_torch, spec_to_mel_torch
from .models import MultiPeriodDiscriminator, SynthesizerTrn

_LOGGER = logging.getLogger(__name__)


class VitsModel(L.LightningModule):
    def __init__(
        self,
        batch_size: int = 32,
        sample_rate: int = 22050,
        num_symbols: int = 256,
        num_speakers: int = 1,
        # audio
        resblock="2",
        resblock_kernel_sizes=(3, 5, 7),
        resblock_dilation_sizes=(
            (1, 2),
            (2, 6),
            (3, 12),
        ),
        upsample_rates=(8, 8, 4),
        upsample_initial_channel=256,
        upsample_kernel_sizes=(16, 16, 8),
        # mel
        filter_length: int = 1024,
        hop_length: int = 256,
        win_length: int = 1024,
        mel_channels: int = 80,
        mel_fmin: float = 0.0,
        mel_fmax: Optional[float] = None,
        # model
        inter_channels: int = 192,
        hidden_channels: int = 192,
        filter_channels: int = 768,
        n_heads: int = 2,
        n_layers: int = 6,
        kernel_size: int = 3,
        p_dropout: float = 0.1,
        n_layers_q: int = 3,
        use_spectral_norm: bool = False,
        gin_channels: int = 0,
        use_sdp: bool = True,
        segment_size: int = 8192,
        # training
        learning_rate: float = 2e-4,
        learning_rate_d: float = 1e-4,
        betas: tuple[float, float] = (0.8, 0.99),
        betas_d: tuple[float, float] = (0.5, 0.9),
        eps: float = 1e-9,
        lr_decay: float = 0.999875,
        lr_decay_d: float = 0.9999,
        init_lr_ratio: float = 1.0,
        warmup_epochs: int = 0,
        c_mel: int = 45,
        c_kl: float = 1.0,
        grad_clip: Optional[float] = 1.0,
        # Fine-tuning mode: layer-wise learning rates
        finetune_mode: bool = False,
        encoder_lr_scale: float = 0.1,  # Text encoder: 10% of base LR (protect)
        flow_lr_scale: float = 0.5,     # Flow/alignment: 50% of base LR (moderate)
        decoder_lr_scale: float = 1.0,  # Decoder: full LR (adapt fast)
        # Dynamic learning rate scheduler
        lr_scheduler_type: str = "exponential",  # "exponential", "plateau", "cosine_warmup_restart"
        lr_patience: int = 5,           # Epochs to wait before reducing (plateau)
        lr_factor: float = 0.5,         # Factor to reduce LR by (plateau)
        lr_min: float = 1e-7,           # Minimum learning rate floor
        lr_cooldown: int = 2,           # Cooldown epochs after reduction (plateau)
        cosine_t0: int = 10,            # Restart period for cosine annealing
        cosine_t_mult: int = 2,         # Multiplier for restart period
        # unused
        dataset: object = None,
        **kwargs,
    ):
        super().__init__()
        self.save_hyperparameters()

        if isinstance(self.hparams.resblock_kernel_sizes, str):
            self.hparams.resblock_kernel_sizes = ast.literal_eval(
                self.hparams.resblock_kernel_sizes
            )

        if isinstance(self.hparams.resblock_dilation_sizes, str):
            self.hparams.resblock_dilation_sizes = ast.literal_eval(
                self.hparams.resblock_dilation_sizes
            )

        if isinstance(self.hparams.upsample_rates, str):
            self.hparams.upsample_rates = ast.literal_eval(self.hparams.upsample_rates)

        if isinstance(self.hparams.upsample_kernel_sizes, str):
            self.hparams.upsample_kernel_sizes = ast.literal_eval(
                self.hparams.upsample_kernel_sizes
            )

        if isinstance(self.hparams.betas, str):
            self.hparams.betas = ast.literal_eval(self.hparams.betas)

        expected_hop_length = reduce(operator.mul, self.hparams.upsample_rates, 1)
        if expected_hop_length != hop_length:
            raise ValueError("Upsample rates do not match hop length")

        # Need to use manual optimization because we have multiple optimizers
        self.automatic_optimization = False

        self.batch_size = batch_size

        if (self.hparams.num_speakers > 1) and (self.hparams.gin_channels <= 0):
            # Default gin_channels for multi-speaker model
            self.hparams.gin_channels = 512

        # Set up models
        self.model_g = SynthesizerTrn(
            n_vocab=num_symbols,
            spec_channels=self.hparams.filter_length // 2 + 1,
            segment_size=self.hparams.segment_size // self.hparams.hop_length,
            inter_channels=self.hparams.inter_channels,
            hidden_channels=self.hparams.hidden_channels,
            filter_channels=self.hparams.filter_channels,
            n_heads=self.hparams.n_heads,
            n_layers=self.hparams.n_layers,
            kernel_size=self.hparams.kernel_size,
            p_dropout=self.hparams.p_dropout,
            resblock=self.hparams.resblock,
            resblock_kernel_sizes=self.hparams.resblock_kernel_sizes,
            resblock_dilation_sizes=self.hparams.resblock_dilation_sizes,
            upsample_rates=self.hparams.upsample_rates,
            upsample_initial_channel=self.hparams.upsample_initial_channel,
            upsample_kernel_sizes=self.hparams.upsample_kernel_sizes,
            n_speakers=self.hparams.num_speakers,
            gin_channels=self.hparams.gin_channels,
            use_sdp=self.hparams.use_sdp,
        )
        self.model_d = MultiPeriodDiscriminator(
            use_spectral_norm=self.hparams.use_spectral_norm
        )

    def on_load_checkpoint(self, checkpoint: dict) -> None:
        """Handle backward compatibility for optimizer states."""
        if "optimizer_states" not in checkpoint or not checkpoint["optimizer_states"]:
            return

        # Check generator optimizer (index 0)
        opt_g_state = checkpoint["optimizer_states"][0]
        if not opt_g_state:
            return

        # Determine expected number of parameter groups
        # If finetune_mode is True, we have 3 groups (encoder, flow, decoder)
        # If finetune_mode is False, we have 1 group
        expected_g_groups = 3 if self.hparams.finetune_mode else 1
        
        # Check actual groups in checkpoint
        ckpt_g_groups = len(opt_g_state["param_groups"])
        
        if ckpt_g_groups != expected_g_groups:
            _LOGGER.warning(
                "Optimizer parameter groups mismatch: checkpoint has %d, model expects %d. "
                "Discarding optimizer states from checkpoint to avoid loading error. "
                "This is expected if you are switching between finetune_mode=True/False "
                "or loading an old checkpoint with new settings.",
                ckpt_g_groups,
                expected_g_groups,
            )
            # clear optimizer states to force fresh initialization
            checkpoint["optimizer_states"] = []

    def forward(self, text, text_lengths, scales, sid=None):
        noise_scale = scales[0]
        length_scale = scales[1]
        noise_scale_w = scales[2]
        audio, *_ = self.model_g.infer(
            text,
            text_lengths,
            noise_scale=noise_scale,
            length_scale=length_scale,
            noise_scale_w=noise_scale_w,
            sid=sid,
        )

        return audio

    def _compute_loss(self, batch: Batch):
        # g step
        x, x_lengths, y, _, spec, spec_lengths, speaker_ids = (
            batch.phoneme_ids,
            batch.phoneme_lengths,
            batch.audios,
            batch.audio_lengths,
            batch.spectrograms,
            batch.spectrogram_lengths,
            batch.speaker_ids if batch.speaker_ids is not None else None,
        )
        (
            y_hat,
            l_length,
            _attn,
            ids_slice,
            _x_mask,
            z_mask,
            (_z, z_p, m_p, logs_p, _m_q, logs_q),
        ) = self.model_g(x, x_lengths, spec, spec_lengths, speaker_ids)

        mel = spec_to_mel_torch(
            spec,
            self.hparams.filter_length,
            self.hparams.mel_channels,
            self.hparams.sample_rate,
            self.hparams.mel_fmin,
            self.hparams.mel_fmax,
        )
        y_mel = slice_segments(
            mel,
            ids_slice,
            self.hparams.segment_size // self.hparams.hop_length,
        )
        y_hat_mel = mel_spectrogram_torch(
            y_hat.squeeze(1),
            self.hparams.filter_length,
            self.hparams.mel_channels,
            self.hparams.sample_rate,
            self.hparams.hop_length,
            self.hparams.win_length,
            self.hparams.mel_fmin,
            self.hparams.mel_fmax,
        )
        y = slice_segments(
            y,
            ids_slice * self.hparams.hop_length,
            self.hparams.segment_size,
        )  # slice

        # Trim to avoid padding issues
        y_hat = y_hat[..., : y.shape[-1]]

        _y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = self.model_d(y, y_hat)

        with autocast(self.device.type, enabled=False):
            # Generator loss
            loss_dur = torch.sum(l_length.float())
            loss_mel = F.l1_loss(y_mel, y_hat_mel) * self.hparams.c_mel
            loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * self.hparams.c_kl

            loss_fm = feature_loss(fmap_r, fmap_g)
            loss_gen, _losses_gen = generator_loss(y_d_hat_g)
            loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl

        # d step
        y_d_hat_r, y_d_hat_g, _, _ = self.model_d(y, y_hat.detach())

        with autocast(self.device.type, enabled=False):
            # Discriminator
            loss_disc, _losses_disc_r, _losses_disc_g = discriminator_loss(
                y_d_hat_r, y_d_hat_g
            )
            loss_disc_all = loss_disc

        return loss_gen_all, loss_disc_all

    def training_step(self, batch: Batch, batch_idx: int):
        opt_g, opt_d = self.optimizers()
        loss_g, loss_d = self._compute_loss(batch)

        self.log("loss_g", loss_g, batch_size=self.batch_size)
        opt_g.zero_grad()
        self.manual_backward(loss_g, retain_graph=True)
        opt_g.step()

        self.log("loss_d", loss_d, batch_size=self.batch_size)
        opt_d.zero_grad()
        self.manual_backward(loss_d)
        opt_d.step()

    def validation_step(self, batch: Batch, batch_idx: int):
        loss_g, _loss_d = self._compute_loss(batch)
        val_loss = loss_g  # only generator loss matters
        self.log("val_loss", val_loss, batch_size=self.batch_size)
        return val_loss

    def on_validation_end(self) -> None:
        # Manual scheduler stepping for manual optimization
        if not self.trainer.sanity_checking:
            val_loss = self.trainer.callback_metrics.get("val_loss")
            schedulers = self.lr_schedulers()
            if schedulers is not None:
                if not isinstance(schedulers, list):
                    schedulers = [schedulers]
                
                for scheduler in schedulers:
                    if isinstance(scheduler, ReduceLROnPlateau):
                        if val_loss is not None:
                            scheduler.step(val_loss)
                    else:
                        scheduler.step()

        # Generate audio examples after validation, but not during sanity check
        if self.trainer.sanity_checking:
            return super().on_validation_end()

        if (
            getattr(self, "logger", None)
            and hasattr(self.logger, "experiment")
            and hasattr(self.logger.experiment, "add_audio")
        ):
            # Generate audio examples
            # Requires tensorboard
            for utt_idx, test_utt in enumerate(self.trainer.datamodule.test_dataset):
                text = test_utt.phoneme_ids.unsqueeze(0).to(self.device)
                text_lengths = torch.LongTensor([len(test_utt.phoneme_ids)]).to(
                    self.device
                )
                scales = [0.667, 1.0, 0.8]
                sid = (
                    test_utt.speaker_id.to(self.device)
                    if test_utt.speaker_id is not None
                    else None
                )
                test_audio = self(text, text_lengths, scales, sid=sid).detach()

                # Scale to make louder in [-1, 1]
                test_audio = test_audio * (1.0 / max(0.01, abs(test_audio).max()))

                tag = test_utt.text or str(utt_idx)
                self.logger.experiment.add_audio(
                    tag, test_audio, sample_rate=self.hparams.sample_rate
                )

        return super().on_validation_end()

    def _get_generator_param_groups(self):
        """Get parameter groups with layer-wise learning rates for fine-tuning.
        
        Separates the generator into three groups:
        - encoder: Text encoder (enc_p) and embeddings - protected with low LR
        - flow: Flow and duration predictor - moderate LR for prosody adaptation  
        - decoder: HiFi-GAN decoder (dec) and posterior encoder (enc_q) - high LR for voice adaptation
        """
        base_lr = self.hparams.learning_rate
        
        # Encoder components (protect phoneme knowledge)
        encoder_params = list(chain(
            self.model_g.enc_p.parameters(),
            self.model_g.emb.parameters() if hasattr(self.model_g, 'emb') else [],
        ))
        
        # Flow components (moderate adaptation for prosody)
        flow_params = list(chain(
            self.model_g.flow.parameters(),
            self.model_g.dp.parameters(),  # duration predictor
        ))
        
        # Decoder components (fast adaptation for voice characteristics)
        decoder_params = list(chain(
            self.model_g.dec.parameters(),
            self.model_g.enc_q.parameters(),  # posterior encoder
        ))
        
        # Add speaker embedding if multi-speaker
        if hasattr(self.model_g, 'emb_g') and self.model_g.emb_g is not None:
            decoder_params.extend(self.model_g.emb_g.parameters())
        
        param_groups = [
            {
                "params": encoder_params,
                "lr": base_lr * self.hparams.encoder_lr_scale,
                "name": "encoder",
            },
            {
                "params": flow_params,
                "lr": base_lr * self.hparams.flow_lr_scale,
                "name": "flow",
            },
            {
                "params": decoder_params,
                "lr": base_lr * self.hparams.decoder_lr_scale,
                "name": "decoder",
            },
        ]
        
        _LOGGER.info(
            "Fine-tuning mode: encoder_lr=%.2e, flow_lr=%.2e, decoder_lr=%.2e",
            base_lr * self.hparams.encoder_lr_scale,
            base_lr * self.hparams.flow_lr_scale,
            base_lr * self.hparams.decoder_lr_scale,
        )
        
        return param_groups

    def _create_scheduler(self, optimizer, is_discriminator=False):
        """Create learning rate scheduler based on scheduler type."""
        scheduler_type = self.hparams.lr_scheduler_type
        
        if scheduler_type == "exponential":
            # Original behavior - continuous exponential decay
            gamma = self.hparams.lr_decay_d if is_discriminator else self.hparams.lr_decay
            scheduler = ExponentialLR(optimizer, gamma=gamma)
            return scheduler
        
        elif scheduler_type == "plateau":
            # Reduce LR when validation loss plateaus - good for escaping local maxima
            scheduler = ReduceLROnPlateau(
                optimizer,
                mode="min",
                factor=self.hparams.lr_factor,
                patience=self.hparams.lr_patience,
                min_lr=self.hparams.lr_min,
                cooldown=self.hparams.lr_cooldown,
                verbose=True,
            )
            return scheduler
        
        elif scheduler_type == "cosine_warmup_restart":
            # Cosine annealing with warm restarts - periodic LR increases to escape local maxima
            scheduler = CosineAnnealingWarmRestarts(
                optimizer,
                T_0=self.hparams.cosine_t0,
                T_mult=self.hparams.cosine_t_mult,
                eta_min=self.hparams.lr_min,
            )
            return scheduler
        
        else:
            raise ValueError(
                f"Unknown lr_scheduler_type: {scheduler_type}. "
                "Choose from: 'exponential', 'plateau', 'cosine_warmup_restart'"
            )

    def configure_optimizers(self):
        # Generator optimizer with optional layer-wise learning rates
        if self.hparams.finetune_mode:
            param_groups_g = self._get_generator_param_groups()
        else:
            param_groups_g = [
                {"params": self.model_g.parameters(), "lr": self.hparams.learning_rate}
            ]
        
        opt_g = torch.optim.AdamW(
            param_groups_g,
            betas=self.hparams.betas,
            eps=self.hparams.eps,
        )
        
        # Discriminator optimizer (full learning rate)
        opt_d = torch.optim.AdamW(
            self.model_d.parameters(),
            lr=self.hparams.learning_rate_d,
            betas=self.hparams.betas_d,
            eps=self.hparams.eps,
        )
        
        optimizers = [opt_g, opt_d]
        
        # Create schedulers based on scheduler type
        scheduler_g = self._create_scheduler(opt_g, is_discriminator=False)
        scheduler_d = self._create_scheduler(opt_d, is_discriminator=True)
        schedulers = [scheduler_g, scheduler_d]
        
        _LOGGER.info(
            "Using %s learning rate scheduler (finetune_mode=%s)",
            self.hparams.lr_scheduler_type,
            self.hparams.finetune_mode,
        )
        
        return optimizers, schedulers
