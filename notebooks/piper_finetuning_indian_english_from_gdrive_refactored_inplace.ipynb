{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title-header"
      },
      "source": [
        "# Piper TTS Fine-Tuning: Indian English Voice\n",
        "\n",
        "This notebook provides a complete pipeline for fine-tuning the Piper TTS model.\n",
        "\n",
        "**Supports two modes:**\n",
        "- **COLAB mode (`COLAB = True`)**: Uses Google Drive for data storage (Proof of Concept)\n",
        "- **Production mode (`COLAB = False`)**: Uses AWS S3 for data storage (Production-ready)\n",
        "\n",
        "**Compatible with:** Google Colab, AWS SageMaker\n",
        "\n",
        "## Overview\n",
        "1. Environment Configuration (COLAB/AWS mode selection)\n",
        "2. Environment Setup & GPU Check\n",
        "3. Install Software Dependencies\n",
        "4. Data ETL (Extract, Transform, Load)\n",
        "5. Training Configuration & Execution\n",
        "6. Model Export (ONNX)\n",
        "7. Save & Test Model\n",
        "8. Troubleshooting (Optional)\n",
        "\n",
        "---\n",
        "\n",
        "**Repository:** https://github.com/Vinit-source/piper1-gpl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "first-steps-header"
      },
      "source": [
        "# ğŸ”§ **1. Environment Configuration** ğŸ”§\n",
        "\n",
        "Set the `COLAB` constant to select between:\n",
        "- `COLAB = True`: Google Drive mode (Proof of Concept)\n",
        "- `COLAB = False`: AWS S3 mode (Production)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "anti-disconnect",
        "outputId": "be091762-935f-48b5-dc12-adf105a88350"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\nfunction ClickConnect(){\nconsole.log(\"Working\");\ndocument.querySelector(\"colab-toolbar-button#connect\").click()\n}\nsetInterval(ClickConnect,60000)\n",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ENVIRONMENT MODE SELECTION\n",
        "# =============================================================================\n",
        "# Set COLAB = True for Google Drive mode (Proof of Concept)\n",
        "# Set COLAB = False for AWS S3 mode (Production)\n",
        "\n",
        "COLAB: bool = True  # Toggle between Colab (Google Drive) and AWS (S3) mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION DATACLASS\n",
        "# =============================================================================\n",
        "# Centralized configuration for all pipeline parameters.\n",
        "# All required fields must be set - no fallback values are used.\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "import os\n",
        "import sys\n",
        "\n",
        "@dataclass\n",
        "class PiperConfig:\n",
        "    \"\"\"\n",
        "    Configuration for Piper TTS fine-tuning pipeline.\n",
        "    \n",
        "    Attributes:\n",
        "        colab_mode: Whether running in Colab (Google Drive) or AWS (S3) mode\n",
        "        \n",
        "    Google Drive Configuration (COLAB = True):\n",
        "        gdrive_dataset_path: Path to dataset folder in Google Drive\n",
        "        gdrive_output_path: Path to save outputs in Google Drive\n",
        "        \n",
        "    AWS S3 Configuration (COLAB = False):\n",
        "        s3_bucket: S3 bucket name\n",
        "        s3_dataset_prefix: S3 prefix for dataset\n",
        "        s3_checkpoint_prefix: S3 prefix for checkpoints\n",
        "        aws_region: AWS region\n",
        "        aws_access_key_id: AWS access key (optional, uses IAM role if not set)\n",
        "        aws_secret_access_key: AWS secret key (optional, uses IAM role if not set)\n",
        "        \n",
        "    Model Configuration:\n",
        "        model_name: Name for the output model\n",
        "        espeak_voice: eSpeak voice for phonemization\n",
        "        sample_rate: Audio sample rate in Hz\n",
        "        \n",
        "    Training Configuration:\n",
        "        batch_size: Training batch size\n",
        "        max_epochs: Maximum training epochs\n",
        "        validation_split: Fraction of data for validation\n",
        "        num_test_examples: Number of test examples for audio generation\n",
        "        learning_rate: Learning rate for fine-tuning\n",
        "        precision: Training precision (e.g., \"16-mixed\", \"32\")\n",
        "        checkpoint_epochs: Save checkpoint every N epochs\n",
        "        device: Training device (\"cpu\" or \"gpu\")\n",
        "        use_pretrained: Whether to use pretrained checkpoint\n",
        "        resume_training: Whether to resume from existing checkpoint\n",
        "    \"\"\"\n",
        "    \n",
        "    # Environment mode\n",
        "    colab_mode: bool = True\n",
        "    \n",
        "    # ==================== GOOGLE DRIVE CONFIGURATION ====================\n",
        "    # Required when COLAB = True\n",
        "    gdrive_dataset_path: str = \"<GDRIVE_DATASET_PATH>\"  # e.g., \"/content/drive/MyDrive/Piper-POC-Training/\"\n",
        "    gdrive_output_path: str = \"<GDRIVE_OUTPUT_PATH>\"    # e.g., \"/content/drive/MyDrive/Piper-POC-Training/output\"\n",
        "    \n",
        "    # ==================== AWS S3 CONFIGURATION ====================\n",
        "    # Required when COLAB = False\n",
        "    s3_bucket: str = \"<YOUR_S3_BUCKET_NAME>\"           # e.g., \"my-tts-training-bucket\"\n",
        "    s3_dataset_prefix: str = \"<S3_DATASET_PATH>\"       # e.g., \"datasets/spicor\"\n",
        "    s3_checkpoint_prefix: str = \"<S3_CHECKPOINT_PATH>\" # e.g., \"checkpoints/piper\"\n",
        "    aws_region: str = \"<AWS_REGION>\"                   # e.g., \"us-east-1\"\n",
        "    aws_access_key_id: Optional[str] = None            # Leave None to use IAM role\n",
        "    aws_secret_access_key: Optional[str] = None        # Leave None to use IAM role\n",
        "    \n",
        "    # ==================== MODEL CONFIGURATION ====================\n",
        "    model_name: str = \"<MODEL_NAME>\"                   # e.g., \"en_IN-spicor-medium\"\n",
        "    espeak_voice: str = \"en-us\"                        # eSpeak voice for phonemization\n",
        "    sample_rate: int = 22050                           # Audio sample rate\n",
        "    num_speakers: int = 1                              # Number of speakers (1 for single speaker)\n",
        "    \n",
        "    # ==================== TRAINING CONFIGURATION ====================\n",
        "    batch_size: int = 8                                # Reduce if out of memory\n",
        "    max_epochs: int = 4000                             # Maximum training epochs\n",
        "    validation_split: float = 0.0                      # Validation split (0.0 to disable)\n",
        "    num_test_examples: int = 0                         # Test examples for audio generation\n",
        "    learning_rate: float = 1e-4                        # Learning rate\n",
        "    precision: str = \"16-mixed\"                        # Training precision\n",
        "    checkpoint_epochs: int = 200                       # Save checkpoint every N epochs\n",
        "    device: str = \"gpu\"                                # \"cpu\" or \"gpu\"\n",
        "    use_pretrained: bool = True                        # Use pretrained checkpoint\n",
        "    resume_training: bool = False                      # Resume from existing checkpoint\n",
        "    \n",
        "    # ==================== LOCAL PATHS (Auto-configured) ====================\n",
        "    base_dir: str = field(default=\"\")\n",
        "    local_dataset_dir: str = field(default=\"\")\n",
        "    local_wavs_dir: str = field(default=\"\")\n",
        "    local_cache_dir: str = field(default=\"\")\n",
        "    local_output_dir: str = field(default=\"\")\n",
        "    piper_dir: str = field(default=\"\")\n",
        "    \n",
        "    # ==================== HUGGING FACE CHECKPOINT ====================\n",
        "    hf_checkpoint_repo: str = \"rhasspy/piper-checkpoints\"\n",
        "    hf_checkpoint_path: str = \"en/en_US/ljspeech/high/ljspeech-2000.ckpt\"\n",
        "    hf_config_path: str = \"en/en_US/ljspeech/high/config.json\"\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        \"\"\"Initialize derived paths based on environment mode.\"\"\"\n",
        "        if self.colab_mode:\n",
        "            self.base_dir = \"/content\"\n",
        "            self.piper_dir = \"/content/piper1-gpl\"\n",
        "        else:\n",
        "            self.base_dir = \"./piper_training\"\n",
        "            self.piper_dir = \"./piper1-gpl\"\n",
        "        \n",
        "        self.local_dataset_dir = f\"{self.base_dir}/dataset\"\n",
        "        self.local_wavs_dir = f\"{self.local_dataset_dir}/wavs\"\n",
        "        self.local_cache_dir = f\"{self.base_dir}/audio_cache\"\n",
        "        self.local_output_dir = f\"{self.base_dir}/output/{self.model_name}\"\n",
        "    \n",
        "    def validate(self) -> None:\n",
        "        \"\"\"\n",
        "        Validate configuration and raise errors for missing required fields.\n",
        "        No fallback values - all placeholders must be replaced.\n",
        "        \"\"\"\n",
        "        errors = []\n",
        "        \n",
        "        # Validate model name\n",
        "        if self.model_name == \"<MODEL_NAME>\" or not self.model_name:\n",
        "            errors.append(\"model_name: Must be set (e.g., 'en_IN-spicor-medium')\")\n",
        "        \n",
        "        if self.colab_mode:\n",
        "            # Validate Google Drive configuration\n",
        "            if self.gdrive_dataset_path == \"<GDRIVE_DATASET_PATH>\" or not self.gdrive_dataset_path:\n",
        "                errors.append(\"gdrive_dataset_path: Must be set for COLAB mode\")\n",
        "            if self.gdrive_output_path == \"<GDRIVE_OUTPUT_PATH>\" or not self.gdrive_output_path:\n",
        "                errors.append(\"gdrive_output_path: Must be set for COLAB mode\")\n",
        "        else:\n",
        "            # Validate AWS S3 configuration\n",
        "            if self.s3_bucket == \"<YOUR_S3_BUCKET_NAME>\" or not self.s3_bucket:\n",
        "                errors.append(\"s3_bucket: Must be set for AWS mode\")\n",
        "            if self.s3_dataset_prefix == \"<S3_DATASET_PATH>\" or not self.s3_dataset_prefix:\n",
        "                errors.append(\"s3_dataset_prefix: Must be set for AWS mode\")\n",
        "            if self.s3_checkpoint_prefix == \"<S3_CHECKPOINT_PATH>\" or not self.s3_checkpoint_prefix:\n",
        "                errors.append(\"s3_checkpoint_prefix: Must be set for AWS mode\")\n",
        "            if self.aws_region == \"<AWS_REGION>\" or not self.aws_region:\n",
        "                errors.append(\"aws_region: Must be set for AWS mode\")\n",
        "        \n",
        "        # Validate training parameters\n",
        "        if self.batch_size <= 0:\n",
        "            errors.append(\"batch_size: Must be positive\")\n",
        "        if self.max_epochs <= 0:\n",
        "            errors.append(\"max_epochs: Must be positive\")\n",
        "        if self.sample_rate <= 0:\n",
        "            errors.append(\"sample_rate: Must be positive\")\n",
        "        if self.device not in (\"cpu\", \"gpu\"):\n",
        "            errors.append(\"device: Must be 'cpu' or 'gpu'\")\n",
        "        \n",
        "        if errors:\n",
        "            error_msg = \"Configuration validation failed:\\n\" + \"\\n\".join(f\"  - {e}\" for e in errors)\n",
        "            raise ValueError(error_msg)\n",
        "    \n",
        "    def create_directories(self) -> None:\n",
        "        \"\"\"Create all required local directories.\"\"\"\n",
        "        dirs = [\n",
        "            self.local_dataset_dir,\n",
        "            self.local_wavs_dir,\n",
        "            self.local_cache_dir,\n",
        "            self.local_output_dir,\n",
        "        ]\n",
        "        for d in dirs:\n",
        "            Path(d).mkdir(parents=True, exist_ok=True)\n",
        "            print(f\"Created directory: {d}\")\n",
        "        \n",
        "        # Create Google Drive output directory if in COLAB mode\n",
        "        if self.colab_mode:\n",
        "            Path(self.gdrive_output_path).mkdir(parents=True, exist_ok=True)\n",
        "            print(f\"Created Google Drive output directory: {self.gdrive_output_path}\")\n",
        "\n",
        "\n",
        "def detect_runtime_environment() -> str:\n",
        "    \"\"\"\n",
        "    Detect the current runtime environment.\n",
        "    \n",
        "    Returns:\n",
        "        str: 'colab', 'sagemaker', or 'local'\n",
        "    \"\"\"\n",
        "    if 'google.colab' in sys.modules:\n",
        "        return 'colab'\n",
        "    elif os.environ.get('SM_CURRENT_HOST'):\n",
        "        return 'sagemaker'\n",
        "    return 'local'\n",
        "\n",
        "\n",
        "# Display detected environment\n",
        "RUNTIME_ENV = detect_runtime_environment()\n",
        "print(f\"Detected runtime environment: {RUNTIME_ENV}\")\n",
        "print(f\"Mode: {'Google Colab (Google Drive)' if COLAB else 'Production (AWS S3)'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# INITIALIZE CONFIGURATION\n",
        "# =============================================================================\n",
        "# Update the configuration values below before running the pipeline.\n",
        "# All placeholder values (e.g., \"<MODEL_NAME>\") must be replaced.\n",
        "\n",
        "config = PiperConfig(\n",
        "    colab_mode=COLAB,\n",
        "    \n",
        "    # ----- Google Drive Configuration (for COLAB = True) -----\n",
        "    gdrive_dataset_path=\"/content/drive/MyDrive/Piper-POC-Training/\",\n",
        "    gdrive_output_path=\"/content/drive/MyDrive/Piper-POC-Training/output\",\n",
        "    \n",
        "    # ----- AWS S3 Configuration (for COLAB = False) -----\n",
        "    s3_bucket=\"<YOUR_S3_BUCKET_NAME>\",\n",
        "    s3_dataset_prefix=\"<S3_DATASET_PATH>\",\n",
        "    s3_checkpoint_prefix=\"<S3_CHECKPOINT_PATH>\",\n",
        "    aws_region=\"<AWS_REGION>\",\n",
        "    aws_access_key_id=None,  # Set to None to use IAM role\n",
        "    aws_secret_access_key=None,\n",
        "    \n",
        "    # ----- Model Configuration -----\n",
        "    model_name=\"en_IN-spicor-medium\",\n",
        "    espeak_voice=\"en-us\",\n",
        "    sample_rate=22050,\n",
        "    num_speakers=1,  # 1 for single speaker\n",
        "    \n",
        "    # ----- Training Configuration -----\n",
        "    batch_size=8,\n",
        "    max_epochs=4000,\n",
        "    validation_split=0.0,\n",
        "    num_test_examples=0,\n",
        "    learning_rate=1e-4,\n",
        "    precision=\"16-mixed\",\n",
        "    checkpoint_epochs=200,\n",
        "    device=\"gpu\",\n",
        "    use_pretrained=True,\n",
        "    resume_training=False,\n",
        ")\n",
        "\n",
        "# Validate configuration - raises error if any required field is missing\n",
        "config.validate()\n",
        "\n",
        "# Create local directories\n",
        "config.create_directories()\n",
        "\n",
        "# Display configuration summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CONFIGURATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Mode: {'COLAB (Google Drive)' if config.colab_mode else 'AWS (S3)'}\")\n",
        "print(f\"Model name: {config.model_name}\")\n",
        "print(f\"Sample rate: {config.sample_rate}\")\n",
        "print(f\"Batch size: {config.batch_size}\")\n",
        "print(f\"Max epochs: {config.max_epochs}\")\n",
        "print(f\"Device: {config.device}\")\n",
        "print(f\"Use pretrained: {config.use_pretrained}\")\n",
        "print(f\"Local output: {config.local_output_dir}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "check-gpu",
        "outputId": "89872c44-317a-4915-bc6e-ed754564c1a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "# ğŸ–¥ï¸ **2. Environment Setup** ğŸ–¥ï¸\n",
        "\n",
        "Set up the runtime environment including GPU check and Google Drive mount (if in COLAB mode)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mount-gdrive",
        "outputId": "f2a8b4c3-fd63-4802-a46d-f8fa4296eb37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# GPU CHECK\n",
        "# =============================================================================\n",
        "# Check available GPU. A higher capable GPU leads to faster training speeds.\n",
        "# Default Colab GPU is Tesla T4.\n",
        "\n",
        "print(\"Checking GPU availability...\")\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    print(f\"\\nâœ… GPU available: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ No GPU detected. Training will be slow on CPU.\")\n",
        "    if config.device == \"gpu\":\n",
        "        raise RuntimeError(\"Configuration specifies 'gpu' but no GPU is available. Set config.device='cpu' or use a GPU-enabled runtime.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MOUNT GOOGLE DRIVE (COLAB MODE ONLY)\n",
        "# =============================================================================\n",
        "# Mount Google Drive to access dataset and save outputs.\n",
        "# This cell only runs in COLAB mode.\n",
        "\n",
        "if COLAB:\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"âœ… Google Drive mounted successfully.\")\n",
        "    \n",
        "    # Verify dataset path exists\n",
        "    if not os.path.exists(config.gdrive_dataset_path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Dataset path not found in Google Drive: {config.gdrive_dataset_path}\\n\"\n",
        "            \"Please verify the path exists and contains your dataset.\"\n",
        "        )\n",
        "    print(f\"âœ… Dataset path verified: {config.gdrive_dataset_path}\")\n",
        "else:\n",
        "    print(\"Skipping Google Drive mount (AWS mode).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COLAB ANTI-DISCONNECT (COLAB MODE ONLY)\n",
        "# =============================================================================\n",
        "# Prevents automatic disconnection in Google Colab.\n",
        "# Note: Colab will still disconnect after 6-12 hours regardless.\n",
        "\n",
        "if COLAB:\n",
        "    import IPython\n",
        "    js_code = '''\n",
        "    function ClickConnect(){\n",
        "        console.log(\"Anti-disconnect: clicking connect button\");\n",
        "        document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "    }\n",
        "    setInterval(ClickConnect, 60000)\n",
        "    '''\n",
        "    display(IPython.display.Javascript(js_code))\n",
        "    print(\"âœ… Anti-disconnect script activated.\")\n",
        "else:\n",
        "    print(\"Skipping anti-disconnect (not running in Colab).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-header"
      },
      "source": [
        "# ğŸ“¦ **3. Install Software Dependencies** ğŸ“¦\n",
        "\n",
        "Install Piper TTS and all required dependencies for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install-software",
        "outputId": "11591c66-bbae-4627-ed3b-ec9994be6d2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package libpcaudio0:amd64.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libpcaudio0_1.1-6build2_amd64.deb ...\n",
            "Unpacking libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../1-libsonic0_0.2.0-11build1_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Selecting previously unselected package espeak-ng-data:amd64.\n",
            "Preparing to unpack .../2-espeak-ng-data_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package libespeak-ng1:amd64.\n",
            "Preparing to unpack .../3-libespeak-ng1_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package espeak-ng.\n",
            "Preparing to unpack .../4-espeak-ng_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package libespeak-ng-dev:amd64.\n",
            "Preparing to unpack .../5-libespeak-ng-dev_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking libespeak-ng-dev:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package ninja-build.\n",
            "Preparing to unpack .../6-ninja-build_1.10.1-1_amd64.deb ...\n",
            "Unpacking ninja-build (1.10.1-1) ...\n",
            "Setting up ninja-build (1.10.1-1) ...\n",
            "Setting up libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Setting up espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libespeak-ng-dev:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "Cloning into '/content/piper1-gpl'...\n",
            "remote: Enumerating objects: 902, done.\u001b[K\n",
            "remote: Counting objects: 100% (322/322), done.\u001b[K\n",
            "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
            "remote: Total 902 (delta 250), reused 221 (delta 214), pack-reused 580 (from 3)\u001b[K\n",
            "Receiving objects: 100% (902/902), 10.43 MiB | 14.40 MiB/s, done.\n",
            "Resolving deltas: 100% (508/508), done.\n",
            "/content/piper1-gpl\n",
            "Switched to a new branch 'release0.3.1'\n",
            "Uninstalling previous Piper TTS installation...\n",
            "\u001b[33mWARNING: Skipping piper-tts as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mReinstalling Piper TTS with training dependencies...\n",
            "Obtaining file:///content/piper1-gpl\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnxruntime<2,>=1 (from piper-tts==1.3.0)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: torch<3,>=2 in /usr/local/lib/python3.12/dist-packages (from piper-tts==1.3.0) (2.9.0+cu126)\n",
            "Collecting lightning<3,>=2 (from piper-tts==1.3.0)\n",
            "  Downloading lightning-2.6.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard<3,>=2 in /usr/local/lib/python3.12/dist-packages (from piper-tts==1.3.0) (2.19.0)\n",
            "Collecting tensorboardX<3,>=2 (from piper-tts==1.3.0)\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting jsonargparse>=4.27.7 (from jsonargparse[signatures]>=4.27.7; extra == \"train\"->piper-tts==1.3.0)\n",
            "  Downloading jsonargparse-4.44.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pathvalidate<4,>=3 (from piper-tts==1.3.0)\n",
            "  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting onnx<2,>=1 (from piper-tts==1.3.0)\n",
            "  Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting pysilero-vad<3,>=2.1 (from piper-tts==1.3.0)\n",
            "  Downloading pysilero_vad-2.1.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: cython<4,>=3 in /usr/local/lib/python3.12/dist-packages (from piper-tts==1.3.0) (3.0.12)\n",
            "Requirement already satisfied: librosa<1 in /usr/local/lib/python3.12/dist-packages (from piper-tts==1.3.0) (0.11.0)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.12/dist-packages (from jsonargparse>=4.27.7->jsonargparse[signatures]>=4.27.7; extra == \"train\"->piper-tts==1.3.0) (6.0.3)\n",
            "Requirement already satisfied: docstring-parser>=0.17 in /usr/local/lib/python3.12/dist-packages (from jsonargparse[signatures]>=4.27.7; extra == \"train\"->piper-tts==1.3.0) (0.17.0)\n",
            "Collecting typeshed-client>=2.8.2 (from jsonargparse[signatures]>=4.27.7; extra == \"train\"->piper-tts==1.3.0)\n",
            "  Downloading typeshed_client-2.8.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.0) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.0) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.0) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.0) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.0) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.0) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.0) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.0) (1.1.2)\n",
            "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.0) (2025.3.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<3,>=2->piper-tts==1.3.0)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.12/dist-packages (from lightning<3,>=2->piper-tts==1.3.0) (25.0)\n",
            "Collecting torchmetrics<3.0,>0.7.0 (from lightning<3,>=2->piper-tts==1.3.0)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from lightning<3,>=2->piper-tts==1.3.0) (4.67.1)\n",
            "Collecting pytorch-lightning (from lightning<3,>=2->piper-tts==1.3.0)\n",
            "  Downloading pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx<2,>=1->piper-tts==1.3.0) (5.29.5)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx<2,>=1->piper-tts==1.3.0) (0.5.4)\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1->piper-tts==1.3.0)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1->piper-tts==1.3.0) (25.9.23)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1->piper-tts==1.3.0) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2->piper-tts==1.3.0) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2->piper-tts==1.3.0) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2->piper-tts==1.3.0) (3.10)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2->piper-tts==1.3.0) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2->piper-tts==1.3.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2->piper-tts==1.3.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2->piper-tts==1.3.0) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (3.20.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.0) (3.5.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.0) (3.13.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa<1->piper-tts==1.3.0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa<1->piper-tts==1.3.0) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa<1->piper-tts==1.3.0) (2.32.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa<1->piper-tts==1.3.0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa<1->piper-tts==1.3.0) (2.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime<2,>=1->piper-tts==1.3.0) (1.3.0)\n",
            "Requirement already satisfied: importlib_resources>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from typeshed-client>=2.8.2->jsonargparse[signatures]>=4.27.7; extra == \"train\"->piper-tts==1.3.0) (6.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard<3,>=2->piper-tts==1.3.0) (3.0.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1->piper-tts==1.3.0)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.0) (1.22.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa<1->piper-tts==1.3.0) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa<1->piper-tts==1.3.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa<1->piper-tts==1.3.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa<1->piper-tts==1.3.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa<1->piper-tts==1.3.0) (2025.11.12)\n",
            "Downloading jsonargparse-4.44.0-py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m241.8/241.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.6.0-py3-none-any.whl (845 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m846.0/846.0 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n",
            "Downloading pysilero_vad-2.1.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeshed_client-2.8.2-py3-none-any.whl (760 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m760.5/760.5 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: piper-tts\n",
            "  Building editable for piper-tts (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for piper-tts: filename=piper_tts-1.3.0-0.editable-py3-none-any.whl size=15435 sha256=917ba1545fa0f09032b8a92158271e77cfb653c8544591dd572416caa0163fac\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l7hiw_fe/wheels/22/38/4e/d9fd0ab8ba964eb8332fc89984f961285397f819ab17291146\n",
            "Successfully built piper-tts\n",
            "Installing collected packages: typeshed-client, tensorboardX, pathvalidate, lightning-utilities, jsonargparse, humanfriendly, onnx, coloredlogs, onnxruntime, torchmetrics, pysilero-vad, piper-tts, pytorch-lightning, lightning\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 jsonargparse-4.44.0 lightning-2.6.0 lightning-utilities-0.15.2 onnx-1.20.0 onnxruntime-1.23.2 pathvalidate-3.3.1 piper-tts-1.3.0 pysilero-vad-2.1.1 pytorch-lightning-2.6.0 tensorboardX-2.6.4 torchmetrics-1.8.2 typeshed-client-2.8.2\n",
            "Building monotonic alignment module...\n",
            "Compiling /content/piper1-gpl/src/piper/train/vits/monotonic_align/core.pyx because it changed.\n",
            "[1/1] Cythonizing /content/piper1-gpl/src/piper/train/vits/monotonic_align/core.pyx\n",
            "/usr/local/lib/python3.12/dist-packages/Cython/Compiler/Main.py:381: FutureWarning: Cython directive 'language_level' not set, using '3str' for now (Py3). This has changed from earlier releases! File: /content/piper1-gpl/src/piper/train/vits/monotonic_align/core.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "performance hint: core.pyx:7:5: Exception check on 'maximum_path_each' will always require the GIL to be acquired.\n",
            "Possible solutions:\n",
            "\t1. Declare 'maximum_path_each' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n",
            "\t2. Use an 'int' return type on 'maximum_path_each' to allow an error code to be returned.\n",
            "performance hint: core.pyx:38:6: Exception check on 'maximum_path_c' will always require the GIL to be acquired.\n",
            "Possible solutions:\n",
            "\t1. Declare 'maximum_path_c' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n",
            "\t2. Use an 'int' return type on 'maximum_path_c' to allow an error code to be returned.\n",
            "performance hint: core.pyx:42:21: Exception check after calling 'maximum_path_each' will always require the GIL to be acquired.\n",
            "Possible solutions:\n",
            "\t1. Declare 'maximum_path_each' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n",
            "\t2. Use an 'int' return type on 'maximum_path_each' to allow an error code to be returned.\n",
            "\n",
            "âœ… Piper TTS installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# INSTALL SYSTEM DEPENDENCIES\n",
        "# =============================================================================\n",
        "# Install required system packages for audio processing and building native extensions.\n",
        "\n",
        "print(\"Installing system dependencies...\")\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq espeak-ng build-essential cmake ninja-build libespeak-ng1 libespeak-ng-dev\n",
        "print(\"âœ… System dependencies installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y4uuHy_y_cCT",
        "outputId": "85131fae-67bf-4c1d-bf0b-1ab5a1e1f74b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/piper1-gpl\n",
            "Collecting scikit-build\n",
            "  Using cached scikit_build-0.18.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: distro in /usr/local/lib/python3.12/dist-packages (from scikit-build) (1.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from scikit-build) (25.0)\n",
            "Requirement already satisfied: setuptools>=42.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-build) (75.2.0)\n",
            "Requirement already satisfied: wheel>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from scikit-build) (0.45.1)\n",
            "Using cached scikit_build-0.18.1-py3-none-any.whl (85 kB)\n",
            "Installing collected packages: scikit-build\n",
            "Successfully installed scikit-build-0.18.1\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "-- Trying 'Ninja' generator\n",
            "--------------------------------\n",
            "---------------------------\n",
            "----------------------\n",
            "-----------------\n",
            "------------\n",
            "-------\n",
            "--\n",
            "Not searching for unused variables given on the command line.\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Configuring done (0.6s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/piper1-gpl/_cmake_test_compile/build\n",
            "--\n",
            "-------\n",
            "------------\n",
            "-----------------\n",
            "----------------------\n",
            "---------------------------\n",
            "--------------------------------\n",
            "-- Trying 'Ninja' generator - success\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Configuring Project\n",
            "  Working directory:\n",
            "    /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build\n",
            "  Command:\n",
            "    /usr/local/lib/python3.12/dist-packages/cmake/data/bin/cmake /content/piper1-gpl -G Ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-install/src/piper -DPYTHON_VERSION_STRING:STRING=3.12.12 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/usr/local/lib/python3.12/dist-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/usr/bin/python3 -DPYTHON_INCLUDE_DIR:PATH=/usr/include/python3.12 -DPYTHON_LIBRARY:PATH=/usr/lib/x86_64-linux-gnu/libpython3.12.so -DPython_EXECUTABLE:PATH=/usr/bin/python3 -DPython_ROOT_DIR:PATH=/usr -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/usr/include/python3.12 -DPython_NumPy_INCLUDE_DIRS:PATH=/usr/local/lib/python3.12/dist-packages/numpy/_core/include -DPython3_EXECUTABLE:PATH=/usr/bin/python3 -DPython3_ROOT_DIR:PATH=/usr -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/usr/include/python3.12 -DPython3_NumPy_INCLUDE_DIRS:PATH=/usr/local/lib/python3.12/dist-packages/numpy/_core/include -DCMAKE_BUILD_TYPE:STRING=Release\n",
            "\n",
            "Not searching for unused variables given on the command line.\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Python: /usr/include/python3.12 (found version \"3.12.12\") found components: Development.Module Development.SABIModule\n",
            "-- Configuring done (0.5s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build\n",
            "[2/12] Performing download step (git clone) for 'espeak_ng_external'\u001b[K\n",
            "Cloning into 'espeak_ng_external'...\n",
            "HEAD is now at 212928b3 Added some words in the ru_listx file. (#2150)\n",
            "[3/12] Performing disconnected update step for 'espeak_ng_external'\u001b[K\n",
            "-- Already at requested ref: 212928b394a96e8fd2096616bfd54e17845c48f6\n",
            "[5/12] Performing configure step for 'espeak_ng_external'\u001b[K\n",
            "CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Looking for mkstemp\n",
            "-- Looking for mkstemp - found\n",
            "-- Configuration:\n",
            "--   shared: OFF\n",
            "--   mbrola: OFF (MBROLA_BIN-NOTFOUND)\n",
            "--   libsonic: OFF (sonic /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/_deps/sonic-git-src)\n",
            "--   libpcaudio: OFF (PCAUDIO_LIB-NOTFOUND PCAUDIO_INC-NOTFOUND)\n",
            "--   klatt: OFF\n",
            "--   speech-player: OFF\n",
            "--   async: OFF\n",
            "-- Configuring done (2.5s)\n",
            "-- Generating done (0.1s)\n",
            "-- Build files have been written to: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build\n",
            "[6/12] Performing build step for 'espeak_ng_external'\u001b[K\n",
            "[1/167] Building C object src/ucd-tools/CMakeFiles/ucd.dir/src/case.c.o\n",
            "[2/167] Building C object CMakeFiles/sonic.dir/_deps/sonic-git-src/sonic.c.o\n",
            "[3/167] Building C object src/CMakeFiles/espeak-ng-bin.dir/espeak-ng.c.o\n",
            "[4/167] Building C object src/ucd-tools/CMakeFiles/ucd.dir/src/ctype.c.o\n",
            "[5/167] Building C object src/ucd-tools/CMakeFiles/ucd.dir/src/scripts.c.o\n",
            "[6/167] Building C object src/ucd-tools/CMakeFiles/ucd.dir/src/categories.c.o\n",
            "[7/167] Building C object src/ucd-tools/CMakeFiles/ucd.dir/src/tostring.c.o\n",
            "[8/167] Building C object src/ucd-tools/CMakeFiles/ucd.dir/src/proplist.c.o\n",
            "[9/167] Linking C static library src/ucd-tools/libucd.a\n",
            "[10/167] Building CXX object src/speechPlayer/CMakeFiles/speechPlayer.dir/src/speechPlayer.cpp.o\n",
            "[11/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/common.c.o\n",
            "[12/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/mnemonics.c.o\n",
            "[13/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/error.c.o\n",
            "[14/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/ieee80.c.o\n",
            "[15/167] Building CXX object src/speechPlayer/CMakeFiles/speechPlayer.dir/src/frame.cpp.o\n",
            "[16/167] Building CXX object src/speechPlayer/CMakeFiles/speechPlayer.dir/src/speechWaveGenerator.cpp.o\n",
            "[17/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/compiledict.c.o\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledict.c: In function â€˜espeak_ng_CompileDictionaryâ€™:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledict.c:1558:30: warning: â€˜rules.txtâ€™ directive writing 9 bytes into a region of size between 6 and 205 [-Wformat-overflow=]\n",
            " 1558 |         sprintf(fname_in, \"%srules.txt\", path);\n",
            "      |                              ^~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledict.c:1558:9: note: â€˜sprintfâ€™ output between 10 and 209 bytes into a destination of size 205\n",
            " 1558 |         sprintf(fname_in, \"%srules.txt\", path);\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "[18/167] Linking CXX static library src/speechPlayer/libspeechPlayer.a\n",
            "[19/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/encoding.c.o\n",
            "[20/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/compiledata.c.o\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c: In function â€˜LoadSpectâ€™:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:924:47: warning: â€˜%sâ€™ directive output may be truncated writing up to 199 bytes into a region of size 180 [-Wformat-truncation=]\n",
            "  924 |         snprintf(filename, sizeof(filename), \"%s/%s\", ctx->phsrc, path);\n",
            "      |                                               ^~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:924:9: note: â€˜snprintfâ€™ output 2 or more bytes (assuming 201) into a destination of size 180\n",
            "  924 |         snprintf(filename, sizeof(filename), \"%s/%s\", ctx->phsrc, path);\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c: In function â€˜CompilePhonemeFilesâ€™:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2278:42: warning: â€˜%sâ€™ directive writing up to 255 bytes into a region of size between 80 and 279 [-Wformat-overflow=]\n",
            " 2278 |                         sprintf(buf, \"%s/%s\", ctx->phsrc, ctx->item_string);\n",
            "      |                                          ^~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2278:25: note: â€˜sprintfâ€™ output between 2 and 456 bytes into a destination of size 280\n",
            " 2278 |                         sprintf(buf, \"%s/%s\", ctx->phsrc, ctx->item_string);\n",
            "      |                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c: In function â€˜espeak_ng_CompilePhonemeDataPathâ€™:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2368:27: warning: â€˜/phonemesâ€™ directive writing 9 bytes into a region of size between 1 and 200 [-Wformat-overflow=]\n",
            " 2368 |         sprintf(fname, \"%s/phonemes\", ctx->phsrc);\n",
            "      |                           ^~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2368:9: note: â€˜sprintfâ€™ output between 10 and 209 bytes into a destination of size 200\n",
            " 2368 |         sprintf(fname, \"%s/phonemes\", ctx->phsrc);\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2376:28: warning: â€˜%sâ€™ directive writing 17 bytes into a region of size between 0 and 199 [-Wformat-overflow=]\n",
            " 2376 |         sprintf(fname, \"%s/%s\", phdst, \"phondata-manifest\");\n",
            "      |                            ^~          ~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2376:9: note: â€˜sprintfâ€™ output between 19 and 218 bytes into a destination of size 200\n",
            " 2376 |         sprintf(fname, \"%s/%s\", phdst, \"phondata-manifest\");\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2394:28: warning: â€˜%sâ€™ directive writing 8 bytes into a region of size between 0 and 199 [-Wformat-overflow=]\n",
            " 2394 |         sprintf(fname, \"%s/%s\", phdst, \"phondata\");\n",
            "      |                            ^~          ~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2394:9: note: â€˜sprintfâ€™ output between 10 and 209 bytes into a destination of size 200\n",
            " 2394 |         sprintf(fname, \"%s/%s\", phdst, \"phondata\");\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2404:28: warning: â€˜%sâ€™ directive writing 9 bytes into a region of size between 0 and 199 [-Wformat-overflow=]\n",
            " 2404 |         sprintf(fname, \"%s/%s\", phdst, \"phonindex\");\n",
            "      |                            ^~          ~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2404:9: note: â€˜sprintfâ€™ output between 11 and 210 bytes into a destination of size 200\n",
            " 2404 |         sprintf(fname, \"%s/%s\", phdst, \"phonindex\");\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2415:28: warning: â€˜%sâ€™ directive writing 7 bytes into a region of size between 0 and 199 [-Wformat-overflow=]\n",
            " 2415 |         sprintf(fname, \"%s/%s\", phdst, \"phontab\");\n",
            "      |                            ^~          ~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2415:9: note: â€˜sprintfâ€™ output between 9 and 208 bytes into a destination of size 200\n",
            " 2415 |         sprintf(fname, \"%s/%s\", phdst, \"phontab\");\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2427:27: warning: â€˜/compile_prog_logâ€™ directive writing 17 bytes into a region of size between 1 and 200 [-Wformat-overflow=]\n",
            " 2427 |         sprintf(fname, \"%s/compile_prog_log\", ctx->phsrc);\n",
            "      |                           ^~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2427:9: note: â€˜sprintfâ€™ output between 18 and 217 bytes into a destination of size 200\n",
            " 2427 |         sprintf(fname, \"%s/compile_prog_log\", ctx->phsrc);\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "[21/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/langopts.c.o\n",
            "[22/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/intonation.c.o\n",
            "[23/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/dictionary.c.o\n",
            "[24/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/phoneme.c.o\n",
            "[25/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/phonemelist.c.o\n",
            "[26/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/numbers.c.o\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c: In function â€˜LookupNum3â€™:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c:1331:60: warning: â€˜%sâ€™ directive writing up to 159 bytes into a region of size between 0 and 49 [-Wformat-overflow=]\n",
            " 1331 |                                 sprintf(ph_thousands, \"%s%c%s%c\", ph_digits, phonEND_WORD, ph_10T, phonEND_WORD);\n",
            "      |                                                            ^~                              ~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c:1331:33: note: â€˜sprintfâ€™ output between 3 and 211 bytes into a destination of size 50\n",
            " 1331 |                                 sprintf(ph_thousands, \"%s%c%s%c\", ph_digits, phonEND_WORD, ph_10T, phonEND_WORD);\n",
            "      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c:1329:56: warning: â€˜%sâ€™ directive writing up to 159 bytes into a region of size 50 [-Wformat-overflow=]\n",
            " 1329 |                                 sprintf(ph_thousands, \"%s%c%s%c\", ph_10T, phonEND_WORD, ph_digits, phonEND_WORD);\n",
            "      |                                                        ^~         ~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c:1329:33: note: â€˜sprintfâ€™ output between 3 and 211 bytes into a destination of size 50\n",
            " 1329 |                                 sprintf(ph_thousands, \"%s%c%s%c\", ph_10T, phonEND_WORD, ph_digits, phonEND_WORD);\n",
            "      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c:1395:36: warning: â€˜%sâ€™ directive writing up to 49 bytes into a region of size between 40 and 100 [-Wformat-overflow=]\n",
            " 1395 |                 sprintf(buf1, \"%s%s%s%s\", ph_thousands, ph_thousand_and, ph_digits, ph_100);\n",
            "      |                                    ^~                                    ~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c:1395:17: note: â€˜sprintfâ€™ output between 1 and 269 bytes into a destination of size 100\n",
            " 1395 |                 sprintf(buf1, \"%s%s%s%s\", ph_thousands, ph_thousand_and, ph_digits, ph_100);\n",
            "      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c: In function â€˜TranslateNumber_1â€™:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c:1575:63: warning: â€˜sprintfâ€™ may write a terminating nul past the end of the destination [-Wformat-overflow=]\n",
            " 1575 |                                         sprintf(string, \"_x#%s\", suffix);\n",
            "      |                                                               ^\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c:1575:41: note: â€˜sprintfâ€™ output between 4 and 33 bytes into a destination of size 32\n",
            " 1575 |                                         sprintf(string, \"_x#%s\", suffix);\n",
            "      |                                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "[27/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/setlengths.c.o\n",
            "[28/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/readclause.c.o\n",
            "[29/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/soundicon.c.o\n",
            "[30/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/spect.c.o\n",
            "[31/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/synthdata.c.o\n",
            "[32/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/ssml.c.o\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/ssml.c: In function â€˜ParseSsmlReferenceâ€™:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/ssml.c:1015:50: warning: format â€˜%xâ€™ expects argument of type â€˜unsigned int *â€™, but argument 3 has type â€˜int *â€™ [-Wformat=]\n",
            " 1015 |                         return sscanf(&ref[2], \"%x\", c1);\n",
            "      |                                                 ~^   ~~\n",
            "      |                                                  |   |\n",
            "      |                                                  |   int *\n",
            "      |                                                  unsigned int *\n",
            "      |                                                 %x\n",
            "[33/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/tr_languages.c.o\n",
            "[34/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/synthesize.c.o\n",
            "[35/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/translate.c.o\n",
            "[36/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/translateword.c.o\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/translateword.c: In function â€˜TranslateLetterâ€™:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/translateword.c:1020:39: warning: â€˜%sâ€™ directive writing up to 29 bytes into a region of size between 0 and 79 [-Wformat-overflow=]\n",
            " 1020 |                 sprintf(ph_buf2, \"%c%s%s%s\", 0xff, ph_alphabet, capital, ph_buf); // the 0xff marker will be removed or replaced in SetSpellingStress()\n",
            "      |                                       ^~                        ~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/translateword.c:1020:17: note: â€˜sprintfâ€™ output between 2 and 189 bytes into a destination of size 80\n",
            " 1020 |                 sprintf(ph_buf2, \"%c%s%s%s\", 0xff, ph_alphabet, capital, ph_buf); // the 0xff marker will be removed or replaced in SetSpellingStress()\n",
            "      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/translateword.c:1018:39: warning: â€˜%sâ€™ directive writing up to 79 bytes into a region of size between 0 and 79 [-Wformat-overflow=]\n",
            " 1018 |                 sprintf(ph_buf2, \"%c%s%s%s\", 0xff, ph_alphabet, ph_buf, capital);\n",
            "      |                                       ^~                        ~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/translateword.c:1018:17: note: â€˜sprintfâ€™ output between 2 and 189 bytes into a destination of size 80\n",
            " 1018 |                 sprintf(ph_buf2, \"%c%s%s%s\", 0xff, ph_alphabet, ph_buf, capital);\n",
            "      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "[37/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/voices.c.o\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/voices.c: In function â€˜LoadVoiceâ€™:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/voices.c:460:33: warning: â€˜%sâ€™ directive writing up to 39 bytes into a region of size between 19 and 190 [-Wformat-overflow=]\n",
            "  460 |                 sprintf(buf, \"%s%s\", path_voices, voicename); // look in the main voices directory\n",
            "      |                                 ^~                ~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/voices.c:460:17: note: â€˜sprintfâ€™ output between 1 and 211 bytes into a destination of size 190\n",
            "  460 |                 sprintf(buf, \"%s%s\", path_voices, voicename); // look in the main voices directory\n",
            "      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/voices.c:464:41: warning: â€˜%sâ€™ directive writing up to 39 bytes into a region of size between 19 and 190 [-Wformat-overflow=]\n",
            "  464 |                         sprintf(buf, \"%s%s\", path_voices, voicename); // look in the main languages directory\n",
            "      |                                         ^~                ~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/voices.c:464:25: note: â€˜sprintfâ€™ output between 1 and 211 bytes into a destination of size 190\n",
            "  464 |                         sprintf(buf, \"%s%s\", path_voices, voicename); // look in the main languages directory\n",
            "      |                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/voices.c: In function â€˜SetVoiceScoresâ€™:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/voices.c:977:41: warning: â€˜%sâ€™ directive writing up to 79 bytes into a region of size between 73 and 232 [-Wformat-overflow=]\n",
            "  977 |                 sprintf(buf, \"%s/voices/%s\", path_home, language);\n",
            "      |                                         ^~              ~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/voices.c:977:17: note: â€˜sprintfâ€™ output between 9 and 247 bytes into a destination of size 240\n",
            "  977 |                 sprintf(buf, \"%s/voices/%s\", path_home, language);\n",
            "      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "[38/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/speech.c.o\n",
            "[39/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/wavegen.c.o\n",
            "[40/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/espeak_api.c.o\n",
            "[41/167] Linking C static library src/libespeak-ng/libespeak-ng.a\n",
            "[42/167] Linking C executable src/espeak-ng\n",
            "[43/167] Compile intonations\n",
            "Compiled 34 intonation tunes: 0 errors.\n",
            "[44/167] Link espeak-ng to compat names\n",
            "[45/167] Compile phonemes\n",
            "Compiling phoneme data: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/../phsource/phonemes\n",
            "\n",
            "Refs 4914,  Reused 3937\n",
            "Compiled phonemes: 0 errors.\n",
            "[46/167] Generating espeak-ng-data/be_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/be_dict'\n",
            "Using phonemetable: 'be'\n",
            "Compiling: 'be_list'\n",
            "\t77 entries\n",
            "Compiling: 'be_rules'\n",
            "\t83 rules, 33 groups (32)\n",
            "\n",
            "[47/167] Generating espeak-ng-data/am_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/am_dict'\n",
            "Using phonemetable: 'am'\n",
            "Compiling: 'am_list'\n",
            "\t31 entries\n",
            "Compiling: 'am_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'am_rules'\n",
            "\t345 rules, 7 groups (0)\n",
            "\n",
            "[48/167] Generating espeak-ng-data/an_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/an_dict'\n",
            "Using phonemetable: 'an'\n",
            "Compiling: 'an_list'\n",
            "\t484 entries\n",
            "Compiling: 'an_rules'\n",
            "\t184 rules, 29 groups (0)\n",
            "\n",
            "[49/167] Generating espeak-ng-data/as_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/as_dict'\n",
            "Using phonemetable: 'as'\n",
            "Compiling: 'as_list'\n",
            "\t209 entries\n",
            "Compiling: 'as_rules'\n",
            "\t146 rules, 66 groups (66)\n",
            "\n",
            "[50/167] Generating espeak-ng-data/az_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/az_dict'\n",
            "Using phonemetable: 'az'\n",
            "Compiling: 'az_list'\n",
            "\t84 entries\n",
            "Compiling: 'az_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'az_rules'\n",
            "\t58 rules, 34 groups (0)\n",
            "\n",
            "[51/167] Generating espeak-ng-data/af_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/af_dict'\n",
            "Using phonemetable: 'af'\n",
            "Compiling: 'af_list'\n",
            "\t1584 entries\n",
            "Compiling: 'af_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'af_rules'\n",
            "\t5202 rules, 60 groups (0)\n",
            "\n",
            "[52/167] Generating espeak-ng-data/ba_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ba_dict'\n",
            "Using phonemetable: 'ba'\n",
            "Compiling: 'ba_list'\n",
            "\t70 entries\n",
            "Compiling: 'ba_rules'\n",
            "\t52 rules, 44 groups (0)\n",
            "\n",
            "[53/167] Generating espeak-ng-data/bg_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/bg_dict'\n",
            "Using phonemetable: 'bg'\n",
            "Compiling: 'bg_listx'\n",
            "\t2790 entries\n",
            "Compiling: 'bg_list'\n",
            "\t231 entries\n",
            "Compiling: 'bg_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'bg_rules'\n",
            "\t118 rules, 31 groups (30)\n",
            "\n",
            "[54/167] Generating espeak-ng-data/bn_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/bn_dict'\n",
            "Using phonemetable: 'bn'\n",
            "Compiling: 'bn_list'\n",
            "\t380 entries\n",
            "Compiling: 'bn_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'bn_rules'\n",
            "\t168 rules, 68 groups (67)\n",
            "\n",
            "[55/167] Generating espeak-ng-data/bpy_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/bpy_dict'\n",
            "Using phonemetable: 'bpy'\n",
            "Compiling: 'bpy_list'\n",
            "\t179 entries\n",
            "Compiling: 'bpy_rules'\n",
            "\t212 rules, 63 groups (63)\n",
            "\n",
            "[56/167] Generating espeak-ng-data/bs_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/bs_dict'\n",
            "Using phonemetable: 'hr'\n",
            "Compiling: 'bs_list'\n",
            "\t613 entries\n",
            "Compiling: 'bs_emoji'\n",
            "\t1635 entries\n",
            "Compiling: 'bs_rules'\n",
            "\t112 rules, 34 groups (0)\n",
            "\n",
            "[57/167] Generating espeak-ng-data/chr_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/chr_dict'\n",
            "Using phonemetable: 'chr'\n",
            "Compiling: 'chr_list'\n",
            "\t0 entries\n",
            "Compiling: 'chr_rules'\n",
            "\t198 rules, 27 groups (0)\n",
            "\n",
            "[58/167] Generating espeak-ng-data/cmn_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/cmn_dict'\n",
            "Using phonemetable: 'cmn'\n",
            "Compiling: 'cmn_list'\n",
            "\t3860 entries\n",
            "Compiling: 'cmn_listx'\n",
            "\t77860 entries\n",
            "Compiling: 'cmn_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'cmn_rules'\n",
            "\t136 rules, 28 groups (0)\n",
            "\n",
            "[59/167] Generating espeak-ng-data/cs_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/cs_dict'\n",
            "Using phonemetable: 'cs'\n",
            "Compiling: 'cs_list'\n",
            "\t357 entries\n",
            "Compiling: 'cs_emoji'\n",
            "\t1689 entries\n",
            "Compiling: 'cs_rules'\n",
            "\t506 rules, 48 groups (0)\n",
            "\n",
            "[60/167] Generating espeak-ng-data/cv_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/cv_dict'\n",
            "Using phonemetable: 'cv'\n",
            "Compiling: 'cv_list'\n",
            "\t0 entries\n",
            "Compiling: 'cv_rules'\n",
            "\t39 rules, 37 groups (0)\n",
            "\n",
            "[61/167] Generating espeak-ng-data/cy_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/cy_dict'\n",
            "Using phonemetable: 'cy'\n",
            "Compiling: 'cy_list'\n",
            "\t166 entries\n",
            "Compiling: 'cy_emoji'\n",
            "\t1636 entries\n",
            "Compiling: 'cy_rules'\n",
            "\t210 rules, 27 groups (0)\n",
            "\n",
            "[62/167] Generating espeak-ng-data/ca_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ca_dict'\n",
            "Using phonemetable: 'ca'\n",
            "Compiling: 'ca_list'\n",
            "\t18455 entries\n",
            "Compiling: 'ca_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ca_rules'\n",
            "\t965 rules, 32 groups (0)\n",
            "\n",
            "[63/167] Generating espeak-ng-data/de_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/de_dict'\n",
            "Using phonemetable: 'de'\n",
            "Compiling: 'de_list'\n",
            "\t998 entries\n",
            "Compiling: 'de_emoji'\n",
            "\t1688 entries\n",
            "Compiling: 'de_rules'\n",
            "\t1330 rules, 34 groups (0)\n",
            "\n",
            "[64/167] Generating espeak-ng-data/el_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/el_dict'\n",
            "Using phonemetable: 'el'\n",
            "Compiling: 'el_list'\n",
            "\t379 entries\n",
            "Compiling: 'el_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'el_rules'\n",
            "\t182 rules, 27 groups (26)\n",
            "\n",
            "[65/167] Generating espeak-ng-data/da_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/da_dict'\n",
            "Using phonemetable: 'da'\n",
            "Compiling: 'da_list'\n",
            "\t11152 entries\n",
            "Compiling: 'da_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'da_rules'\n",
            "\t9269 rules, 56 groups (0)\n",
            "\n",
            "[66/167] Generating espeak-ng-data/eo_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/eo_dict'\n",
            "Using phonemetable: 'eo'\n",
            "Compiling: 'eo_list'\n",
            "\t242 entries\n",
            "Compiling: 'eo_rules'\n",
            "\t130 rules, 27 groups (0)\n",
            "\n",
            "[67/167] Generating espeak-ng-data/ar_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ar_dict'\n",
            "Using phonemetable: 'ar'\n",
            "Compiling: 'ar_listx'\n",
            "\t30089 entries\n",
            "Compiling: 'ar_list'\n",
            "\t252 entries\n",
            "Compiling: 'ar_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ar_rules'\n",
            "\t383 rules, 39 groups (37)\n",
            "\n",
            "[68/167] Generating espeak-ng-data/es_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/es_dict'\n",
            "Using phonemetable: 'es'\n",
            "Compiling: 'es_list'\n",
            "\t371 entries\n",
            "Compiling: 'es_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'es_rules'\n",
            "\t192 rules, 29 groups (0)\n",
            "\n",
            "[69/167] Generating espeak-ng-data/et_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/et_dict'\n",
            "Using phonemetable: 'et'\n",
            "Compiling: 'et_list'\n",
            "\t303 entries\n",
            "Compiling: 'et_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'et_rules'\n",
            "\t221 rules, 31 groups (0)\n",
            "\n",
            "[70/167] Generating espeak-ng-data/eu_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/eu_dict'\n",
            "Using phonemetable: 'eu'\n",
            "Compiling: 'eu_list'\n",
            "\t194 entries\n",
            "Compiling: 'eu_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'eu_rules'\n",
            "\t159 rules, 27 groups (0)\n",
            "\n",
            "[71/167] Generating espeak-ng-data/fi_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/fi_dict'\n",
            "Using phonemetable: 'fi'\n",
            "Compiling: 'fi_list'\n",
            "\t332 entries\n",
            "Compiling: 'fi_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'fi_rules'\n",
            "\t144 rules, 29 groups (0)\n",
            "\n",
            "[72/167] Generating espeak-ng-data/en_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/en_dict'\n",
            "Using phonemetable: 'en'\n",
            "Compiling: 'en_list'\n",
            "\t5560 entries\n",
            "Compiling: 'en_emoji'\n",
            "\t1690 entries\n",
            "Compiling: 'en_rules'\n",
            "\t6793 rules, 104 groups (0)\n",
            "\n",
            "[73/167] Generating espeak-ng-data/fr_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/fr_dict'\n",
            "Using phonemetable: 'fr'\n",
            "Compiling: 'fr_list'\n",
            "\t885 entries\n",
            "Compiling: 'fr_emoji'\n",
            "\t1638 entries\n",
            "Compiling: 'fr_rules'\n",
            "\t1207 rules, 33 groups (0)\n",
            "\n",
            "[74/167] Generating espeak-ng-data/gd_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/gd_dict'\n",
            "Using phonemetable: 'gd'\n",
            "Compiling: 'gd_list'\n",
            "\t104 entries\n",
            "Compiling: 'gd_emoji'\n",
            "\t1636 entries\n",
            "Compiling: 'gd_rules'\n",
            "\t252 rules, 35 groups (0)\n",
            "\n",
            "[75/167] Generating espeak-ng-data/ga_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ga_dict'\n",
            "Using phonemetable: 'ga'\n",
            "Compiling: 'ga_list'\n",
            "\t250 entries\n",
            "Compiling: 'ga_emoji'\n",
            "\t1636 entries\n",
            "Compiling: 'ga_rules'\n",
            "\t481 rules, 34 groups (0)\n",
            "\n",
            "[76/167] Generating espeak-ng-data/grc_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/grc_dict'\n",
            "Using phonemetable: 'grc'\n",
            "Compiling: 'grc_list'\n",
            "\t22 entries\n",
            "Compiling: 'grc_rules'\n",
            "\t136 rules, 55 groups (27)\n",
            "\n",
            "[77/167] Generating espeak-ng-data/gn_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/gn_dict'\n",
            "Using phonemetable: 'gn'\n",
            "Compiling: 'gn_list'\n",
            "\t150 entries\n",
            "Compiling: 'gn_rules'\n",
            "\t58 rules, 48 groups (0)\n",
            "\n",
            "[78/167] Generating espeak-ng-data/gu_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/gu_dict'\n",
            "Using phonemetable: 'gu'\n",
            "Compiling: 'gu_list'\n",
            "\t174 entries\n",
            "Compiling: 'gu_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'gu_rules'\n",
            "\t269 rules, 79 groups (77)\n",
            "\n",
            "[79/167] Generating espeak-ng-data/hak_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/hak_dict'\n",
            "Using phonemetable: 'hak'\n",
            "Compiling: 'hak_list'\n",
            "\t22 entries\n",
            "Compiling: 'hak_rules'\n",
            "\t261 rules, 27 groups (0)\n",
            "\n",
            "[80/167] Generating espeak-ng-data/fa_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/fa_dict'\n",
            "Using phonemetable: 'fa'\n",
            "Compiling: 'fa_list'\n",
            "\t13233 entries\n",
            "Compiling: 'fa_emoji'\n",
            "\t1649 entries\n",
            "Compiling: 'fa_rules'\n",
            "\t3922 rules, 93 groups (45)\n",
            "\n",
            "[81/167] Generating espeak-ng-data/haw_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/haw_dict'\n",
            "Using phonemetable: 'haw'\n",
            "Compiling: 'haw_list'\n",
            "\t100 entries\n",
            "Compiling: 'haw_rules'\n",
            "\t40 rules, 30 groups (0)\n",
            "\n",
            "[82/167] Generating espeak-ng-data/he_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/he_dict'\n",
            "Using phonemetable: 'he'\n",
            "Compiling: 'he_listx'\n",
            "\t197 entries\n",
            "Compiling: 'he_list'\n",
            "\t93 entries\n",
            "Compiling: 'he_rules'\n",
            "\t723 rules, 65 groups (0)\n",
            "\n",
            "[83/167] Generating espeak-ng-data/ht_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ht_dict'\n",
            "Using phonemetable: 'ht'\n",
            "Compiling: 'ht_list'\n",
            "\t50 entries\n",
            "Compiling: 'ht_rules'\n",
            "\t36 rules, 32 groups (0)\n",
            "\n",
            "[84/167] Generating espeak-ng-data/hi_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/hi_dict'\n",
            "Using phonemetable: 'hi'\n",
            "Compiling: 'hi_list'\n",
            "\t316 entries\n",
            "Compiling: 'hi_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'hi_rules'\n",
            "\t301 rules, 87 groups (86)\n",
            "\n",
            "[85/167] Generating espeak-ng-data/hr_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/hr_dict'\n",
            "Using phonemetable: 'hr'\n",
            "Compiling: 'hr_list'\n",
            "\t613 entries\n",
            "Compiling: 'hr_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'hr_rules'\n",
            "\t112 rules, 34 groups (0)\n",
            "\n",
            "[86/167] Generating espeak-ng-data/hy_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/hy_dict'\n",
            "Using phonemetable: 'hy'\n",
            "Compiling: 'hy_list'\n",
            "\t170 entries\n",
            "Compiling: 'hy_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'hy_rules'\n",
            "\t96 rules, 38 groups (38)\n",
            "\n",
            "[87/167] Generating espeak-ng-data/id_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/id_dict'\n",
            "Using phonemetable: 'id'\n",
            "Compiling: 'id_list'\n",
            "\t132 entries\n",
            "Compiling: 'id_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'id_rules'\n",
            "\t78 rules, 27 groups (0)\n",
            "\n",
            "[88/167] Generating espeak-ng-data/hu_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/hu_dict'\n",
            "Using phonemetable: 'hu'\n",
            "Compiling: 'hu_list'\n",
            "\t5352 entries\n",
            "Compiling: 'hu_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'hu_rules'\n",
            "\t5309 rules, 48 groups (0)\n",
            "\n",
            "[89/167] Generating espeak-ng-data/io_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/io_dict'\n",
            "Using phonemetable: 'eo'\n",
            "Compiling: 'io_list'\n",
            "\t50 entries\n",
            "Compiling: 'io_rules'\n",
            "\t78 rules, 26 groups (0)\n",
            "\n",
            "[90/167] Generating espeak-ng-data/is_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/is_dict'\n",
            "Using phonemetable: 'is'\n",
            "Compiling: 'is_list'\n",
            "\t327 entries\n",
            "Compiling: 'is_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'is_rules'\n",
            "\t205 rules, 37 groups (0)\n",
            "\n",
            "[91/167] Generating espeak-ng-data/ja_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ja_dict'\n",
            "Using phonemetable: 'ja'\n",
            "Compiling: 'ja_list'\n",
            "\t23 entries\n",
            "Compiling: 'ja_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ja_rules'\n",
            "\t457 rules, 53 groups (0)\n",
            "\n",
            "[92/167] Generating espeak-ng-data/jbo_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/jbo_dict'\n",
            "Using phonemetable: 'jbo'\n",
            "Compiling: 'jbo_list'\n",
            "\t85 entries\n",
            "Compiling: 'jbo_rules'\n",
            "\t64 rules, 27 groups (0)\n",
            "\n",
            "[93/167] Generating espeak-ng-data/ka_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ka_dict'\n",
            "Using phonemetable: 'ka'\n",
            "Compiling: 'ka_list'\n",
            "\t148 entries\n",
            "Compiling: 'ka_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ka_rules'\n",
            "\t50 rules, 44 groups (43)\n",
            "\n",
            "[94/167] Generating espeak-ng-data/kk_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/kk_dict'\n",
            "Using phonemetable: 'kk'\n",
            "Compiling: 'kk_list'\n",
            "\t51 entries\n",
            "Compiling: 'kk_emoji'\n",
            "\t0 entries\n",
            "Compiling: 'kk_rules'\n",
            "\t42 rules, 42 groups (36)\n",
            "\n",
            "[95/167] Generating espeak-ng-data/it_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/it_dict'\n",
            "Using phonemetable: 'it'\n",
            "Compiling: 'it_listx'\n",
            "\t4309 entries\n",
            "Compiling: 'it_list'\n",
            "\t5230 entries\n",
            "Compiling: 'it_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'it_rules'\n",
            "\t714 rules, 30 groups (0)\n",
            "\n",
            "[96/167] Generating espeak-ng-data/kl_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/kl_dict'\n",
            "Using phonemetable: 'kl'\n",
            "Compiling: 'kl_list'\n",
            "\t102 entries\n",
            "Compiling: 'kl_rules'\n",
            "\t80 rules, 30 groups (0)\n",
            "\n",
            "[97/167] Generating espeak-ng-data/kn_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/kn_dict'\n",
            "Using phonemetable: 'kn'\n",
            "Compiling: 'kn_list'\n",
            "\t262 entries\n",
            "Compiling: 'kn_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'kn_rules'\n",
            "\t115 rules, 55 groups (50)\n",
            "\n",
            "[98/167] Generating espeak-ng-data/ko_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ko_dict'\n",
            "Using phonemetable: 'ko'\n",
            "Compiling: 'ko_list'\n",
            "\t133 entries\n",
            "Compiling: 'ko_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ko_rules'\n",
            "\t128 rules, 68 groups (40)\n",
            "\n",
            "[99/167] Generating espeak-ng-data/ku_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ku_dict'\n",
            "Using phonemetable: 'ku'\n",
            "Compiling: 'ku_list'\n",
            "\t106 entries\n",
            "Compiling: 'ku_rules'\n",
            "\t46 rules, 32 groups (0)\n",
            "\n",
            "[100/167] Generating espeak-ng-data/kok_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/kok_dict'\n",
            "Using phonemetable: 'kok'\n",
            "Compiling: 'kok_list'\n",
            "\t189 entries\n",
            "Compiling: 'kok_rules'\n",
            "\t337 rules, 88 groups (0)\n",
            "\n",
            "[101/167] Generating espeak-ng-data/ky_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ky_dict'\n",
            "Using phonemetable: 'ky'\n",
            "Compiling: 'ky_list'\n",
            "\t131 entries\n",
            "Compiling: 'ky_emoji'\n",
            "\t1636 entries\n",
            "Compiling: 'ky_rules'\n",
            "\t113 rules, 33 groups (0)\n",
            "\n",
            "[102/167] Generating espeak-ng-data/ia_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ia_dict'\n",
            "Using phonemetable: 'ia'\n",
            "Compiling: 'ia_listx'\n",
            "\t15620 entries\n",
            "Compiling: 'ia_list'\n",
            "\t90 entries\n",
            "Compiling: 'ia_rules'\n",
            "\t79 rules, 26 groups (0)\n",
            "\n",
            "[103/167] Generating espeak-ng-data/la_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/la_dict'\n",
            "Using phonemetable: 'la'\n",
            "Compiling: 'la_list'\n",
            "\t215 entries\n",
            "Compiling: 'la_rules'\n",
            "\t113 rules, 31 groups (0)\n",
            "\n",
            "[104/167] Generating espeak-ng-data/lfn_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/lfn_dict'\n",
            "Using phonemetable: 'base2'\n",
            "Compiling: 'lfn_list'\n",
            "\t127 entries\n",
            "Compiling: 'lfn_rules'\n",
            "\t76 rules, 27 groups (0)\n",
            "\n",
            "[105/167] Generating espeak-ng-data/lt_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/lt_dict'\n",
            "Using phonemetable: 'lt'\n",
            "Compiling: 'lt_list'\n",
            "\t167 entries\n",
            "Compiling: 'lt_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'lt_rules'\n",
            "\t210 rules, 38 groups (0)\n",
            "\n",
            "[106/167] Generating espeak-ng-data/lv_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/lv_dict'\n",
            "Using phonemetable: 'lv'\n",
            "Compiling: 'lv_list'\n",
            "\t887 entries\n",
            "Compiling: 'lv_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'lv_rules'\n",
            "\t1523 rules, 62 groups (0)\n",
            "\n",
            "[107/167] Generating espeak-ng-data/mi_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/mi_dict'\n",
            "Using phonemetable: 'mi'\n",
            "Compiling: 'mi_list'\n",
            "\t14 entries\n",
            "Compiling: 'mi_rules'\n",
            "\t27 rules, 18 groups (0)\n",
            "\n",
            "[108/167] Generating espeak-ng-data/mk_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/mk_dict'\n",
            "Using phonemetable: 'mk'\n",
            "Compiling: 'mk_list'\n",
            "\t188 entries\n",
            "Compiling: 'mk_emoji'\n",
            "\t1636 entries\n",
            "Compiling: 'mk_rules'\n",
            "\t96 rules, 34 groups (31)\n",
            "\n",
            "[109/167] Generating espeak-ng-data/ml_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ml_dict'\n",
            "Using phonemetable: 'ml'\n",
            "Compiling: 'ml_list'\n",
            "\t165 entries\n",
            "Compiling: 'ml_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ml_rules'\n",
            "\t141 rules, 45 groups (42)\n",
            "\n",
            "[110/167] Generating espeak-ng-data/mr_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/mr_dict'\n",
            "Using phonemetable: 'mr'\n",
            "Compiling: 'mr_list'\n",
            "\t235 entries\n",
            "Compiling: 'mr_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'mr_rules'\n",
            "\t298 rules, 87 groups (85)\n",
            "\n",
            "[111/167] Generating espeak-ng-data/ms_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ms_dict'\n",
            "Using phonemetable: 'id'\n",
            "Compiling: 'ms_list'\n",
            "\t703 entries\n",
            "Compiling: 'ms_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ms_rules'\n",
            "\t125 rules, 27 groups (0)\n",
            "\n",
            "[112/167] Generating espeak-ng-data/mt_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/mt_dict'\n",
            "Using phonemetable: 'mt'\n",
            "Compiling: 'mt_list'\n",
            "\t232 entries\n",
            "Compiling: 'mt_rules'\n",
            "\t99 rules, 31 groups (0)\n",
            "\n",
            "[113/167] Generating espeak-ng-data/mto_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/mto_dict'\n",
            "Using phonemetable: 'mto'\n",
            "Compiling: 'mto_list'\n",
            "\t185 entries\n",
            "Compiling: 'mto_rules'\n",
            "\t56 rules, 23 groups (0)\n",
            "\n",
            "[114/167] Generating espeak-ng-data/my_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/my_dict'\n",
            "Using phonemetable: 'my'\n",
            "Compiling: 'my_list'\n",
            "\t11 entries\n",
            "Compiling: 'my_emoji'\n",
            "\t1644 entries\n",
            "Compiling: 'my_rules'\n",
            "\t78 rules, 64 groups (0)\n",
            "\n",
            "[115/167] Generating espeak-ng-data/nci_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/nci_dict'\n",
            "Using phonemetable: 'nci'\n",
            "Compiling: 'nci_list'\n",
            "\t16 entries\n",
            "Compiling: 'nci_rules'\n",
            "\t41 rules, 21 groups (0)\n",
            "\n",
            "[116/167] Generating espeak-ng-data/ne_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ne_dict'\n",
            "Using phonemetable: 'ne'\n",
            "Compiling: 'ne_list'\n",
            "\t193 entries\n",
            "Compiling: 'ne_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ne_rules'\n",
            "\t520 rules, 100 groups (92)\n",
            "\n",
            "[117/167] Generating espeak-ng-data/nl_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/nl_dict'\n",
            "Using phonemetable: 'nl'\n",
            "Compiling: 'nl_list'\n",
            "\t1797 entries\n",
            "Compiling: 'nl_emoji'\n",
            "\t1690 entries\n",
            "Compiling: 'nl_rules'\n",
            "\t826 rules, 37 groups (0)\n",
            "\n",
            "[118/167] Generating espeak-ng-data/nog_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/nog_dict'\n",
            "Using phonemetable: 'nog'\n",
            "Compiling: 'nog_list'\n",
            "\t37 entries\n",
            "Compiling: 'nog_rules'\n",
            "\t180 rules, 33 groups (0)\n",
            "\n",
            "[119/167] Generating espeak-ng-data/no_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/no_dict'\n",
            "Using phonemetable: 'no'\n",
            "Compiling: 'no_list'\n",
            "\t236 entries\n",
            "Compiling: 'no_rules'\n",
            "\t142 rules, 32 groups (0)\n",
            "\n",
            "[120/167] Generating espeak-ng-data/om_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/om_dict'\n",
            "Using phonemetable: 'om'\n",
            "Compiling: 'om_list'\n",
            "\t87 entries\n",
            "Compiling: 'om_rules'\n",
            "\t53 rules, 33 groups (0)\n",
            "\n",
            "[121/167] Generating espeak-ng-data/or_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/or_dict'\n",
            "Using phonemetable: 'or'\n",
            "Compiling: 'or_list'\n",
            "\t198 entries\n",
            "Compiling: 'or_emoji'\n",
            "\t1634 entries\n",
            "Compiling: 'or_rules'\n",
            "\t197 rules, 67 groups (66)\n",
            "\n",
            "[122/167] Generating espeak-ng-data/pap_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/pap_dict'\n",
            "Using phonemetable: 'base2'\n",
            "Compiling: 'pap_list'\n",
            "\t86 entries\n",
            "Compiling: 'pap_rules'\n",
            "\t68 rules, 32 groups (0)\n",
            "\n",
            "[123/167] Generating espeak-ng-data/pa_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/pa_dict'\n",
            "Using phonemetable: 'pa'\n",
            "Compiling: 'pa_list'\n",
            "\t196 entries\n",
            "Compiling: 'pa_emoji'\n",
            "\t1636 entries\n",
            "Compiling: 'pa_rules'\n",
            "\t283 rules, 68 groups (65)\n",
            "\n",
            "[124/167] Generating espeak-ng-data/qya_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/qya_dict'\n",
            "Using phonemetable: 'qya'\n",
            "Compiling: 'qya_list'\n",
            "\t38 entries\n",
            "Compiling: 'qya_rules'\n",
            "\t70 rules, 38 groups (0)\n",
            "\n",
            "[125/167] Generating espeak-ng-data/ro_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ro_dict'\n",
            "Using phonemetable: 'ro'\n",
            "Compiling: 'ro_list'\n",
            "\t2167 entries\n",
            "Compiling: 'ro_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ro_rules'\n",
            "\t349 rules, 32 groups (0)\n",
            "\n",
            "[126/167] Generating espeak-ng-data/sd_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/sd_dict'\n",
            "Using phonemetable: 'sd'\n",
            "Compiling: 'sd_list'\n",
            "\t266 entries\n",
            "Compiling: 'sd_emoji'\n",
            "\t1565 entries\n",
            "Compiling: 'sd_rules'\n",
            "\t372 rules, 73 groups (47)\n",
            "\n",
            "[127/167] Generating espeak-ng-data/py_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/py_dict'\n",
            "Using phonemetable: 'py'\n",
            "Compiling: 'py_list'\n",
            "\t106 entries\n",
            "Compiling: 'py_rules'\n",
            "\t31 rules, 28 groups (0)\n",
            "\n",
            "[128/167] Generating espeak-ng-data/qdb_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/qdb_dict'\n",
            "Using phonemetable: 'qdb'\n",
            "Compiling: 'qdb_list'\n",
            "\t169 entries\n",
            "Compiling: 'qdb_rules'\n",
            "\t38 rules, 25 groups (0)\n",
            "\n",
            "[129/167] Generating espeak-ng-data/qu_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/qu_dict'\n",
            "Using phonemetable: 'qu'\n",
            "Compiling: 'qu_list'\n",
            "\t76 entries\n",
            "Compiling: 'qu_rules'\n",
            "\t39 rules, 28 groups (0)\n",
            "\n",
            "[130/167] Generating espeak-ng-data/quc_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/quc_dict'\n",
            "Using phonemetable: 'quc'\n",
            "Compiling: 'quc_list'\n",
            "\t10 entries\n",
            "Compiling: 'quc_emoji'\n",
            "\t0 entries\n",
            "Compiling: 'quc_rules'\n",
            "\t35 rules, 27 groups (0)\n",
            "\n",
            "[131/167] Generating espeak-ng-data/pl_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/pl_dict'\n",
            "Using phonemetable: 'pl'\n",
            "Compiling: 'pl_list'\n",
            "\t2956 entries\n",
            "Compiling: 'pl_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'pl_rules'\n",
            "\t869 rules, 46 groups (0)\n",
            "\n",
            "[132/167] Generating espeak-ng-data/piqd_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/piqd_dict'\n",
            "Using phonemetable: 'piqd'\n",
            "Compiling: 'piqd_list'\n",
            "\t36 entries\n",
            "Compiling: 'piqd_rules'\n",
            "\t35 rules, 23 groups (0)\n",
            "\n",
            "[133/167] Generating espeak-ng-data/pt_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/pt_dict'\n",
            "Using phonemetable: 'pt-pt'\n",
            "Compiling: 'pt_list'\n",
            "\t2006 entries\n",
            "Compiling: 'pt_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'pt_rules'\n",
            "\t1105 rules, 39 groups (0)\n",
            "\n",
            "[134/167] Generating espeak-ng-data/lb_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/lb_dict'\n",
            "Using phonemetable: 'lb'\n",
            "Compiling: 'lb_list'\n",
            "\t35646 entries\n",
            "Compiling: 'lb_emoji'\n",
            "\t25 entries\n",
            "Compiling: 'lb_rules'\n",
            "\t175 rules, 50 groups (0)\n",
            "\n",
            "[135/167] Generating espeak-ng-data/shn_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/shn_dict'\n",
            "Using phonemetable: 'shn'\n",
            "Compiling: 'shn_list'\n",
            "\t4175 entries\n",
            "Compiling: 'shn_rules'\n",
            "\t229 rules, 53 groups (0)\n",
            "\n",
            "[136/167] Generating espeak-ng-data/si_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/si_dict'\n",
            "Using phonemetable: 'si'\n",
            "Compiling: 'si_list'\n",
            "\t152 entries\n",
            "Compiling: 'si_emoji'\n",
            "\t1648 entries\n",
            "Compiling: 'si_rules'\n",
            "\t142 rules, 75 groups (73)\n",
            "\n",
            "[137/167] Generating espeak-ng-data/sjn_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/sjn_dict'\n",
            "Using phonemetable: 'sjn'\n",
            "Compiling: 'sjn_list'\n",
            "\t21 entries\n",
            "Compiling: 'sjn_rules'\n",
            "\t71 rules, 40 groups (0)\n",
            "\n",
            "[138/167] Generating espeak-ng-data/sk_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/sk_dict'\n",
            "Using phonemetable: 'sk'\n",
            "Compiling: 'sk_list'\n",
            "\t348 entries\n",
            "Compiling: 'sk_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'sk_rules'\n",
            "\t535 rules, 46 groups (0)\n",
            "\n",
            "[139/167] Generating espeak-ng-data/smj_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/smj_dict'\n",
            "Using phonemetable: 'smj'\n",
            "Compiling: 'smj_list'\n",
            "\t2816 entries\n",
            "Compiling: 'smj_rules'\n",
            "\t136 rules, 34 groups (0)\n",
            "\n",
            "[140/167] Generating espeak-ng-data/sl_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/sl_dict'\n",
            "Using phonemetable: 'sl'\n",
            "Compiling: 'sl_list'\n",
            "\t196 entries\n",
            "Compiling: 'sl_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'sl_rules'\n",
            "\t108 rules, 31 groups (0)\n",
            "\n",
            "[141/167] Generating espeak-ng-data/sq_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/sq_dict'\n",
            "Using phonemetable: 'sq'\n",
            "Compiling: 'sq_list'\n",
            "\t132 entries\n",
            "Compiling: 'sq_emoji'\n",
            "\t1636 entries\n",
            "Compiling: 'sq_rules'\n",
            "\t100 rules, 29 groups (0)\n",
            "\n",
            "[142/167] Generating espeak-ng-data/sr_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/sr_dict'\n",
            "Using phonemetable: 'sr'\n",
            "Compiling: 'sr_list'\n",
            "\t613 entries\n",
            "Compiling: 'sr_emoji'\n",
            "\t1568 entries\n",
            "Compiling: 'sr_rules'\n",
            "\t112 rules, 34 groups (0)\n",
            "\n",
            "[143/167] Generating espeak-ng-data/sv_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/sv_dict'\n",
            "Using phonemetable: 'sv'\n",
            "Compiling: 'sv_list'\n",
            "\t341 entries\n",
            "Compiling: 'sv_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'sv_rules'\n",
            "\t697 rules, 30 groups (0)\n",
            "\n",
            "[144/167] Generating espeak-ng-data/sw_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/sw_dict'\n",
            "Using phonemetable: 'sw'\n",
            "Compiling: 'sw_list'\n",
            "\t170 entries\n",
            "Compiling: 'sw_emoji'\n",
            "\t1638 entries\n",
            "Compiling: 'sw_rules'\n",
            "\t63 rules, 27 groups (0)\n",
            "\n",
            "[145/167] Generating espeak-ng-data/te_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/te_dict'\n",
            "Using phonemetable: 'te'\n",
            "Compiling: 'te_list'\n",
            "\t155 entries\n",
            "Compiling: 'te_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'te_rules'\n",
            "\t112 rules, 56 groups (53)\n",
            "\n",
            "[146/167] Generating espeak-ng-data/th_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/th_dict'\n",
            "Using phonemetable: 'th'\n",
            "Compiling: 'th_list'\n",
            "\t10 entries\n",
            "Compiling: 'th_rules'\n",
            "\t139 rules, 64 groups (0)\n",
            "\n",
            "[147/167] Generating espeak-ng-data/ta_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ta_dict'\n",
            "Using phonemetable: 'ta'\n",
            "Compiling: 'ta_list'\n",
            "\t551 entries\n",
            "Compiling: 'ta_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ta_rules'\n",
            "\t5040 rules, 33 groups (28)\n",
            "\n",
            "[148/167] Generating espeak-ng-data/tk_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/tk_dict'\n",
            "Using phonemetable: 'tk'\n",
            "Compiling: 'tk_listx'\n",
            "\t868 entries\n",
            "Compiling: 'tk_list'\n",
            "\t62 entries\n",
            "Compiling: 'tk_rules'\n",
            "\t318 rules, 30 groups (0)\n",
            "\n",
            "[149/167] Generating espeak-ng-data/ti_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ti_dict'\n",
            "Using phonemetable: 'ti'\n",
            "Compiling: 'ti_list'\n",
            "\t31 entries\n",
            "Compiling: 'ti_emoji'\n",
            "\t1690 entries\n",
            "Compiling: 'ti_rules'\n",
            "\t361 rules, 7 groups (0)\n",
            "\n",
            "[150/167] Generating espeak-ng-data/tn_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/tn_dict'\n",
            "Using phonemetable: 'tn'\n",
            "Compiling: 'tn_list'\n",
            "\t110 entries\n",
            "Compiling: 'tn_rules'\n",
            "\t82 rules, 27 groups (0)\n",
            "\n",
            "[151/167] Generating espeak-ng-data/tt_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/tt_dict'\n",
            "Using phonemetable: 'tt'\n",
            "Compiling: 'tt_list'\n",
            "\t74 entries\n",
            "Compiling: 'tt_rules'\n",
            "\t49 rules, 40 groups (34)\n",
            "\n",
            "[152/167] Generating espeak-ng-data/tr_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/tr_dict'\n",
            "Using phonemetable: 'tr'\n",
            "Compiling: 'tr_listx'\n",
            "\t147 entries\n",
            "Compiling: 'tr_list'\n",
            "\t175 entries\n",
            "Compiling: 'tr_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'tr_rules'\n",
            "\t215 rules, 37 groups (0)\n",
            "\n",
            "[153/167] Generating espeak-ng-data/ug_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ug_dict'\n",
            "Using phonemetable: 'ug'\n",
            "Compiling: 'ug_list'\n",
            "\t37 entries\n",
            "Compiling: 'ug_rules'\n",
            "\t81 rules, 61 groups (0)\n",
            "\n",
            "[154/167] Generating espeak-ng-data/uk_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/uk_dict'\n",
            "Using phonemetable: 'uk'\n",
            "Compiling: 'uk_list'\n",
            "\t167 entries\n",
            "Compiling: 'uk_rules'\n",
            "\t47 rules, 34 groups (33)\n",
            "\n",
            "[155/167] Generating espeak-ng-data/uz_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/uz_dict'\n",
            "Using phonemetable: 'uz'\n",
            "Compiling: 'uz_list'\n",
            "\t122 entries\n",
            "Compiling: 'uz_rules'\n",
            "\t35 rules, 26 groups (0)\n",
            "\n",
            "[156/167] Generating espeak-ng-data/vi_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/vi_dict'\n",
            "Using phonemetable: 'vi'\n",
            "Compiling: 'vi_list'\n",
            "\t135 entries\n",
            "Compiling: 'vi_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'vi_rules'\n",
            "\t592 rules, 97 groups (0)\n",
            "\n",
            "[157/167] Generating espeak-ng-data/ur_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ur_dict'\n",
            "Using phonemetable: 'ur'\n",
            "Compiling: 'ur_list'\n",
            "\t3028 entries\n",
            "Compiling: 'ur_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ur_rules'\n",
            "\t1488 rules, 62 groups (44)\n",
            "\n",
            "[158/167] Generating espeak-ng-data/yue_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/yue_dict'\n",
            "Using phonemetable: 'yue'\n",
            "Compiling: 'yue_list'\n",
            "\t3822 entries\n",
            "Compiling: 'yue_listx'\n",
            "\t33813 entries\n",
            "Compiling: 'yue_emoji'\n",
            "\t1635 entries\n",
            "Compiling: 'yue_rules'\n",
            "\t79 rules, 27 groups (0)\n",
            "\n",
            "[159/167] Generating espeak-ng-data/ru_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ru_dict'\n",
            "Using phonemetable: 'ru'\n",
            "Compiling: 'ru_listx'\n",
            "\t811332 entries\n",
            "Compiling: 'ru_list'\n",
            "\t287 entries\n",
            "Compiling: 'ru_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ru_rules'\n",
            "\t437 rules, 34 groups (33)\n",
            "\n",
            "[160/167] Building C object tests/CMakeFiles/test_ieee80.dir/ieee80.c.o\n",
            "[161/167] Linking C executable tests/test_ieee80\n",
            "[162/167] Building C object tests/CMakeFiles/test_api.dir/api.c.o\n",
            "[163/167] Linking C executable tests/test_api\n",
            "[164/167] Building C object tests/CMakeFiles/test_readclause.dir/readclause.c.o\n",
            "[165/167] Building C object tests/CMakeFiles/test_encoding.dir/encoding.c.o\n",
            "[166/167] Linking C executable tests/test_readclause\n",
            "[167/167] Linking C executable tests/test_encoding\n",
            "[7/12] Performing install step for 'espeak_ng_external'\u001b[K\n",
            "[1/2] Link espeak-ng to compat names\n",
            "[1/2] Install the project...\n",
            "-- Install configuration: \"\"\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/hi_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/az_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/kk_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/hy_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/mk_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/te_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lb_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/bn_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/py_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/jbo_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/be_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ht_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/tr_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/gu_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/bpy_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ka_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/tk_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/phondata-manifest\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/kn_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ky_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/pa_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/phondata\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/de_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/af_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/mi_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/da_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/uk_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/pt_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/gn_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/tt_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/sw_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/nog_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/cv_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ms_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/it_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/sd_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/shn_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/gd_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/no_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ro_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/eu_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/et_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ar_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/piqd_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lfn_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/io_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/nci_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/quc_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ko_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/hak_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/pl_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ia_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/sq_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/chr_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/mto_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/my_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/eo_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/pap_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/kl_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/fr_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/qdb_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/phonindex\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/cy_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/is_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/sr_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/sl_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/he_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/id_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ur_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/fi_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ca_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/qya_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/cs_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ba_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/intonations\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/si_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/el_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/yue_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ug_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/vi_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ta_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/tn_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/grc_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ml_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/cmn_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sit\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sit/yue\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sit/cmn-Latn-pinyin\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sit/my\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sit/cmn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sit/yue-Latn-jyutping\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sit/hak\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ccs\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ccs/ka\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ine\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ine/hy\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ine/sq\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ine/hyw\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/qu\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/iro\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/iro/chr\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/cus\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/cus/om\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/tai\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/tai/th\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/tai/shn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/miz\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/miz/mto\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/myn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/myn/quc\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zlw\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zlw/pl\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zlw/cs\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zlw/sk\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/aav\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/aav/vi-VN-x-south\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/aav/vi-VN-x-central\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/aav/vi\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/cel\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/cel/cy\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/cel/gd\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/cel/ga\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/esx\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/esx/kl\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/poz\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/poz/ms\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/poz/id\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/poz/mi\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/itc\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/itc/la\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/azc\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/azc/nci\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sai\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sai/gn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sem\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sem/ti\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sem/ar\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sem/mt\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sem/am\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sem/he\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ko\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/dra\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/dra/ta\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/dra/ml\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/dra/te\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/dra/kn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zls\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zls/sr\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zls/bs\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zls/hr\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zls/bg\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zls/sl\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zls/mk\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/urj\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/urj/hu\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/urj/et\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/urj/smj\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/urj/fi\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/jpx\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/jpx/ja\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ira\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ira/ku\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ira/ps\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ira/fa-Latn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ira/fa\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/map\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/map/haw\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zle\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zle/ru\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zle/uk\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zle/ru-cl\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zle/be\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zle/ru-LV\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/si\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/kok\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/ne\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/ur\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/as\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/gu\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/bn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/bpy\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/sd\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/or\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/pa\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/hi\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/mr\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/eu\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en-US-nyc\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en-Shaw\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en-GB-x-rp\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/nl\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en-029\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en-GB-x-gbclan\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/de\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/af\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/lb\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en-US\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en-GB-x-gbcwmd\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en-GB-scotland\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/ca-va\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/ca-ba\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/an\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/es\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/it\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/ca\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/ca-nw\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/pap\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/pt-BR\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/pt\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/fr-BE\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/ro\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/ht\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/es-419\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/fr-CH\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/fr\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/grk\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/grk/grc\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/grk/el\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmq\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmq/is\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmq/sv\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmq/da\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmq/fo\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmq/nb\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/bat\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/bat/lt\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/bat/ltg\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/bat/lv\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/bnt\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/bnt/sw\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/bnt/tn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/az\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/ky\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/ug\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/kaa\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/kk\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/tt\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/cv\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/ba\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/nog\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/tr\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/uz\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/tk\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/xex\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/sjn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/ia\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/py\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/lfn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/qdb\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/jbo\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/eo\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/io\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/qya\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/piqd\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/an_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/nl_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/bs_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/klatt3\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/UniRobot\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/RicishayMax3\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Demonic\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/whisper\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/linda\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Reed\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/klatt4\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Henrique\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/benjamin\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/adam\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Gene\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Diogo\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/f5\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/victor\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/steph\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/m4\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/miguel\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/iven4\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robosoft3\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Hugo\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/belinda\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Gene2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Mr serious\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/f3\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robosoft8\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Mike\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/grandma\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/grandpa\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/norbert\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/mike2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Mario\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/boris\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/edward\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/ed\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/m1\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/zac\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/m2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/anikaRobot\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/ian\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/m8\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/david\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Tweaky\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/AnxiousAndy\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Nguyen\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Annie\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/sandro\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/iven3\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Michael\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/john\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/iven\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/kaukovalta\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robosoft2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robosoft\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Andy\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robosoft5\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/michel\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robosoft6\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robert\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Marco\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/iven2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/croak\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/fast\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/f4\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/steph2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Storm\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/RicishayMax2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/announcer\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robosoft4\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/klatt2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/marcelo\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/m7\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/m6\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/edward2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/pablo\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/f1\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/m5\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/max\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/klatt\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/quincy\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Jacky\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/paul\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/anika\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/RicishayMax\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Alicia\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/aunty\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/klatt6\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/pedro\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/travis\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/antonio\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Lee\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/caleb\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/gustave\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/rob\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/shelby\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/whisperf\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Andrea\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robosoft7\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/m3\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/f2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Alex\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/steph3\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/klatt5\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Denis\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/or_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/en_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/qu_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/sv_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/la_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lt_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ja_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/haw_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/uz_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/hr_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/sk_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/phontab\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/th_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/as_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ru_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lv_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/mr_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/es_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ti_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ku_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/om_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/bg_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ne_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/hu_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/mt_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/am_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ga_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/fa_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/kok_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/sjn_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/smj_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/lib/pkgconfig/espeak-ng.pc\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/vim/vimfiles/ftdetect\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/vim/vimfiles/ftdetect/espeakfiletype.vim\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/vim/vimfiles/syntax\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/vim/vimfiles/syntax/espeaklist.vim\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/vim/vimfiles/syntax/espeakrules.vim\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/bin/espeak-ng\n",
            "-- Set non-toolchain portion of runtime path of \"/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/bin/espeak-ng\" to \"/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/lib\"\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/include/espeak\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/include/espeak/speak_lib.h\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/include/espeak-ng\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/include/espeak-ng/espeak_ng.h\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/include/espeak-ng/encoding.h\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/include/espeak-ng/speak_lib.h\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/lib/libespeak-ng.a\n",
            "[11/12] Install the project...\u001b[K\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-install/src/piper/./espeakbridge.so\n",
            "copying _skbuild/linux-x86_64-3.12/cmake-install/src/piper/espeakbridge.so -> src/piper/espeakbridge.so\n",
            "\n",
            "running build_ext\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CLONE AND INSTALL PIPER TTS\n",
        "# =============================================================================\n",
        "# Clone the piper1-gpl repository and install with training dependencies.\n",
        "\n",
        "import os\n",
        "\n",
        "PIPER_REPO_URL = \"https://github.com/Vinit-source/piper1-gpl.git\"\n",
        "PIPER_BRANCH = \"main\"\n",
        "PIPER_COMMIT = \"fee9b9cefae4ebf9e196cfe994dea418f051506c\"  # Stable release commit\n",
        "\n",
        "# Clone repository if not exists\n",
        "if not os.path.exists(config.piper_dir):\n",
        "    print(f\"Cloning Piper repository from {PIPER_REPO_URL}...\")\n",
        "    !git clone -b {PIPER_BRANCH} {PIPER_REPO_URL} {config.piper_dir}\n",
        "else:\n",
        "    print(f\"Repository already exists at {config.piper_dir}\")\n",
        "\n",
        "# Change to piper directory\n",
        "%cd {config.piper_dir}\n",
        "\n",
        "# Checkout specific commit for reproducibility\n",
        "print(f\"Checking out commit {PIPER_COMMIT}...\")\n",
        "!git checkout -b release0.3.1 {PIPER_COMMIT} 2>/dev/null || git checkout release0.3.1\n",
        "\n",
        "# Uninstall previous installation to ensure clean rebuild\n",
        "print(\"Removing any previous Piper TTS installation...\")\n",
        "!pip uninstall -y piper-tts 2>/dev/null || true\n",
        "\n",
        "# Install Piper with training dependencies\n",
        "print(\"Installing Piper TTS with training dependencies...\")\n",
        "!pip install -e .[train]\n",
        "\n",
        "# Build monotonic alignment module (required for VITS training)\n",
        "print(\"Building monotonic alignment module...\")\n",
        "!bash build_monotonic_align.sh\n",
        "\n",
        "print(\"\\nâœ… Piper TTS installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqafKCU6FJXw",
        "outputId": "4996f446-821f-4c6a-dbe0-b63d53b2e084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnxscript\n",
            "  Downloading onnxscript-0.5.6-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from onnxscript) (2.0.2)\n",
            "Collecting onnx_ir<2,>=0.1.12 (from onnxscript)\n",
            "  Downloading onnx_ir-0.1.12-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: onnx>=1.16 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (1.20.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxscript) (25.0)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (4.15.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.16->onnxscript) (5.29.5)\n",
            "Downloading onnxscript-0.5.6-py3-none-any.whl (683 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m683.0/683.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx_ir-0.1.12-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx_ir, onnxscript\n",
            "Successfully installed onnx_ir-0.1.12 onnxscript-0.5.6\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# BUILD NATIVE EXTENSIONS\n",
        "# =============================================================================\n",
        "# Build the eSpeak bridge and other native extensions.\n",
        "\n",
        "%cd {config.piper_dir}\n",
        "\n",
        "print(\"Installing scikit-build...\")\n",
        "!pip install scikit-build\n",
        "\n",
        "print(\"Building native extensions...\")\n",
        "!python3 setup.py build_ext --inplace\n",
        "\n",
        "print(\"Installing additional dependencies...\")\n",
        "!pip install onnxscript\n",
        "\n",
        "print(\"\\nâœ… Native extensions built successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset-header"
      },
      "source": [
        "# ğŸ“¥ **4. Data ETL (Extract, Transform, Load)** ğŸ“¥\n",
        "\n",
        "Download and process the dataset. Supports both Google Drive (COLAB mode) and AWS S3 (Production mode)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dataset-config",
        "outputId": "b11bb456-6029-4466-f413-94264e96dc2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset path: /content/drive/MyDrive/Piper-POC-Training/\n",
            "Output path: /content/drive/MyDrive/Piper-POC-Training/output\n",
            "Local dataset dir: /content/dataset\n",
            "Local output dir: /content/output/en_IN-spicor-medium\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# DATA LOADER CLASSES\n",
        "# =============================================================================\n",
        "# Abstract data loading with support for both Google Drive and S3.\n",
        "\n",
        "import os\n",
        "import wave\n",
        "import zipfile\n",
        "import datetime\n",
        "import shutil\n",
        "import logging\n",
        "from abc import ABC, abstractmethod\n",
        "from pathlib import Path\n",
        "from typing import Tuple, List, Optional\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class DataLoaderBase(ABC):\n",
        "    \"\"\"Abstract base class for data loading operations.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: PiperConfig):\n",
        "        self.config = config\n",
        "    \n",
        "    @abstractmethod\n",
        "    def download_dataset(self) -> int:\n",
        "        \"\"\"Download dataset to local directory. Returns number of files downloaded.\"\"\"\n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def upload_checkpoint(self, local_path: str, remote_key: str) -> bool:\n",
        "        \"\"\"Upload checkpoint to remote storage. Returns success status.\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class GoogleDriveDataLoader(DataLoaderBase):\n",
        "    \"\"\"Data loader for Google Drive (COLAB mode).\"\"\"\n",
        "    \n",
        "    def __init__(self, config: PiperConfig):\n",
        "        super().__init__(config)\n",
        "        if not config.colab_mode:\n",
        "            raise ValueError(\"GoogleDriveDataLoader requires COLAB mode (config.colab_mode=True)\")\n",
        "    \n",
        "    def download_dataset(self) -> int:\n",
        "        \"\"\"\n",
        "        Copy dataset from Google Drive to local directory.\n",
        "        \n",
        "        Expected folder structure in Google Drive:\n",
        "            your_dataset_folder/\n",
        "            â”œâ”€â”€ wavs/           # Audio files (WAV format, 22050Hz recommended)\n",
        "            â”‚   â”œâ”€â”€ 1.wav\n",
        "            â”‚   â”œâ”€â”€ 2.wav\n",
        "            â”‚   â””â”€â”€ ...\n",
        "            â””â”€â”€ metadata.csv    # Transcript file (format: wavs/filename.wav|text)\n",
        "        \n",
        "        Returns:\n",
        "            int: Number of audio files copied\n",
        "        \"\"\"\n",
        "        gdrive_path = self.config.gdrive_dataset_path.strip()\n",
        "        \n",
        "        # Validate source path exists\n",
        "        if not os.path.exists(gdrive_path):\n",
        "            raise FileNotFoundError(f\"Dataset folder not found in Google Drive: {gdrive_path}\")\n",
        "        \n",
        "        # Check for wavs folder or zip file\n",
        "        gdrive_wavs_path = os.path.join(gdrive_path, \"wavs\")\n",
        "        gdrive_wavs_zip = os.path.join(gdrive_path, \"wavs.zip\")\n",
        "        \n",
        "        if os.path.exists(gdrive_wavs_zip):\n",
        "            logger.info(\"Found wavs.zip, extracting...\")\n",
        "            with zipfile.ZipFile(gdrive_wavs_zip, 'r') as zip_ref:\n",
        "                zip_ref.extractall(self.config.local_dataset_dir)\n",
        "            \n",
        "            # Handle nested wavs folder if present\n",
        "            nested_wavs = f\"{self.config.local_dataset_dir}/wavs/wavs\"\n",
        "            if os.path.exists(nested_wavs):\n",
        "                for f in os.listdir(nested_wavs):\n",
        "                    shutil.move(f\"{nested_wavs}/{f}\", f\"{self.config.local_wavs_dir}/{f}\")\n",
        "                    \n",
        "        elif os.path.exists(gdrive_wavs_path):\n",
        "            logger.info(\"Found wavs folder, copying...\")\n",
        "            # Use shell copy for better performance with large datasets\n",
        "            os.system(f'cp -r \"{gdrive_wavs_path}\"/* \"{self.config.local_wavs_dir}/\"')\n",
        "        else:\n",
        "            raise FileNotFoundError(\n",
        "                f\"No 'wavs' folder or 'wavs.zip' found in {gdrive_path}\\n\"\n",
        "                \"Expected structure:\\n\"\n",
        "                \"  your_dataset_folder/\\n\"\n",
        "                \"  â”œâ”€â”€ wavs/\\n\"\n",
        "                \"  â”‚   â”œâ”€â”€ 1.wav\\n\"\n",
        "                \"  â”‚   â””â”€â”€ ...\\n\"\n",
        "                \"  â””â”€â”€ metadata.csv\"\n",
        "            )\n",
        "        \n",
        "        # Clean up macOS ghost files\n",
        "        self._cleanup_ghost_files(self.config.local_wavs_dir)\n",
        "        \n",
        "        # Count files\n",
        "        audio_count = len([f for f in os.listdir(self.config.local_wavs_dir) if f.endswith('.wav')])\n",
        "        logger.info(f\"Copied {audio_count} audio files from Google Drive\")\n",
        "        \n",
        "        return audio_count\n",
        "    \n",
        "    def upload_checkpoint(self, local_path: str, remote_key: str) -> bool:\n",
        "        \"\"\"\n",
        "        Copy checkpoint to Google Drive.\n",
        "        \n",
        "        Args:\n",
        "            local_path: Path to local checkpoint file\n",
        "            remote_key: Relative path within gdrive_output_path\n",
        "            \n",
        "        Returns:\n",
        "            bool: Success status\n",
        "        \"\"\"\n",
        "        try:\n",
        "            dest_path = os.path.join(self.config.gdrive_output_path, remote_key)\n",
        "            Path(dest_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy(local_path, dest_path)\n",
        "            logger.info(f\"Uploaded checkpoint to Google Drive: {dest_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to upload checkpoint: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def _cleanup_ghost_files(self, directory: str) -> None:\n",
        "        \"\"\"Delete macOS ghost files (._*) from directory.\"\"\"\n",
        "        count = 0\n",
        "        for filename in os.listdir(directory):\n",
        "            if filename.startswith(\"._\"):\n",
        "                file_path = os.path.join(directory, filename)\n",
        "                os.remove(file_path)\n",
        "                count += 1\n",
        "        if count > 0:\n",
        "            logger.info(f\"Cleaned up {count} macOS artifact files.\")\n",
        "\n",
        "\n",
        "class S3DataLoader(DataLoaderBase):\n",
        "    \"\"\"Data loader for AWS S3 (Production mode).\"\"\"\n",
        "    \n",
        "    def __init__(self, config: PiperConfig):\n",
        "        super().__init__(config)\n",
        "        if config.colab_mode:\n",
        "            raise ValueError(\"S3DataLoader requires AWS mode (config.colab_mode=False)\")\n",
        "        \n",
        "        self.s3_client = self._create_s3_client()\n",
        "    \n",
        "    def _create_s3_client(self):\n",
        "        \"\"\"Create S3 client with optional credentials.\"\"\"\n",
        "        try:\n",
        "            import boto3\n",
        "            from botocore.exceptions import ClientError\n",
        "        except ImportError:\n",
        "            raise ImportError(\"boto3 is required for S3 operations. Install with: pip install boto3\")\n",
        "        \n",
        "        kwargs = {'region_name': self.config.aws_region}\n",
        "        if self.config.aws_access_key_id and self.config.aws_secret_access_key:\n",
        "            kwargs['aws_access_key_id'] = self.config.aws_access_key_id\n",
        "            kwargs['aws_secret_access_key'] = self.config.aws_secret_access_key\n",
        "        \n",
        "        return boto3.client('s3', **kwargs)\n",
        "    \n",
        "    def _list_objects(self, prefix: str) -> List[str]:\n",
        "        \"\"\"List all objects under a prefix.\"\"\"\n",
        "        objects = []\n",
        "        paginator = self.s3_client.get_paginator('list_objects_v2')\n",
        "        for page in paginator.paginate(Bucket=self.config.s3_bucket, Prefix=prefix):\n",
        "            if 'Contents' in page:\n",
        "                objects.extend([obj['Key'] for obj in page['Contents']])\n",
        "        return objects\n",
        "    \n",
        "    def _download_file(self, s3_key: str, local_path: str) -> bool:\n",
        "        \"\"\"Download a single file from S3.\"\"\"\n",
        "        from botocore.exceptions import ClientError\n",
        "        try:\n",
        "            Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "            self.s3_client.download_file(self.config.s3_bucket, s3_key, local_path)\n",
        "            return True\n",
        "        except ClientError as e:\n",
        "            logger.error(f\"Failed to download {s3_key}: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def download_dataset(self) -> int:\n",
        "        \"\"\"\n",
        "        Download dataset from S3 to local directory.\n",
        "        \n",
        "        Returns:\n",
        "            int: Number of files downloaded\n",
        "        \"\"\"\n",
        "        from tqdm import tqdm\n",
        "        \n",
        "        prefix = self.config.s3_dataset_prefix\n",
        "        objects = self._list_objects(prefix)\n",
        "        \n",
        "        if not objects:\n",
        "            raise FileNotFoundError(\n",
        "                f\"No objects found in s3://{self.config.s3_bucket}/{prefix}\\n\"\n",
        "                \"Please verify the S3 bucket and prefix are correct.\"\n",
        "            )\n",
        "        \n",
        "        downloaded = 0\n",
        "        for s3_key in tqdm(objects, desc=\"Downloading dataset from S3\"):\n",
        "            relative_path = s3_key[len(prefix):].lstrip('/')\n",
        "            local_path = os.path.join(self.config.local_dataset_dir, relative_path)\n",
        "            if self._download_file(s3_key, local_path):\n",
        "                downloaded += 1\n",
        "        \n",
        "        logger.info(f\"Downloaded {downloaded} files from S3\")\n",
        "        return downloaded\n",
        "    \n",
        "    def upload_checkpoint(self, local_path: str, remote_key: str) -> bool:\n",
        "        \"\"\"\n",
        "        Upload checkpoint to S3.\n",
        "        \n",
        "        Args:\n",
        "            local_path: Path to local checkpoint file\n",
        "            remote_key: S3 key (relative to s3_checkpoint_prefix)\n",
        "            \n",
        "        Returns:\n",
        "            bool: Success status\n",
        "        \"\"\"\n",
        "        from botocore.exceptions import ClientError\n",
        "        try:\n",
        "            s3_key = f\"{self.config.s3_checkpoint_prefix}/{remote_key}\"\n",
        "            self.s3_client.upload_file(local_path, self.config.s3_bucket, s3_key)\n",
        "            logger.info(f\"Uploaded checkpoint to s3://{self.config.s3_bucket}/{s3_key}\")\n",
        "            return True\n",
        "        except ClientError as e:\n",
        "            logger.error(f\"Failed to upload {local_path}: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "def get_data_loader(config: PiperConfig) -> DataLoaderBase:\n",
        "    \"\"\"\n",
        "    Factory function to get the appropriate data loader based on configuration.\n",
        "    \n",
        "    Args:\n",
        "        config: PiperConfig instance\n",
        "        \n",
        "    Returns:\n",
        "        DataLoaderBase: Appropriate data loader instance\n",
        "    \"\"\"\n",
        "    if config.colab_mode:\n",
        "        return GoogleDriveDataLoader(config)\n",
        "    else:\n",
        "        return S3DataLoader(config)\n",
        "\n",
        "\n",
        "# Initialize data loader\n",
        "data_loader = get_data_loader(config)\n",
        "print(f\"âœ… Data loader initialized: {type(data_loader).__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lHwXnPMDZVQ"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DOWNLOAD DATASET\n",
        "# =============================================================================\n",
        "# Download/copy dataset from remote storage to local directory.\n",
        "\n",
        "print(\"Downloading dataset...\")\n",
        "audio_count = data_loader.download_dataset()\n",
        "print(f\"\\nâœ… Dataset loaded: {audio_count} audio files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "extract-dataset",
        "outputId": "d6dccbb9-9408-42f7-ca0d-fb00c6711caa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found wavs.zip, extracting...\n",
            "\n",
            "âœ… Dataset loaded: 10 audio files, total duration: 0:01:24\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# DATASET STATISTICS UTILITY\n",
        "# =============================================================================\n",
        "# Calculate and display dataset statistics.\n",
        "\n",
        "import wave\n",
        "import datetime\n",
        "\n",
        "def get_dataset_duration(wav_path: str) -> Tuple[int, str]:\n",
        "    \"\"\"\n",
        "    Calculate total duration of all WAV files in a directory.\n",
        "    \n",
        "    Args:\n",
        "        wav_path: Path to directory containing WAV files\n",
        "        \n",
        "    Returns:\n",
        "        Tuple[int, str]: (count of files, formatted duration string)\n",
        "    \"\"\"\n",
        "    total_duration = 0.0\n",
        "    total_count = 0\n",
        "    \n",
        "    if not os.path.exists(wav_path):\n",
        "        raise FileNotFoundError(f\"WAV directory not found: {wav_path}\")\n",
        "    \n",
        "    for file_name in os.listdir(wav_path):\n",
        "        if not file_name.endswith(\".wav\"):\n",
        "            continue\n",
        "        \n",
        "        full_path = os.path.join(wav_path, file_name)\n",
        "        try:\n",
        "            with wave.open(full_path, \"rb\") as wave_file:\n",
        "                frames = wave_file.getnframes()\n",
        "                rate = wave_file.getframerate()\n",
        "                duration = frames / float(rate)\n",
        "                total_duration += duration\n",
        "                total_count += 1\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Skipping bad file {file_name}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    duration_str = str(datetime.timedelta(seconds=round(total_duration, 0)))\n",
        "    return total_count, duration_str\n",
        "\n",
        "\n",
        "# Calculate and display dataset statistics\n",
        "audio_count, dataset_duration = get_dataset_duration(config.local_wavs_dir)\n",
        "print(f\"\\nğŸ“Š Dataset Statistics:\")\n",
        "print(f\"   Audio files: {audio_count}\")\n",
        "print(f\"   Total duration: {dataset_duration}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "yoPJDlnZD7aw",
        "outputId": "c3170008-36f3-4686-9a63-d9e539c5ba35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found transcript file: transcripts.txt in local wavs directory.\n",
            "Applying text normalization to transcripts.txt and saving to /content/dataset/metadata.csv\n",
            "Processed 10 lines for text normalization.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"display(df_metadata\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"They are also used in autism therapy which helps children achieve greater engagement, alertness, balance, language and processing skills.\",\n          \"No F O Bs in Cotton Green railway station, passengers risk lives to move from one platform to other.\",\n          \"Agriculturist Machaiah from Pulikotu village said that the Horticulture department has been distributing orange saplings to farmers at subsidised rates.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-22072bc9-44f5-4c5d-874d-4d1ca7f97866\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Vegetables are exported to all the areas like ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>They are also used in autism therapy which hel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Agriculturist Machaiah from Pulikotu village s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Yo Yo Honey Singh cheering up a singer in Delh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>No F O Bs in Cotton Green railway station, pas...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22072bc9-44f5-4c5d-874d-4d1ca7f97866')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-22072bc9-44f5-4c5d-874d-4d1ca7f97866 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-22072bc9-44f5-4c5d-874d-4d1ca7f97866');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7516612c-30be-417e-92e3-e2f4bf893a15\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7516612c-30be-417e-92e3-e2f4bf893a15')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7516612c-30be-417e-92e3-e2f4bf893a15 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   filename                                               text\n",
              "0         1  Vegetables are exported to all the areas like ...\n",
              "1         2  They are also used in autism therapy which hel...\n",
              "2         3  Agriculturist Machaiah from Pulikotu village s...\n",
              "3         4  Yo Yo Honey Singh cheering up a singer in Delh...\n",
              "4         5  No F O Bs in Cotton Green railway station, pas..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TEXT NORMALIZATION AND TRANSCRIPT PROCESSING\n",
        "# =============================================================================\n",
        "# Process transcript file with text normalization protocols.\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class TranscriptProcessor:\n",
        "    \"\"\"\n",
        "    Process transcript files with text normalization.\n",
        "    \n",
        "    Applies normalization protocols:\n",
        "    1. Orthographic expansion of non-standard words (currency, percentages, symbols)\n",
        "    2. Acronym handling (spacing out consecutive capitals)\n",
        "    3. Punctuation and prosodic boundaries (ensure terminal punctuation)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Supported transcript file names\n",
        "    TRANSCRIPT_FILES = [\"metadata.csv\", \"transcripts.txt\", \"transcript.txt\", \"metadata.txt\"]\n",
        "    \n",
        "    @staticmethod\n",
        "    def clean_line(text: str) -> str:\n",
        "        \"\"\"\n",
        "        Apply text normalization protocols to a single line of text.\n",
        "        \n",
        "        Args:\n",
        "            text: Input text line\n",
        "            \n",
        "        Returns:\n",
        "            str: Normalized text\n",
        "        \"\"\"\n",
        "        # --- Protocol 1: Orthographic Expansion ---\n",
        "        # Currency: $50 -> 50 dollars\n",
        "        text = re.sub(r'\\$(\\d+(?:\\.\\d+)?)', r'\\1 dollars', text)\n",
        "        \n",
        "        # Percentages: 50% -> 50 percent\n",
        "        text = re.sub(r'(\\d+)%', r'\\1 percent', text)\n",
        "        \n",
        "        # Ampersand: & -> and\n",
        "        text = text.replace('&', ' and ')\n",
        "        \n",
        "        # Plus sign: + -> plus\n",
        "        text = text.replace('+', ' plus ')\n",
        "        \n",
        "        # --- Protocol 2: Acronym Handling ---\n",
        "        # Space out consecutive capitals: \"IAS\" -> \"I A S\"\n",
        "        def space_acronym(match):\n",
        "            return \" \".join(match.group(1))\n",
        "        text = re.sub(r'\\b([A-Z]{2,})\\b', space_acronym, text)\n",
        "        \n",
        "        # --- Protocol 3: Punctuation and Prosodic Boundaries ---\n",
        "        text = text.strip()\n",
        "        if text and text[-1] not in ['.', '!', '?']:\n",
        "            text += '.'\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    @classmethod\n",
        "    def process_file(cls, input_filename: str, output_filename: str) -> int:\n",
        "        \"\"\"\n",
        "        Process transcript file: read, normalize each line, and write to output.\n",
        "        \n",
        "        Args:\n",
        "            input_filename: Path to input transcript file\n",
        "            output_filename: Path to output processed file\n",
        "            \n",
        "        Returns:\n",
        "            int: Number of lines processed\n",
        "            \n",
        "        Raises:\n",
        "            FileNotFoundError: If input file doesn't exist\n",
        "            ValueError: If file contains no valid entries\n",
        "        \"\"\"\n",
        "        if not os.path.exists(input_filename):\n",
        "            raise FileNotFoundError(f\"Transcript file not found: {input_filename}\")\n",
        "        \n",
        "        lines_processed = 0\n",
        "        \n",
        "        with open(input_filename, 'r', encoding='utf-8') as infile, \\\n",
        "             open(output_filename, 'w', encoding='utf-8') as outfile:\n",
        "            \n",
        "            for line in infile:\n",
        "                if '|' in line:\n",
        "                    parts = line.strip().split('|', 1)\n",
        "                    if len(parts) == 2:\n",
        "                        file_id, original_text = parts\n",
        "                        cleaned_text = cls.clean_line(original_text)\n",
        "                        outfile.write(f\"{file_id}|{cleaned_text}\\n\")\n",
        "                        lines_processed += 1\n",
        "                    else:\n",
        "                        logger.warning(f\"Skipping malformed line (missing text after '|'): {line.strip()}\")\n",
        "                else:\n",
        "                    logger.warning(f\"Skipping non-ID|Text line: {line.strip()}\")\n",
        "        \n",
        "        if lines_processed == 0:\n",
        "            raise ValueError(\n",
        "                f\"No valid transcript entries found in {input_filename}.\\n\"\n",
        "                \"Expected format: filename.wav|transcript text\"\n",
        "            )\n",
        "        \n",
        "        logger.info(f\"Processed {lines_processed} lines for text normalization.\")\n",
        "        return lines_processed\n",
        "    \n",
        "    @classmethod\n",
        "    def find_and_process_transcript(cls, wavs_dir: str, output_dir: str) -> str:\n",
        "        \"\"\"\n",
        "        Find transcript file in wavs directory and process it.\n",
        "        \n",
        "        Args:\n",
        "            wavs_dir: Directory containing wavs and transcript file\n",
        "            output_dir: Directory to write processed metadata.csv\n",
        "            \n",
        "        Returns:\n",
        "            str: Path to processed metadata.csv\n",
        "            \n",
        "        Raises:\n",
        "            FileNotFoundError: If no transcript file is found\n",
        "        \"\"\"\n",
        "        source_path = None\n",
        "        \n",
        "        for tf in cls.TRANSCRIPT_FILES:\n",
        "            current_attempt = os.path.join(wavs_dir, tf)\n",
        "            if os.path.exists(current_attempt):\n",
        "                logger.info(f\"Found transcript file: {tf}\")\n",
        "                source_path = current_attempt\n",
        "                break\n",
        "        \n",
        "        if source_path is None:\n",
        "            raise FileNotFoundError(\n",
        "                f\"No transcript file found in {wavs_dir}.\\n\"\n",
        "                f\"Expected one of: {cls.TRANSCRIPT_FILES}\"\n",
        "            )\n",
        "        \n",
        "        output_path = os.path.join(output_dir, \"metadata.csv\")\n",
        "        logger.info(f\"Processing transcript: {source_path} -> {output_path}\")\n",
        "        cls.process_file(source_path, output_path)\n",
        "        \n",
        "        return output_path\n",
        "\n",
        "\n",
        "# Process transcript file\n",
        "metadata_csv_path = TranscriptProcessor.find_and_process_transcript(\n",
        "    config.local_wavs_dir,\n",
        "    config.local_dataset_dir\n",
        ")\n",
        "\n",
        "# Display processed metadata\n",
        "df_metadata = pd.read_csv(metadata_csv_path, sep='|', header=None, names=['filename', 'text'])\n",
        "print(f\"\\nâœ… Processed transcript: {len(df_metadata)} entries\")\n",
        "print(\"\\nğŸ“‹ Sample entries:\")\n",
        "display(df_metadata.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training-header"
      },
      "source": [
        "# ğŸ¤– **5. Training** ğŸ¤–\n",
        "\n",
        "Configure and run the Piper TTS fine-tuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "training-config",
        "outputId": "607af939-4abb-4105-db7f-83e8d6540758"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language: en-us\n",
            "Sample rate: 22050\n",
            "Batch size: 8\n",
            "Max epochs: 4000\n",
            "Validation split: 0.0\n",
            "Use pretrained: True\n",
            "Resume training: False\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CHECKPOINT MANAGER\n",
        "# =============================================================================\n",
        "# Manage pretrained checkpoints from Hugging Face.\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "\n",
        "class CheckpointManager:\n",
        "    \"\"\"Manage model checkpoints for training and fine-tuning.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: PiperConfig):\n",
        "        self.config = config\n",
        "        self.pretrained_ckpt_path = os.path.join(config.base_dir, \"pretrained.ckpt\")\n",
        "    \n",
        "    def download_pretrained_checkpoint(self) -> str:\n",
        "        \"\"\"\n",
        "        Download pretrained checkpoint from Hugging Face.\n",
        "        \n",
        "        Returns:\n",
        "            str: Path to downloaded checkpoint\n",
        "            \n",
        "        Raises:\n",
        "            RuntimeError: If download fails\n",
        "        \"\"\"\n",
        "        if os.path.exists(self.pretrained_ckpt_path):\n",
        "            logger.info(f\"Pretrained checkpoint already exists: {self.pretrained_ckpt_path}\")\n",
        "            return self.pretrained_ckpt_path\n",
        "        \n",
        "        logger.info(\"Downloading pretrained checkpoint from Hugging Face...\")\n",
        "        \n",
        "        try:\n",
        "            downloaded_path = hf_hub_download(\n",
        "                repo_id=self.config.hf_checkpoint_repo,\n",
        "                filename=self.config.hf_checkpoint_path,\n",
        "                repo_type=\"dataset\",\n",
        "                local_dir=os.path.join(self.config.base_dir, \"checkpoints\"),\n",
        "            )\n",
        "            \n",
        "            # Also download the config for reference\n",
        "            hf_hub_download(\n",
        "                repo_id=self.config.hf_checkpoint_repo,\n",
        "                filename=self.config.hf_config_path,\n",
        "                repo_type=\"dataset\",\n",
        "                local_dir=os.path.join(self.config.base_dir, \"checkpoints\"),\n",
        "            )\n",
        "            \n",
        "            # Copy to expected location\n",
        "            shutil.copy(downloaded_path, self.pretrained_ckpt_path)\n",
        "            \n",
        "            logger.info(f\"Pretrained checkpoint downloaded to: {self.pretrained_ckpt_path}\")\n",
        "            return self.pretrained_ckpt_path\n",
        "            \n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to download pretrained checkpoint: {e}\")\n",
        "    \n",
        "    def find_latest_checkpoint(self, checkpoint_dir: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Find the latest checkpoint in a directory for resuming training.\n",
        "        \n",
        "        Args:\n",
        "            checkpoint_dir: Directory to search for checkpoints\n",
        "            \n",
        "        Returns:\n",
        "            Optional[str]: Path to latest checkpoint, or None if not found\n",
        "        \"\"\"\n",
        "        import glob\n",
        "        import re\n",
        "        \n",
        "        # Look for 'last.ckpt' first\n",
        "        last_ckpt = os.path.join(checkpoint_dir, \"last.ckpt\")\n",
        "        if os.path.exists(last_ckpt):\n",
        "            return last_ckpt\n",
        "        \n",
        "        # Find most recent checkpoint by modification time\n",
        "        checkpoints = glob.glob(f\"{checkpoint_dir}/**/*.ckpt\", recursive=True)\n",
        "        if checkpoints:\n",
        "            latest = max(checkpoints, key=os.path.getmtime)\n",
        "            return latest\n",
        "        \n",
        "        return None\n",
        "    \n",
        "    def upgrade_checkpoint_for_cpu(self, ckpt_path: str) -> None:\n",
        "        \"\"\"\n",
        "        Upgrade checkpoint for CPU compatibility (PyTorch Lightning upgrade).\n",
        "        \n",
        "        Args:\n",
        "            ckpt_path: Path to checkpoint file\n",
        "        \"\"\"\n",
        "        import torch\n",
        "        import pathlib\n",
        "        from argparse import Namespace\n",
        "        from lightning.pytorch.utilities.upgrade_checkpoint import _upgrade\n",
        "        \n",
        "        logger.info(f\"Upgrading checkpoint for compatibility: {ckpt_path}\")\n",
        "        \n",
        "        with torch.serialization.safe_globals([pathlib.PosixPath]):\n",
        "            args = Namespace(path=str(ckpt_path), extension=\".ckpt\", map_to_cpu=True)\n",
        "            _upgrade(args)\n",
        "        \n",
        "        logger.info(\"Checkpoint upgrade complete.\")\n",
        "\n",
        "\n",
        "# Initialize checkpoint manager\n",
        "ckpt_manager = CheckpointManager(config)\n",
        "print(\"âœ… Checkpoint manager initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135,
          "referenced_widgets": [
            "d76c9fdd465444a7a49dfbadec82deec",
            "c0a5664906ae4da9bfb8fb0243bb709b",
            "ecd1eb41504a4dbe8e370cd639cc8841",
            "b37cbe17bce44398931fa0922927bf02",
            "6f1c8f47337d44129edd8b236d31f816",
            "7148e7a1e4f54aaa8bccad9bbe512ecb",
            "1c9a7e1d091041bf8f00e6a00e77c066",
            "5a33fa8f5b7d4ef5accfe5948c735260",
            "66252483676a45ceba797b73d49850e5",
            "a30bbf28c28347d5bf860559e625dec0",
            "8fd4a35190f94317a55614f5deadd200",
            "b596459cae3e4d4ba2bdd11e7c6f20e3",
            "1f919d5117704008886f31c8bc4e381c",
            "5e8549946201493dae02f115c0d1bb52",
            "fafa6ccf3e294292adb35a874b5a9ac2",
            "b46ae31ffd85473598069efe0432d2d2",
            "6b1ca79243bc4c2d834bfe6107407fc9",
            "9280e01c53864f3283ba04f9d02f5f19",
            "a544bd709214476e95b164e124684dbe",
            "8a3c0bdb45774a7799b1f319f1a74378",
            "6d4b31e6470a4719a7a4b1e91b9f3a41",
            "2be4a1bd326048079f6d6f07cc61fdee"
          ]
        },
        "id": "download-pretrained",
        "outputId": "ae440532-0be7-4b2f-d2b0-196ee7d25018"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading pretrained checkpoint from Hugging Face...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d76c9fdd465444a7a49dfbadec82deec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "en/en_US/ljspeech/high/ljspeech-2000.ckp(â€¦):   0%|          | 0.00/998M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b596459cae3e4d4ba2bdd11e7c6f20e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Pretrained checkpoint downloaded to: /content/pretrained.ckpt\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# DOWNLOAD PRETRAINED CHECKPOINT\n",
        "# =============================================================================\n",
        "# Download pretrained checkpoint from Hugging Face for fine-tuning.\n",
        "\n",
        "if config.use_pretrained and not config.resume_training:\n",
        "    pretrained_ckpt_path = ckpt_manager.download_pretrained_checkpoint()\n",
        "    \n",
        "    # Upgrade checkpoint for compatibility\n",
        "    ckpt_manager.upgrade_checkpoint_for_cpu(pretrained_ckpt_path)\n",
        "    \n",
        "    print(f\"\\nâœ… Pretrained checkpoint ready: {pretrained_ckpt_path}\")\n",
        "    \n",
        "elif config.resume_training:\n",
        "    print(\"Resume training mode - will look for existing checkpoint in output folder.\")\n",
        "else:\n",
        "    print(\"Training from scratch (no pretrained checkpoint).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaUcwgCk88Px",
        "outputId": "18f47b02-d8a0-4637-fd77-004b678ee24d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Creating a backup of the existing checkpoint files before overwriting in the upgrade process.\n",
            "INFO:lightning.pytorch.utilities.upgrade_checkpoint:Creating a backup of the existing checkpoint files before overwriting in the upgrade process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to upgrade: /content/pretrained.ckpt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Upgrading checkpoints ...\n",
            "INFO:lightning.pytorch.utilities.upgrade_checkpoint:Upgrading checkpoints ...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.77s/it]\n",
            "INFO: Done.\n",
            "INFO:lightning.pytorch.utilities.upgrade_checkpoint:Done.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upgrade process complete.\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# LAUNCH TENSORBOARD\n",
        "# =============================================================================\n",
        "# TensorBoard allows monitoring training progress in real-time.\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {config.local_output_dir}\n",
        "\n",
        "print(\"âœ… TensorBoard launched. Monitor training progress above.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilFk_zBY_QDq"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# START TRAINING\n",
        "# =============================================================================\n",
        "# Run the Piper TTS training process.\n",
        "\n",
        "import glob\n",
        "import re\n",
        "\n",
        "# Change to piper directory\n",
        "%cd {config.piper_dir}\n",
        "\n",
        "# Prepare paths\n",
        "csv_path = metadata_csv_path\n",
        "config_path = f\"{config.local_output_dir}/{config.model_name}.json\"\n",
        "\n",
        "# Determine checkpoint path argument\n",
        "if config.resume_training:\n",
        "    # Look for existing checkpoint\n",
        "    checkpoints = glob.glob(f\"{config.local_output_dir}/lightning_logs/**/checkpoints/last.ckpt\", recursive=True)\n",
        "    if checkpoints:\n",
        "        # Sort by version number to get the latest\n",
        "        def get_version(path):\n",
        "            match = re.findall(r'version_(\\d+)', path)\n",
        "            return int(match[0]) if match else 0\n",
        "        latest_ckpt = sorted(checkpoints, key=get_version)[-1]\n",
        "        print(f\"Resuming from checkpoint: {latest_ckpt}\")\n",
        "        ckpt_path_arg = f'--ckpt_path \"{latest_ckpt}\"'\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"resume_training=True but no checkpoint found in {config.local_output_dir}/lightning_logs/\\n\"\n",
        "            \"Set resume_training=False to start fresh or ensure checkpoints exist.\"\n",
        "        )\n",
        "elif config.use_pretrained:\n",
        "    ckpt_url = f\"https://huggingface.co/datasets/{config.hf_checkpoint_repo}/blob/main/{config.hf_checkpoint_path}\"\n",
        "    print(f\"Fine-tuning from pretrained checkpoint: {ckpt_url}\")\n",
        "    ckpt_path_arg = f'--ckpt_path \"{ckpt_url}\"'\n",
        "else:\n",
        "    ckpt_path_arg = \"\"\n",
        "    print(\"Training from scratch (no checkpoint).\")\n",
        "\n",
        "# Build training command\n",
        "train_cmd = f\"\"\"\n",
        "python -m piper.train fit \\\\\n",
        "    --data.csv_path \"{csv_path}\" \\\\\n",
        "    --data.cache_dir \"{config.local_cache_dir}\" \\\\\n",
        "    --data.audio_dir \"{config.local_wavs_dir}\" \\\\\n",
        "    --data.espeak_voice \"{config.espeak_voice}\" \\\\\n",
        "    --data.config_path \"{config_path}\" \\\\\n",
        "    --data.voice_name \"{config.model_name}\" \\\\\n",
        "    --data.batch_size {config.batch_size} \\\\\n",
        "    --data.validation_split {config.validation_split} \\\\\n",
        "    --data.num_test_examples {config.num_test_examples} \\\\\n",
        "    --model.sample_rate {config.sample_rate} \\\\\n",
        "    --model.num_speakers {config.num_speakers} \\\\\n",
        "    --trainer.max_epochs {config.max_epochs} \\\\\n",
        "    --trainer.accelerator {config.device} \\\\\n",
        "    --trainer.devices 1 \\\\\n",
        "    --trainer.precision 32 \\\\\n",
        "    --trainer.default_root_dir \"{config.local_output_dir}\" \\\\\n",
        "    --trainer.callbacks+=ModelCheckpoint \\\\\n",
        "    --trainer.callbacks.dirpath \"{config.local_output_dir}/checkpoints\" \\\\\n",
        "    --trainer.callbacks.filename \"piper-{{epoch:04d}}-{{step:08d}}\" \\\\\n",
        "    --trainer.callbacks.save_top_k 3 \\\\\n",
        "    --trainer.callbacks.monitor \"val_loss\" \\\\\n",
        "    --trainer.callbacks.save_last true \\\\\n",
        "    --trainer.callbacks.every_n_epochs {config.checkpoint_epochs} \\\\\n",
        "    --model.learning_rate {config.learning_rate} \\\\\n",
        "    {ckpt_path_arg}\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Training command:\\n{train_cmd}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "!{train_cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save-header"
      },
      "source": [
        "# ğŸ’¾ **6. Save Training Outputs** ğŸ’¾\n",
        "\n",
        "Save trained model checkpoints and configuration to remote storage (Google Drive or S3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "save-to-drive",
        "outputId": "17b99d94-df00-48b1-d611-f20c3c6e5c55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copied lightning_logs to /content/drive/MyDrive/Piper-POC-Training/output/en_IN-spicor-medium\n",
            "Copying /content/output/en_IN-spicor-medium/checkpoints/last.ckpt -> /content/drive/MyDrive/Piper-POC-Training/output/en_IN-spicor-medium/last.ckpt\n",
            "Copying /content/output/en_IN-spicor-medium/en_IN-spicor-medium.json -> /content/drive/MyDrive/Piper-POC-Training/output/en_IN-spicor-medium/en_IN-spicor-medium.json\n",
            "\n",
            "âœ… Model saved to Google Drive: /content/drive/MyDrive/Piper-POC-Training/output/en_IN-spicor-medium\n",
            "\n",
            "Files saved:\n",
            "  - lightning_logs (0.00 MB)\n",
            "  - last.ckpt (806.70 MB)\n",
            "  - en_IN-spicor-medium.json (0.00 MB)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# SAVE TRAINING OUTPUTS TO REMOTE STORAGE\n",
        "# =============================================================================\n",
        "# Copy trained model, checkpoints, and logs to Google Drive or S3.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "def save_training_outputs(config: PiperConfig, data_loader: DataLoaderBase) -> None:\n",
        "    \"\"\"\n",
        "    Save all training outputs to remote storage.\n",
        "    \n",
        "    Args:\n",
        "        config: PiperConfig instance\n",
        "        data_loader: DataLoader instance for upload operations\n",
        "    \"\"\"\n",
        "    if config.colab_mode:\n",
        "        # Create model-specific output directory in Google Drive\n",
        "        gdrive_model_dir = os.path.join(config.gdrive_output_path, config.model_name)\n",
        "        os.makedirs(gdrive_model_dir, exist_ok=True)\n",
        "        \n",
        "        files_to_copy = []\n",
        "        \n",
        "        # Copy last checkpoint\n",
        "        last_ckpt = f\"{config.local_output_dir}/checkpoints/last.ckpt\"\n",
        "        if os.path.exists(last_ckpt):\n",
        "            files_to_copy.append((last_ckpt, f\"{gdrive_model_dir}/last.ckpt\"))\n",
        "        else:\n",
        "            logger.warning(f\"Last checkpoint not found: {last_ckpt}\")\n",
        "        \n",
        "        # Copy config file\n",
        "        config_file = f\"{config.local_output_dir}/{config.model_name}.json\"\n",
        "        if os.path.exists(config_file):\n",
        "            files_to_copy.append((config_file, f\"{gdrive_model_dir}/{config.model_name}.json\"))\n",
        "        else:\n",
        "            logger.warning(f\"Config file not found: {config_file}\")\n",
        "        \n",
        "        # Copy lightning logs directory\n",
        "        lightning_logs = f\"{config.local_output_dir}/lightning_logs\"\n",
        "        if os.path.exists(lightning_logs):\n",
        "            shutil.copytree(lightning_logs, f\"{gdrive_model_dir}/lightning_logs\", dirs_exist_ok=True)\n",
        "            logger.info(f\"Copied lightning_logs to {gdrive_model_dir}\")\n",
        "        \n",
        "        # Copy individual files\n",
        "        for src, dst in files_to_copy:\n",
        "            if os.path.exists(src):\n",
        "                logger.info(f\"Copying {src} -> {dst}\")\n",
        "                shutil.copy(src, dst)\n",
        "        \n",
        "        # Display saved files\n",
        "        print(f\"\\nâœ… Model saved to Google Drive: {gdrive_model_dir}\")\n",
        "        print(\"\\nFiles saved:\")\n",
        "        for f in os.listdir(gdrive_model_dir):\n",
        "            fpath = os.path.join(gdrive_model_dir, f)\n",
        "            if os.path.isfile(fpath):\n",
        "                size_mb = os.path.getsize(fpath) / (1024 * 1024)\n",
        "                print(f\"  - {f} ({size_mb:.2f} MB)\")\n",
        "            else:\n",
        "                print(f\"  - {f}/ (directory)\")\n",
        "    \n",
        "    else:\n",
        "        # Upload to S3\n",
        "        last_ckpt = f\"{config.local_output_dir}/checkpoints/last.ckpt\"\n",
        "        if os.path.exists(last_ckpt):\n",
        "            data_loader.upload_checkpoint(last_ckpt, f\"{config.model_name}/last.ckpt\")\n",
        "        \n",
        "        config_file = f\"{config.local_output_dir}/{config.model_name}.json\"\n",
        "        if os.path.exists(config_file):\n",
        "            data_loader.upload_checkpoint(config_file, f\"{config.model_name}/{config.model_name}.json\")\n",
        "        \n",
        "        print(f\"\\nâœ… Model uploaded to S3: s3://{config.s3_bucket}/{config.s3_checkpoint_prefix}/{config.model_name}/\")\n",
        "\n",
        "\n",
        "# Save training outputs\n",
        "save_training_outputs(config, data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "C_AQElpKNscG",
        "outputId": "8b75bc44-33ed-4a33-c2b2-cfdbd803e272"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Only 'last.ckpt' found. Using: /content/drive/MyDrive/Piper-POC-Training/output/en_IN-spicor-medium_en-US-epochs-3000/last.ckpt\n",
            "WARNING: If this model was trained for 10k epochs on small data, it is likely broken (overfitted).\n",
            "Loading model from /content/pretrained.ckpt...\n",
            "Phonemizing using 'en-us' (Matching training data)...\n",
            "Generated Phonemes (First 10): ['Ã°', 'Éª', 's', ' ', 'Éª', 'z', ' ', 'É', ' ', 't']\n",
            "ID Sequence (First 20): [1, 41, 1, 74, 1, 31, 1, 3, 1, 74, 1, 38, 1, 3, 1, 50, 1, 3, 1, 32]\n",
            "Generating audio...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" >\n",
              "                    <source src=\"data:audio/x-wav;base64,UklGRiTeAQBXQVZFZm10IBAAAAABAAEAIlYAAESsAAACABAAZGF0YQDeAQDO/9n/0v/n/+b/2//X/+L/2f/k/9//0//V/9z/2P/V/+T/1P/f/9b/1//T/9f/5//s/+j/8f/u/+z/8f/3/+7/8f/7//3/BQACAPX//v/5//z/+f/2/wQACwD9/wkADQANAAsACgAUAAgACgAQAAcACQAQAAsAEAANAA8ACwARABMAFAATABMAFAAJABQAFQATAAwAEgATABcAHQAaABwAGAAdAB4AJAAjACkAKwAkACsALwAxADEAKQAtADgAMgAyAEAAOgA6ADIAOgA5ADgAQwBIAEwAPwBMAEsAUQBOAE4AVgBQAEoAUABVAE4AVgBNAFQAWQBhAF8AYABgAGEAagBrAG8AaQBnAHsAdQBsAHIAfACGAHcAhgCBAIMAjACKAI8AlwCTAJIAnQCSAJUAmgCVAI4AnQCTAJUAmgCWAJMAjACSAIcAkACNAI4AhACAAIIAgQB+AHkAeQBzAHcAdQB3AHIAcgBsAGoAcwBrAHIAbQBtAGkAYwBtAGUAYABkAGsAawBsAGsAdABuAGcAaQBuAG0AaABrAGUAbwB1AHQAcQBsAGoAbwB8AHgAcgBwAGcAcQBtAGUAdABzAHEAdQBxAGsAbgBiAHAAbwBpAG0AdQB7AHEAgAB2AHcAfwB8AHQAfgB6AHYAbwBvAHIAeACAAHwAgwB7AHkAdQB2AHQAgwCAAIEAkAB8AJMAkQCbAI0AkwCrAJgAlQCcAKUAogCnAKQAogCoAJ8AoAChAJwAqACjAKgAsACqALkAuAC0ALgAtwDAAMoAvwDDAMgAyQDJAMgAyADDAMkAzADRANEA3ADbAOEA5wDoAO4A7QDsAPAA5QDpAOYA6wD3AOkA6gDsAPUA9wD5APwA/QAEAQMBAQEBAQUBBAEOARcBGQEdAR8BHgEgASQBHAEnASkBGgEkASEBJQEnARwBFwEWARYBFQEOAQUBCwEWARIBEwEMAQEBEQEJAREBHQERARcBEQEPAQ4BEAENAR4BKAEdASQBGwEbASEBKAExASUBNgE0ATABOgE+ATgBLQE7ATsBRAFFAUoBVQFVAVYBXAFaAVQBUgFUAVoBXAFVAVQBXAFdAV4BXwFkAV4BVwFbAVQBWwFfAVkBXAFUAU4BUwFKAVMBUQFJAUcBQwFBAUMBOQEzAUABLAEnATMBLwEpASMBFQEeAR4BFQEQARcBCwEIAQIBCwEHAfkABQH0AA0B9gDxAPkA7ADlAOEA5QDiAOUA0wDcANYA1wDRANAA1QDWANYA1QDWANMA2gDbANQA0wDWANMA2ADKAMUAzwDMANIA0ADPANsA2gDeANEAzgDLAMUAxgDGAMkAywDCALYAvgDDAL8AtwCzAK8AtQCuAKoArQCvALsAtACvALcAqQCmAKcAngCnAKEAmwCZAJkAlwCZAJUAlACXAKIAiwCUAJYAjACgAIkAoACKAHkAjgCZAH4AeACBAH4AhwBlAGsAdgB9AGgAZwBoAGYAZwBcAGIAYwBrAGAAVABKAF4AUQBLADoAMgBDAFAAQQAzADoAQABKAEwANAAtAEsAOQBOAC0ALQBCADYAKQAXADMAKwA3ACwAKwA6ADwAPgBEAEEAQgBDAE8ASwBCAEQASgBHADQASQA5ADQAQAA6ACcAJQA1ACIAIQAxAC0AIwAbABQAGQAVACQACgD9/wIA+f/9//H/9f/r/+X/zf/J/9j/zP/O/8j/wv/V/77/uv/E/6P/p/+1/7T/wv+w/7P/yv+5/7j/s/+1/7v/tP+y/6T/q/+j/53/uf+8/6z/tP+x/5j/kf+W/6j/o/+K/4v/nf+X/5n/o/+Y/6L/lP+N/5//ov+P/5P/mv+e/5b/nP+t/5D/g/+G/4//kv+L/5P/oP+T/4v/eP+V/43/fP+C/23/kv92/3P/fv9v/3v/fv9o/3z/dP+C/4f/e/+D/2//fP9z/3D/Zv9r/2j/cv9r/1v/b/9v/3L/e/9M/3v/VP+K/23/iv+O/3j/ov9p/3j/Of+J/1f/PP98/4H/Yv82/2H/X/9X/1f/aP9C/zv/Mf80/1D/X/9h/zj/Sv9O/1n/Rv9o/2v/WP9W/1L/X/9s/2r/WP9g/3X/WP83/1//Uv9I/1f/W/9a/2n/df93/1v/T/9S/zj/O/9O/0z/Xv84/0H/V/9M/3r/Zf9L/23/Xf8a/1L/i/9L/x3/fv9L/wP/Sv9D/zb/Yf9k/yf/Xv9y/1L/W/9e/0f/Rv9E/yT/Qf9d/zz/Pf9a/yT/LP9S/zz/P/84/1j/V/8x/zH/8/4X/zz/Dv8P/wX/ef8+/xn/P/9h/xn/Q/9l/wX/EP93/1r/8v5m/5v/D//x/on/Rv87/1j/7/5I/xn/Mv85/2b/aP8g/0b/Ev8d/3n/Kv9O/x//GP9h/0r/+v7f/iH/Lf8t/yX/Vv8o/+n+LP+K/zf/EP/I/g7/C/8r/6L/4P4M/xb/Kv8M/x7/Gv8X/8T+GP+A/0T/j/8c/+z+P/87//P+6v5f//j+AP/U/2r/Mv8Q/6r++/48/wz/0v5i//n+dP5g/1n/o/5n/xD/Zf7x/in/Ov82/+T+Fv8L//r+Pv+3/mj+PP8w/9r+NP9+/xT/2f5r//n+6f5a/0D/5P5P/4v/c/90/2L/fP98/xj/H/8U//b+t/9b//f+Nv8WAHf/Hf9K/3b/K/9W/5//B/+6/2T/af9j/2//Xf/U/gn/d/9G/0T/P/9B/3L/Cf9u/xL/1f9X/tf+t/8MAPf+cf73/xX/HAD8/vz+Zv9l/yf/Cf8q/1n/cf9j/3L/mP+N/2L/pP+L/1UAZQCmANoA+QDeAGYBcAGLAiUB9wGYAgcI7AyWBWUDlATxAm0B3P6yBGUD6ADUAbAAegCX/2L+2/zA/Aj9I/7Y/nb/2QBxAAf+gv6z/YT7gfz8/Yv7DP5TAFoA4f/l/0b/z/uv/ZT+HP3T/xkBJv9bAEAAif7s/T7+BP3H/ET+2v6s/xb/FAAx/wv+n/5t/rv9IP83ADYAPQBaAbcAJ/+m/0kAiv+a/vT/3/5mAK0B+v/7/lf/2f8b/kr+gwD4/yD/4v+cABL/Af/yAET+sP4rAOz/dv6c/usAOP+pAKf/bv5M/woAXf/D/uP/AADj/xn/qwCnACQAWP+Y/gr/IP8LAYz/yv1ZABMAtv2YAZ4EUgLKAG7+1/zQ/c4BFgAN/mb+Hf1E/tYBQQGyAaAATP6u/R39o/9FAckBEQAO/8QBxwGqAesA2v/1/73+X//jAKcB5QJBAuwAxv8GAN//lP6n/n0ABwDT/9YAIwCf//n/OADq/vv9Ov4W/03/DwAgACAA3P+O/5D/9P55AFT/Ov97/yQAMwBpACEAY/+b/5f+QwBQ/8oA7P+f/+IAxwCX/8kDTQLtBEcOHg1ZCoUIpwmNBrUEZAMKAt4AgAKLABgB9AO7AhcAM/3G/Un89Pty/e39xf6s//IAef93/2D/9v36/L787P4R/+sAiwF0AUAD9QERALv+Vv9j/y3+oP6y/xEANwB0AIf+Sf3r/Ir7YPpx+zn9Yf6Z/Vn9Zf5Q/RH9nvxX/ID9yP/H/kr+EAEFAfoBnwDNAJEABAAHAokAbALaBCkFKQSKA4EEMgSkA8cDEAOtBIgFFgX6BW8GIwd3BBQDzwM1BCcEywNCAzgEmAWCBNMCeAJVAgoBv/61/gMABAKBAVwALv92/oD/CvzP+wX8NP2e/ZL89PyF/C79cPs7+o35KfsB+5T5Vvkz+Sn67fr09p/3G/iz9h35LvbS9234zfhn+Nz20/kI+nz69/ql+tL8wf6z/uT//f6RARYDeQNPBecEYwgnBnUIHwldBxcNwQuQC50Lcg3SDgYOjA9fDp0P8BAfEEAQmRElElARCBGVEEoRcBFQEMcPNQ/CDssOvAxCC/IJgggqBZAC/gDz/C77Hfi59LjzPfKy8fXsfuki5FPfg91j2M3WpNbo0N3Oxcn0xEXIUcpe0H7PrdIM0uHQ29pd3YPlh+vp8RHz3vhVAO8DlwwyE0wThxiLHcYcCCarJEUouChMJ/kmoiWgJkMlliSII2Qi5CKZIsIg9h9/IJAflh9kIgEhXSOtJE8jASTLI8YhpSBLIOIdOhs2GioWlBRREiUNqwijBDwBqP4J+kv1+e9P6knl4d3k103QV8voxJ63OKyKpNSYLpwktQzEC85u0jC9c7cDyOrRytsz5hDtm/TXAS4JuQtFFtUghRjjE44YYx22LosyPzO5MXorNCjwG4gYvRbWFEQQuAqhCGILpQ/5DEYH4gLJBSIAXAT9CssVSCH6HxMc+RezIF4nqCkAKU0ouCzxMag1TzkOPTg9mTfyLPInxSq+L8At3ycHI4YfJhwoFJoNbQcSBEb6ge9o6pnnfeTg21fQs8PFujy0hawJpUWerZnAjumJbprWogewWLd9sLqyJb73yKTO0ONd8OT7TQyBERUXsCQKK3gtGjEbNf04SjwVQmQ+uj6dNxst8iUjHfwYqBLRDEsFBQVgAsUANABr+O34BvMq9Jj11v9MC8MOABQuEGEUgxuYIAwhMyEkIz8pRDFyNg061TuZOq00qS5RKzct3C1OLLAn2SPnI/oeehhcD8wHXgFq+5H22PGX8sbtvOUd29XOr8gPwR+3+661qPGmiJ5Rl3uM/oVLl1mqmrSJvau5TbZ5xEjOud2g6l36QAUYDAUZ1SDSLUA4STZhNf43OzkFQMdBHUPQPto3XS3xIqMgwxmREo0GXfwl+Pj2a/Z69yT4Avbl8zPqIezx9PwBeQxuDY8MnwyDEzkc8iRkKfIpvijOKMMulDhiQPFBuTycNXAxkTALMtQwwC3vKFAiTB7bGPAW2xCZCCv9TvWF843u3ew+5OzaDNIDyd6/1La1sE6snqI+m1+SyYW/i0WZ3aosuoW8QroyuZS/htAh3uHuuvwjBbkRuxqeJY4vbTWnOU45uTpVPcY/+EUdRIFAzDgfLRQmsh5gGd4OoQUn/cb4LPhb9hT0UvD07NTmRedV6hLz1f7rB7EKXAqZCy0PeRiXINck4SUqKdMuTDbVPCVABkBaPPg3yDFcMYM0wTdMNYotJSh7IUAfVRc2DxwJbQCI/Kf1MPEn7znpROH20sbFJb8SuNezXavVpSGh7plYjyeA+4Y+l2SqPLvHuMu2DLwxxuLWQuFs84MB0ArLF68ddSqCOf464z05PiI+jkNiQlxF6kDjPnk3DSrFI74crRc1C07/2/N58lbz9vDJ7kHtL+xO51ToDumQ8hP/QAXRBswHDQ3aFToe9iJWJkooBy22MS82TTx0QK1Apz24OYI1czWTNWEzDDDYKlQnaCERG1MTWg0ZB6b/3vgr8s/v0+m44z3bKtAVyKa/c7awsLGpX6aiobmXEYoBgC6LeJjcrMa9v7shvPO+ncRN0+Djf/TOAKsL7BV3HQwtjTaZNog8QDsNPDc+iT4rRYRCp0BLNtYo2iPfHJUV+ggk/8D3k/YL9RLxhe947q7uJ+h36WTuBvYWAU4EeAUWCvYQAxdCHIsgZiSxJywsdTAkOOs+3kEoQCs6GDnwNWA0LDN5MbQw+CyyKHMgbRsnFfcNTARe+5/1i/Kk75TovuIj2RzRm8U6u2yzTq0wqvShHJ01lpiC8YUHk8SZ2bHwvPy2SrrcwXzFPNfZ6Hf05P9eDfAW6R/pLqQ1MzavOrs9ajy6QExCP0URQzw8uDReKkYlmiACF0kJgAEo+TH3Ivfp8Y3viu6e7W7oKuxH8Ev2FwHvAXUDDQlWEo0Z1Rw3Hr8f8SQcLXQy2DbVPFM+WD3QOA051DetNm80ei7OLCEqnilXIRIbBxPvChYFFPyH+J3zBPEq6g3ixdlh0QzHb7swtOOtaanso6mf9ZFlgQuLtZRrn92zEbeQr5W2acDFwc7YGedd77v6uAXGDjUZKCsHLlgt7DbON7c5KkJOQBBG9z/ZO3g2fCs4LDAj5hiBDjAIa//0/TT8iPZg+HP1W/KE7ary2PPz9bj8pP2QAsEIdw+AFCQcOh87HUcebyIwKrUzBzgBOjw7VDrDOTgzTjDQMOswdi74KSon8iR/JAwbFBKWCBIBMftm8xTwZOtl6ZXhJdbmye7AOrtJsMGqlaFynXOYOIROixOVn5yqsMayo6pJr6K3jrmDzk3b5edY9Tr+QAbREGkiZiiXKZgwIjJYNb9AkUDgRjBEez+POkYyETA0KR0h9hQqDvQEWwQTBKn+uv0V/OD2tfCq8hzxWPaE/roAeQNkCHMOxRM/GY8afBoAHOMihShWLjA2gDrcO3E5yjcNM9sxhTF4LpsraSs3LHoowiUWHEsUZwsnAxr8oPWm9GDuNuYG38nYw8wYxGO76azIpnGjD6A+lJSJCJKmkNKeiq0nqoytn7Pbtra2qM341jfiK/Nf+hgAKgzEHsohKSQ8L6kvAzFpPyk+YkTARBZD3zx2M/80IyzTIzccaBSUCgQOEgv/BFgDPv/++ZL2f/ul+HT61Ptb+wv97ASLDroSExbMFiAVVhPLG/MgtyebLpYwpDB8MII3zzLYMYIuByi8JmYmuCcdJewkQR1ZFjsMMwW7/374xPNh61Plpd5G3KTTt8WNwLqzeqg2o7KfjY9Hh4qdcZjQpL+zvqyDqcyz9rhWsQ3PL9Vs3sLvg/aF/jYJ4hmaG58aAyavJ6AqvTl6OtpDpD/6P+k8BjMsNWguZSUIIJUcxxImFVEWAA4wC5oIAQFzAFsIDgMaAc4DR/5tAKQGsQ2eEEUUlBKxD2USWhc7HWQg1SPiJk8o9Ci+LiUsiyxbKhcmBiSWIQ0lCCHfH/0ZiRMFDkkHpQAA+oPzZewh5z3g3dvW1jnN68Tlu0uyc6lvpqecfZFmoYakAqjxsoGxaK9othm91rvHxorRFNq94xPwKPemA7UOIRENE4IZRR0TIRIrKSzhNtk4rDngOmg3HjRvLhMptiE/IioeQB5IHykYRRc/FHAPzgwYDQwLaQaECKQHSAcODYQQbA6GEJEP4Ay/D0oUbRTOGNYb0htxHzsgoCNRIXYiKSF5Hisfax3tHUcbjhgwE5UPRgsaBzAEJ/4J9xfwdeqs5kngsd7C1hTQosupwWG727f6sBGqL7DYtBq33Lo0wJO6sL5RxGvDdci9zfnYA9xA5NrtlPNb+coB8wE+BaQLaQ4LF9sYGiLHI3smzCu/Kegs4SppJtQl8SNgIkMl7yNnIN4hIx8qHWsagxsAGO8ToBLtDX0O3w5yFKoR/BD3EGgN6gxwDcMNeQ75EOASOBCpFIwaERZ5GooXjRE2E7gSzRHYD/0QJA+mCXUH7QgoA+j+TPtl9SruN+3W5zvo++Jm44HeF9Y52JvRmNAsyyrLB8g1y3HJjs2U0WrR+9dJ1K7VOtTE14Peud695LfpyeyV7jr0VfeZ9cT87v5o/RQFfggjC0gSGBMnFpAVWhY7GucXMRvYG0YduhvRHMYd0xs5HcIbwxm6FvER9hH9ENIQqhIFDw8S5gt5DUEOUwxaDWkOWA2QCiESuRC/EP4S2BXcDugSPRBcDSsPyQ2CEIYBcws4CZgAtgq3/gz+owTr7zv/UfKO+Of4euy27vLspu7W40nuxehx5I3jY+eB3Q7lFOIH4nTeIttr5pjcveZT5lXmHeIz5xbm1ORL60Xw9Os07bPyFu509ln2XPxK+K/9wf4l/lUFJQP+CTgKPwnXDHcOCw5mEdkPfRHlEPQO4RCgEOsSlhFoE1IMAREGEQQMqA9VEAYNiA5TDQwQJgwmEowPjgs5Ej0QtgdHFV0Q9wfyFPENdhBPByEQwgWYDnoCwwnVCs788BM59vkHtgAi/1X6rv7J/qvzDgT27zX5K/h89Vnybff+82rss/vR6lHvH/I/9GPvIepS+TDvV+998//y+u8P8Uz2Q+tB9Yb2gPBE8vL3ifcR7pH8+fe68Cj84fwO8moCl/er/RD/N/4H/j3/zgcy+jsFpQatAaAD0AyY/tILtgkKBl4EBg9EC7sGRAxtEvoJ3AKRGoADog6nDscNSBHiBYcYtAe+DN8N7Aq2Cj8LVgqTCWkJlwdIC+4BLwfUBDICiQLcBt/+L/qPCX/6Kvl0AQQB4vlL+of9lPdK/Df54vdO/IX4yPhO+Db4F/jc+Er09Piz+Mr2efbf+of7Y/Jm/Ob4bfaU+Tr6ifvI9uL56/qy/ST4lf0S/233swMf9vUAi/niA2v5+f27BeP5tAJu/RUFDv2XAiwCif/7ApIDg/5JB4L+egYwBMz+bQqU/nsG/AQCAA0L3v3ICRADRwIfC/f94wrlAfoDDwkUAQgEdQYlBAUBzwmH/scGdP65BZIBUfpPC5T5EgWc/sIAAgA1/SYArAHF+sAAF/9r/ff+zvwHBCH3KAJ//WX/4/lMAo/9PvtjAXX+G/xf+wkGlvgd/o//PQAD/bH9qgNe+7r/iwC//0X9tQFhAGj+KQGi/l0C/P3RAC0Aff/wAXYAIf/zAWf/6wBZ/gwDVv2RAuwA6/zuBav6CwTc/pQCe/ytA3X9XgEx/xcARAJU/FsEhfyNAiH8TAW8+ssDWv0RAX7/Gf+hAYf9ggG4/ZoCVPpjBW/6wQTm+hMBEAE2/fIAuQCd/TD/QgLH/HUCbfzOA3b7XgO0/dz+JgGk/jwBYf6BAB4B3/61AE0ATv8wAqj9eAN9/HAFL/wTBY/+G/+XAxz/0AFp/TYFXv6vAckAmgIT/p4FiPx8ATMDfv2IA8P8lgOOAOb+vQAuAhj+TgESAEP/oABw/hADvvxDAK0BIv3aAbL82gEz/fsAdv5Z/0X/SP9f/6v9BQDK/jr/e/yAAtX7egDw/TX/7/+A/f3/9/1s/wj/9v4zAIv+2P+GANv89QFO/lcAhf70AM3+RAB0/6QA5gAZ/ocCUv7hAQD/3gGZ/oEBmQDK/1sBWf9eAin+UwPx/ZsBvP9fAn3/pv9QAyP+zwIi/lkDn/7mADYBcf8qAZf/cwHR/n0B0P9XAMj+wALF/XkAWQE7/+b/ff8kARL+PAF5/k0Ao/8//7b/YP9CAGP/9v6b/5n/6P98/kQA2v48/7//Hf6pADj+dgBH/tr/Zf+X/oQALv9s/53/0/97/wAArP43AaP+WgAyAGr/OAAnACsA8v9dAFgA8/+I/34Bzf4PApb/YwDiACX/+gHu/g0B4gBu/38Bk//nAAgA3QCdAJv/qAARASIAxP9ZAab/5wCB/7kABQBvABoAuv8JAMr/RQCL/xwAewCD/ygAXf+3AKz+wgE+/tf/1QA3/rUBPP18Aof+6P/K/7f/uf9eANH/vP4dAF7/BwDL/nkASf+X/6b/WP/Y/t4Anv4PAHL/nP/N//z+fwBz/5H/TgBe/23/AABt/8//cf97APv+EAA/ANL+zAC3/xMAmf9ZADT/RABjADX/bQB1/68ABv93AP7/SwA8/woBo//M/3MAw//eANH+AAH4/wAA8v9xAH3/hwDF//L/uP9OAFMAdf+MAGT/GwBA/xsB3f/w/lYB2/5MAFv/yQA6/4//wQDp/pIAw/6iADIAhP5aAb3+Uf8ZAv/9eAD1/9D/QQAo/1IAkv9BAAgAO/9NABwABf9vAGr/YQDC/2T/dQBl/zcABQAUAFoAnf+a/3AAXwBq/+EATf8UAFUAAABOAAAAxQBV/zEAGwCu/7gBV/5jAab/+v9sAK3/JQFt/ysBC/+fAVb/uP+GAb3/5f6sAhn+EgEeAFP/mQG4/hsBef+2AGH/9wDE/ncAmf+pAGj+TQFWANn+RgFl/y4AiADZ/woAEwBoAH4AJf8+Adj/kQAn//oAbv+zAJr/9wBc/x4A+AAG/gkCjP4OARYA2/74Ahn/7v5rAb/+dQHa/13/FAHU/0b/aAEs/xkApQBW/woC8P3kAVL+/wBaAbD9FQEpALgAyP5RAJoBUf50/0IBYP8BAd3+tQHi/pX/LAHI/uj/vABPAPf9qQFp/6f/TP/eASX/d/0IA7L+av+WAAoApf+e/ggDWv09//UCz/z7/vEBeACn/UMA3QBr/lP/fAFp/1T/E/8nAhH9MACsApn9KQHb/mEBzf5LADgCtPtpAaQAOv/k/qIARAKZ+68BAwMJ/JAAjgKS/LgBOwDR/4H/5v5EAjIAsPsWBh/83f7IARX+mQHp+88FH/v1/8cEzPy+/gMBZwFu/Uj/hgQI+xUAmQDs/6X/gv33AxD6SgGVAb4AWPycA6f+P/0ZAU8AJAAK/9ICPvtRAckC7/2Z/N0B5P8VABv7NgZg/Cb+0QUa+8QB5v6S/0X9nwLl/7r+1ACh/8j+UP/BAKX/YP8TAGcAlABT/RYCHADH/lj/PAKz/bP/8AJp+48CAADc/+n/yf+2/lYCnP0Q/9UE2vmJAhf/BACNAR39DgWo+xoDDAGg/HEBq/+k/0D+3QKA/scBi/kYCDH7cAHuAML4ogkJ+W4A2QJo/JEAJgNB+qADXf1SA5P89AByAUz8uAMG/nEAtvvsBjn36gJuAAgAdvxPAIQFuvbFA9n/FQI4/Sj/6wNO+kEEtvwT+1wGsv1D/E0Chf1HAKMDNPpBA3QAw/uRALwApQCW/XYED/y3/I4HBfeV/mMErf+b/Pn6nAgU+5n+fgE2AOD+9/1MA675FwNf/1v/e/zgBBj7t/5tBqP+G/lX/z0I3fEfBnIAyf/2+0MCHQGP/SH+mgCLAPT37QgX9lkFX/5sASb9PgAI/yD6ZgZh+c4E8/dcBL8BdfuA/YUErPlNAaj+2PoeB5/0iwvk9hIDu/yP/9IDJPRiCUX4igXI+NwCfAIa9R0KIPe4/1cDmvxn/akFd/yM/eL9iQJJ/2j3pgeO93EJffa+AIwDW/l1AlL9EQGV+UAEZvt0/CECvv4XALP86P9n+1QDT//b+qUEpfbyB2z4lwRe9qUHAADJ8hcKQvRXBwj4UgK7+2T/PQH/AY/08AhD/Gv6EAKI9qcJLvdpBMT8fv4vAVcAu/EqCIwCKPBqC2z8zfwO/tEA3AH180MI0frE9t8CmQTY+BX/oQeD+BP+VP9j/GP9MwWq+CYCpwBMA5P5HvyuBVz5/v0pA0X5Q/kzEiL27PJhDbj6f/qCBBP32gHD/Y4EdfIAAp4MwvFq+ZIL1/+17hwGzQgM9+H75gNa/q4CSvd6B9v1XgYLAtTwmQFEB2b6BvWhCl790foqBPMAvO5gC2AC4vA0/2YJX/jm+UQLEfpj9EEKZgGH7kYKKf/a96QCJv76Cerzj/qDFUPp7fg2FWvruAi3AM32SAUU/ewFoOETGWIBf+aBCkv8dgMA+K3/2gmT94j4yAhY9DYF8/n4+00CTfxlAkX7F/zhAuX/hfN+CJn1BwfdAKTzlAqY/kH2FQdv/9nyXQ0M+eT/lfiNCeADzutFCcz+x/d8AQ/6ywLp/x760vv5+w4KsO05/oIMHfM7/6YAEQUQ+OT7MAkx8UUI3f4D98MEE/2nCMnzcwZ+/1v3QwEG9v4LoPOhDzX44e9PFG/8w+d5F/v9HuutCAUFIf9y9IEQJfRCAP/7efN2Dbz6yPrQADMCJP8J+PQIu/W/CD/v/fr6E1Dt8PoHD1n+L+z5E/Hw3u/JHJD3w+H8HOUHBuKCDZMNT/N28J8VE/wa6LYS+gJI7wAHIwNJ8qANIvrC91/3fA00A6voZgkWEZjstv/yElDsP/2yDzz1CPZIDJr+0gF/8+ERH/Dh/7EPf+SLADQO9vTU9XUPOga291j6Cw4J7O3/ZRDb9xb6LQnsAKz3GgYvB6DsdgOiDcDrkf51EY787vZHCXUJa+dFA1sNQe6X+qYU7PlU9RMIgPxHCRv23AKR/pTxJgyrAp74oQpV/Nj/3wGD+9EBVfeZBvD8KfiLBbL8+AW2+oH8hP9u+1QLmPatAyUAXfcEDAcJhfk5+kkEWf5V/hUBTvqPBM4AzfA4Dc37qPnPBKj9IvO4/5YJCP8k/jcBXwmZ/Wb2lf5ECcMAxPrwCcD+0PbCEeQBofs0BKoISPpT+5oIsvrFBecBJvodAGkD3v0T+xgB2vqp+OYB5vWJAMcJqPmo/IAHLvZB/DIKEfg6/OsEhPmZ/LAMKflR+iIKbPz7+Sz99QkN++j3Hg89AkT75fqNB6z/4/PzCn3+AvZyDJz8c/pPAiAGuv337HIGZgj99BQC8Afs/R4BHPuI/oEGewbK8yr9igVs/AIE0Qa5Bgb/rfs3/7wAAwMB+poCGgV+9MP/TgY1ByP93Pvl/FH8//nQ/y4GDfnCA6QL+/o9/Y0NIfeF+P4IKAXM75IDXhF88TQCogol+/X68Pxl/YT8u/woCVD2v/7fBq/3B/+1AREHz/U3+kwMi/dH90UVEPx5/PEB0Ack9nD87ww5+LcDSPwY/kb9PAH4Bcz6DgKvBNr6KwUiAjr+VP9hAh4CHv1vAOgD5f9mAlYL4PSA/wwMvv4A/GwB5QjiAiz9gQH5/twDTfkTAqAG5fxB+gUBMgZ1+C39xQcVB4n2lv/cBzD/0PlDBTL/m/qoAx0CfvzFAwINGvdh+1EIxQEn9RkHMglJAC78YQN9BV75jgWyAOP4bfp5AxL/pgAoBuMCzPdM/kYA0fw8BhX/zP30AOkEsf1xAnwCIALy/IX5uwAr/qAJAwFK/NgDTgMl/Az+2wK0/awA2wNX/3AELAW5AxkE8vtVAYgDOP9X/wAFlAUnAZsBNwJkAEP/BgER/EgBEwAj/i//vf7tBP0BV/5o/jb/AwIMAq8DjQKQAUYBPANwA5QDsAGRAAf+Bf/vAFsAFQIgAZICvv/m+YQEiABm+dX+3f6bAkL9uv9JA8j/BANR//H6dv+XAeUATP52APoHTQOnASMCtAOQAG39Sf6S/hADQP9pA9QGdQIgAU78af9r/zT+BAFKAokBRv+PAusH/QIFALED6wEJ/PL+LwTzA34DAQNYBA8ENwLqAjsBWf8FAJwAGgGgAhoH9gMHAWoCkf/cAAkB8QDb/7UBcAMXAhYCrAIUBL4CzwEsAUH+0v/XBOkEdQLJA3oH6QLJ/mICJP+b/z4BQQGNAD0B0AOqAtT+bPx4/jP+sfxF/sn/PQDFAEcAJv/C/U0AM//v+yv/nv8bAYsBrP46AlUCmACaABT/XACqAJMC1wJZAycEvwLsA3MEFgIGASYFrgQKA/EEAQYGBbIEwQWnBbgFgAWABf8FxQVKBp0FPgcECDoGqQX5BSkHqgVSBqEIHwiDBlMGhQTUBRMEkgPDBFsECwYNA+QB5wIOA8D/MQH/AFr9M/2I/9n8e/su/gz8ZvrJ+Br5N/cH+JD2VfY29231B/Yr9JXxnPLd8erwAfQb8hnxqvMB9JHzx/Ye+PX2BPmN9+/5kPwr/kgCsgBAA/8CuANaCbkHgQrkCrUNQgwADaQRyRHME3sTYRT4Ex4VvhXVFWQW1xeOF7UX9RckGI4YLBfOFjEV1RQ9Ff0U1xK+EmcSWw9kDlcKeAqLBmEDAgLo+nL7fvjG9TPz3+3I63DmV+Ps3QnYDtZ60KDNfspnw+2/3bt5u5DCjcsyzkHNDM3cyJLMEdjV2i7fRecW6QHvHvZQ++gB5gSCDGkKxg0WFyEa9yX/JfIpcilLKIgquivMLnAu7zC6KxgsQy/sLFkvOC2yKM4k5CTfI6wjGyaRI5cjxyHLH7EdOB3gHKkaOhhAFv0W5RgLGjQX0BR/EKoOPwz0BjAFaAL+/7f78vfi8lruneod4THY8dHNycfEjrzUsx6oOabLplyzEru6uAu+G7Cyt6u8lcRg0NTUJeOO5mfysfZw+vsAyAnWDVYXFB4RI4MvqzBwOCk26DTlL/YvijI+L8EvzSqZJ5YlUSJYHNoXlBPcDRMK0glbBiQK9gstCekH9gVCCKMFSAodC5kHBA+iEOAU2h4PI1EhzyDLHgEdoiNJKXArhy3MLnUuLS3uKq0noiaZIlQejhlOFvwWcBWRD3EGQvwC9JbqYeAT2krS1s0yyNa8ora8rLynfJsJjpGX0Zm9opiuo63+qvqy9bnwuVrJsNPs3RLtffni/YcHZRNyGoIgNSl2LUovBTrGPV9BFEPBP6A7BDboMmQwxSmAJjkh8BqhFlcS4whOBeIBCPmo9zn2yvaD9pz5u/ZM9oX8U/6LATsEwwchC0sMPxKGErMYaiMnIzEnvCa5KR8rzi01L9wsky8dLwUuVitPKuMnjiR8ICEahxQFERcLcQak/1j7lPOT6Y3iYtep0/nKUsNyupawXqsEo/2jPZ+JloectqF8p8KyU7cntWa83cdlz3nfROt49Nf+XArNEBwVNh79JOEpIzIdNu827jtRPT092TndNVsvWCrOJZEhwRzbFrsR3gnBA9L/UvoD9lXzuu4W65jr0OwG7Lfvm+2g7yTzdf74CugLVxDACpYHZxTpH2kknDClNlY3zDYKNqUxFDTJOTs0ejG5MZ8zPTWCMfEodyNvHRAY+g/LCVIKFwwXBxr9qfIF7D7kLdzD1aHKvMdrwEO1QrFurb2sD6dIlnOYdZ2ppjC6l7vsuWu/XMeSyjrZ8+Vg8Cf9fQhSDqUR/h0lJIYoPDL0MmEyyTdpOEg83Tv6NTkvBSQ6IB4d6xbQERUMAwZ5/+T6B/Is7hftkucG5tDlTeaf56ftze6W7n7waPFw8F/7sQaiDhUZpR2bGxMfayoJK8c0KjdsN2I30jwVQNM+MECvOPYz5i2qK1UpFCgBKDAj6RzqFdIOyQlgA2cAR/iA9InwGugY4yTbJNhtzybMzsC1tH6w5aiqpHKjyafYnhWneLCnsSq/IcFDwUjDO9IO3SDktPb2/OYFsBIRFi8WGh42I0cp2y7GMfEyrTN5NygyAi7qJqog9xyBFc0SBg4ECC0ExvuA8xDvNu1a6HvoguUi40Lofupx7WztyO5Q7Krxb/qRAjoJ1Q6WEWoYUiQoJ4sp5SvfLpUx/zvwPMM+Ij8FPe825DA9M1YwRzDmLN0mxCIqIaEbHRZpEGoMrwZxAOH7QvhP9WTvnue63nzakNR6z4jD3rpOsjOsoq2mp0alqpyomjukP68uvEC+Q8ExwAfFedcT3kjtr/cGAfAM1w/IFR8bEiGqKLsrqy98MgUz1DiANccxvSzPIFYcaRhVFDsRngqGA/b8Dvmv8XfrAumV5bTj5+Ku4nnkZOuF7/7wlvC18D7zmPtSBb4PBxXcHEEfiCDpKTMwRTg8N1E6CDdgPCdBvELLP903+jPzLEAtRiozKp4oSCTUHDEVHg89DSkJtQJt/dP4KPa88sTs9eO82yrWkdLDyFvCO7n8sEWqWKYrq2ajgZ/3puOmhrC7voS+Qb5GyKDR5tQq6F3yIvgxBiEO1g5pFdYfRiVZKfIwjTFiLyA4tjMyMvIurigdIwAa2xepEXELhwjYAnb5kfXs74zoV+cH5VHhl+OV58rnfuk47Qfs4PEo+M378wJ4B4gQABu4IUwnNytSLXgxSTRlO8Q6tD74PUk7hTp3Nco3ETQKMZIreSbaIrchsx5fGXYVBhDLCvAEFwFG/sH69faJ7PTlKeFk2pLYwc6xyrfAPrv4ts6pD6hqpM+d1JiZoiqswbVxwpvBhMGJxVXTCdxr4az0x/orBocWJRjRGQUm+ib0JzgvAzGaM2c1dzqGNOsumyhoHksZKBMHDRIHngO0/hD5GPG964TnSeNl467hWeBx5CDs4e7c8P/0nfeh/f0DkAmSDI0WdB53II0saDQDNtA1/zXeM/42TD76PoI6LjaaNJ4yYTDDLDMo+iTnItgdzhdMFSEU9w7tB5ABTfzk+YD3lfEV60blIeET2rnRtM73x/rBP7yksd6w+KyOqL+dHZgOpRiytr7owDTEh8EpxxzZTt1Z6sjzjv9WCHcPWBngHaci8ClrKhEuSjOiMVE3QjbHNCwsUCNvHFEXgRVxDJcGIf0Z+cX1D+846cLmgOX04fjirOFu5r/tF/RU9uL4Zvz1/q0Gyg8rGwUf4iczKxUsCTXwN9c3jjgtO7o4Cj39O9Y7njmTNTsx9SjFJzwkYyMcH3kbJBjXEzsP+AeVAc/+x/vR9wDxe+w555LeaNiC0qHOxMYfwkq3ELAeq0ylgqdfmS2YlaLeqBy4Hr95vRu/S8tY0+7XFOs575j5mwtFEVgWkCDPIjYlGCuCLpEx3DI5O0U2hDV2L6kktyBQGWUVGAxjBtwCAv8e9wr0ju3f6Y7qE+Xe4VjjIOy/7CHyEvXn9az8iQauC30OGhkxHHUcPCXiLVgvoTXPNAM0VTmSOrs8xzZvOP024TNuM2cu4S2ZKuAljR9GG78ZJBc/ECEJcwNnAMf7gPce8BzpCOXn3KrVFdBrzrDDWr4ytTOsWq9KpfabNZW6mTOkUa8RuZC3dbxXwdbGctRd2vHnLPR5/uAKTg8JGnEgnSFKKwQudTCxNbszoDr+Nys2QTHsJgIlIR76FscOtgg9AeH/vvsL9cjvOeyw6EvlA+kR6Svs3e8d8h72vvyeA1kI1Q1rEn0TghV1IKAn5y5kM/U0mDSzNVQ8yzZiOoQ6iDlZOc817DXxMYYxaSuLJEIfCxyLGOYStg4JCqgFJwJj+NnwTugE45/bWNHez0zIR8XrvA21SrFgpOWiK5gPkEedyanlsfS2J74burXBtc970bPbZeYF9SD/kQq7FrkcxB91KRIo/Sv+M7wyyTn7N749xDkDMcIsmyNhHJwXpw4RBpoGbQLj/Gn2rPE/7E7rAOq051Lo0Or+7yLyZvmy/+EDWgchC0AN5ws4GHkdUSSrLjIzqzI1M6I64zPIOZ057DY4OJk3iDnpNUE2MjDxK/0k/R5LG3YW7xM3EdsM1QlJA6z81/LQ7cLmZdtq2n7Srs/Tyx7G57/BuFWxAaWuomubH548q5utgrnzuhO8XcKmyHDP/89C3o/jCfA5AOYG1Q1rF9gbnR6AIq8mSSzpLVc3yzR3NNAz1i1eKgolgh/IFycTwg97DT0InQZGAI76bPo89rfy4vKY9Iv1Q/nq/FEAJgXLB2AHxQUqC88Q4hUBGkwiTiWBJzMv6ivZLN0tpS4RLAEu9THZMWczZTBHLVEoISXYH/0Z3hS5ExURQQ0tCrgGUgH1+7DzNule5Jnfgt0b2NTVY9KTy8HGrr4guwKxiKvNtFa1bb0/xZvBKcBpxxPMAcj8z1/S5NW14pTt9fF5+hYCvQJVA38JVA8IEiYcKRwOIXYmTCqKKzwnAia3IkIeWx2zHfkZ2h21G20UvhQjEvoN3wuhCRsHqQOQBk4H+gZeC4kJpwZlBwQFdQecBr8KtgxnDEkS0RC1E3ETvBQLEhoRIhS8FMYXFRhhGBEWnBWBFMkR8w5+DHMJGAj7BRsEKQJeABL9Bvhk9C7zfPFl7TjsJutK6w7qL+kL5abkvONU4C3h0eBb4/rjc+tI743mCuZL6Z7m3uWa6cvqiOhK6w3unuxy73zx2+7F7YPwWPN59MP4Efw2/Fb+fACJAK8BDQQhA+8CIQV5B0YIoApJDKcL9Ay7DawMkAuhDPcMiwyVDnUO2w47EPEP4Q5RDRoO5gs3DBANjAyfDfQNhA0KDYEOOA7kDLcLAAyUCjwLOAvACmwJKQrCCqgJGQqbCEMI6QawBqEGaQfeB0cHJgcZB3UH2AZJB4QGsAUaBWwFOQXuBLQEKwRzA5wCcgFLAPf+HP2t/Mn7k/sh+pX4M/hd9lH06fF/8Lnu6Owu7DDr6+nr6JDo7ueE56nnNOco5//noOjL6TLrS+x77fPulvDs8YzzCfVj9iX4Vfpf/C7+//9oAawCOARnBYYG0QfVCPcJMAuHDI4NUw4nD3MP3g81EN8Q/BBeEd8RHhJdEnYSUxJhEmkSaBIuEiISVxJBEoYSthJ6EjgS9hGYETsRChGkEMoPPg/TDi4OzQ00DTwMIAtpCn8JqgjGB9AGsgXgBCQEVgNBAiAB+P+b/r/9svyA+xz62vjN98D2/PXX9KvzmfJQ8UvwDO/b7aHsa+u36uXpNenT6Fvo5uf65yPoRui96Pnocek46lrrkezp7W/v3/CC8i/0xfUr9674Cfo5+7b8Kf6Q/xcBhQLQAwoFHwYQB9EHkQhZCRwK0gqHC0IM+gzIDVEOpg7qDjsPdg+xD/4PKxA+EHwQsBDgENsQ5hDBEHwQWxAdEO0PjA9DD9MOTQ78DU4NlAzZC/4KGwpeCZ0IxwdAB64G/wVKBZAE6APSAjkCqwG4AB8ApP/b/jv+v/00/az8+/tu+6X6Dfqi+f34Lvjh91j3dPYU9ln1kvTN8yTzcPKU8RPxgfDT72vvCO+K7vrtue1/7T/tc+2i7QDuRO4P79zvnvCF8SPyAPPp8+n06vUL90T4e/mx+iL8aP2A/qr/swC5AcoC9gPoBOcFwwaRB2EIKQnGCScKggr6Ck0LnwscDFYMpwwZDU4NWA2SDYsNdw19DYQNaQ0/DQQNpgxzDC8MrAstC74KEApGCcYIPAhKB4IG/gVABZME/wM+A5kCFQJ4AeQAVwDi/z//qv4//r79Sf3y/Iz8H/za+3D77vp5+v/5lfkg+aP4L/i/9z/36fZk9uX1a/Xj9GD09/OX8y/z1/J38jbyEPLm8ajxhvFa8TrxV/Fw8cHxA/JZ8snyTPPq84/0N/Xy9bv2g/de+Dr5L/oy+0X8Sf1J/lH/TABHATwCOgMqBAIF9QXMBqQHXwgLCY4JIQqyCjYLyQsMDFsMkAzPDAoNMw1ADUANDQ33DMwMmgxQDMYLEQsRCoQK4gp/C1wIwAUnBiUH4QbABMQDuALbAV0B4wB3AHn/Qv5R/c78Lv2//KT7E/rb+fL5hvkR+RX4M/ee9pL2tPZJ9qb1B/W49JX0kvRx9BH02fOq86Pzk/Nf81bzaPN186fzwPOz8+nzQ/Sq9A31bPXe9UD23vaZ91T4EPnQ+Xv6SPtL/DX9E/73/sr/tgCvAboCogOBBFoFJAb8BtUHnwg+CeUJfQoTC5sLEgxvDMEMGQ1ODXMNmw2ZDX8Ndg1PDQgNvgxMDNkLYAvfCj4KgwnLCAgIOgdtBocFpwS1A9YC6QEMASYAN/9T/nb9m/zd+xL7TPqm+fb4ZfjS91L3y/ZZ9gH2pfVY9Rb12fSw9JP0ivRu9F70cvR89Kr0xfTk9B/1T/WB9dT1IfZv9sX2H/eN9/v3bPjx+GT55/l++g77r/s8/NT8c/0m/tf+jf84AN4AfQErAtMChgMjBK0ERAXNBWcG2wZcB80HLAiNCOgINAlxCaUJzQnmCfYJ/gnzCdYJuAmICVYJHAnCCGYIDQilBzcHvwY1BqgFEwWJBPkDXAO7Ah0CjAH0AFcAvf8q/4z+/P17/fz8f/wT/J77Rfvu+pz6TvoI+tX5n/l7+VP5M/kh+Rr5CPkM+Rn5MvlL+Vb5dPm3+dT51vkn+oT6m/rG+hb7Z/uU+9D7NPx4/Kv8//xQ/Zz92f0t/nj+sP7+/k7/k//d/xIAUACXAM8ACgFEAXwBmAHLARUCKgJFAnoCkAK3Ar8C3wLrAuUC/wIDAywD/QIEAzMDEgP9AuoC/ALIAqcCtQKSAnUCQQIvAhIC1wHAAaoBZgFKATQB6QDMALUAZAAgAA0AzP+T/4L/ZP8o/xX/+/7G/o/+g/5z/h/+Ev4K/gX+1f3C/db9wv2g/Zz9mP1+/XX9oP27/av9xP3b/eP92P35/Q7+DP4b/i3+Rf5u/oD+nf6o/rL+2/7s/vb+Ev8y/yz/Uf9r/37/jv+d/6f/rf/T/9j/3f/4//f/4f/7/y8AFgAAAC0AMQAmAD4ALwAdADkANgAlABQAIAA1APv/CgA0AA0ADwD1/+P/8//g/+//4P/C/8b/y/+v/6j/rP+Y/5T/gf95/3z/bf9z/2j/Sv9r/1L/Tv84/zj/Vf9O/zv/Of9E/zH/I/8T/xn/+P4M/wT/Gf8U/yL/Kv8j/0b/Qv9z/1b/Z/85/yn/JP8W/x//FP8T/zL/N/9L/4X/ff+w/7f/uP9z/8H/5f+dABkAnP/m/6T/OwC//+//9P8/AAAAFgAcAFwAbwBZAJ0AlQAyARQBvAG6AYICrwJtA8ACIgOGArwEBgQ2CNwabDWzKPL5qQSFE2AN6QRPAVYTjhBICzT9rQCFCvwC5vNu9BP8If7qAW0ECQC0BL8IMPg48oj2sPwZ8Dv6jwqt/dP86/3v+uUEaQHN+vT4jgAC/fH7CgmEDIcEYv7M/dP2YQC9AxYCvv1eBQYAgvvcBXP/KP1uBRUFgfKqA5oFgP8pBGkE0wD3/G7/5fmWAoEGAwI6/1kEIwF3+CD+kATmA6v8GP2EAlsB0AKABbUExfzi+ZH+jgI4/1P/wgPvBFP+b//fBPUCzgUaCbQLRgRq/aUFbQvBB6sCCwFA+hv02PU5+ScBbwa+/cL4wQEt/7j7tAEvBT30TPVeAWT/7/v/BNEEOO5M7m/4FPIE9K//Lf9M8yL23gHw+3Hx1PND+fP5U/Ya+Vn+YgBDAJH6p/ZJ9wf3LvVb9rv8bf1i/Nf8n/s/+/L5wfc99tv1Vvn8+s/7IQDp/fH6Pfvz+Nr2N/tfAUb+nAAJBrIDmf81APoDHP6X/ZsCdQLKA/YD+wbaCKUG2wLxAkED2gFc/noCYwcZAyUFEwraCDACKgFk/pT9yf48AVcCmwJ3BaUAywAc/3L5g/hp+ZX6afj1+7D+3vr1+Qj7hfdT70rwi+9t7//vjvAF81bxn+8J68zmtOih6XrkmuZe6PPngudS7SzvqOaJ5jrp+OX750rvBe9t8ZLysvUa9hj7MPzC+cABAAFMBA4K5gqRCwIPdBTlFQsU2RYYGZUaNhulGowchRt4GdYbpR5uHUcc3xyLGq8YDxdfFscWaRUDFcsQfxEEE+wPYgteB5wGQgU7A5L+HP8v+1j3tvbD98713O2R6Q7l9+Jt397ZUtci1sLQ+8zWzOzHsb+3t92x0q8muuHIONAY1djQJ8/61OXcauKe48vq8+0B+O4EkwlOD3YYvRitF/kXdxjdIXckESvGK5MruytLJwAkdCC4Gr4WlRZJEZwVhhYJF/8WzA9ACqgDPgWFCHsOoxOXFTAVkxUKFyMYuxn1GUAZIBpjHW0gDCU3KZUqhynWJXMi1SCxITch8h+nHt0bPhp6FQsSCgs0Bsz/BPje88nvf+xA58PgO9gs0L3JJMMSvvi5zLMmrpysK6djpQKuzrdrwGrFXccGyMzOy9nq4svt7vad/lkIpBFtGe8fsSYBLJ8tEjEkNGo1ZDozOvI6czhVM+QtDyeuIP8YPRJzCwYIjgN0Aoz+2/oE93/yR/CY7bHvd/Bj903+oAOnDKoRbBahGAsZvhgqGwEhOShtMOU1yzloOUk5LzcyNgg1KjMOMpgvsS/vLh4uASsMJOIbDBOaCg4Enf9o+wL4z/Ns7JPmrd421mLOGMjYvaO5kLhztCC1jLPUqtiluqB0mESdTbQHw+rLitoA3MXeUOxJ+Tj9WwclFLgZpCUrNK85fD4URldD+z/zO403CjY5NpI3FzEVLYAmKhlSEG0E0fWI7l3oq+IE4bbhPeAt353fi9wu2mneJ+Lf5fHuXPRJAGoSiyC/JHooZSrKKvU0Aj2AR6tOo1ANTZpLvUrMSOZDXT79OBEzHjEYLn4sJCr/JCgblhDICOMB7ADx/v35M/UZ8rnuuuqN6C7lfeBH2GDTnc2yzQTOC8s9xtm+8Lx1uaGwNbCgpn+clqcivYjUKeDo6TvmH+K17/H32/+nDL8Woh0VK884YzxSP6g/jTgIMiAsKCccJwgosyacIvQbuBOzBpz7ue8B4sjcNNjY1WXZLt1d3dje7t4p3M7b0N4/5K7qE/Xc/OkFjRddJNAt5zS0NPg2YjzSRMZIMkzcTddMfE8qTIRI+z2uNicvvigqJkUk+iMMIGAb9RCwCk4GzAAn/qD6k/gD+Lj33fXI8z3ze+7H6JzirN7D3+jfnd9230baONQfy8fEp76/tFuwwqX2kjGdPreYzSjpne+O5evYa9345Vzu/QOoCwcX+yZNM3Y61zugPxE25CrYJZMg8h9/JYgqwymTJ9gaBw3I/q7wx+Xk2S7VYdMb2Djc8t8W4RXekdrC11jUMtcN30zqeviABY8Q1RYEIi8qui+qNt04zTlrP75B60l4T15VJFQTSRpDJjTwMSYrlyiAJbEf1x32F+4WtxL7DIoEe/ww+Bv3YPo7/vH+uv0J+4L38/IN8mTwze807MHsSO1g6UDo/+OF3ELRusiYvh6zlq0To4iaXKGst6fPhOHT7OTdrM8g11bh3+78/GIMVBD0Hscu5jNLOUg5tDQYKkwnXCSPIqYoai7oLNolUx0iDm0AbfRT5pDcYNaT1fnW9Nv130/gK9xG1/TQmM/c1AjeculL9ZwBoAv5FOUb2CE8IlAosi55OLI/7UfgTPJPOFLYS8lGcDqiNr4uQCyHKYMoGSn3JbcilBdqEJIJlwSXAP/9h/5jASoEFAb4Avf/svw69yP0S/E283L1lPa295H0ePBh6dTjBNwF0UTKEsV+vPyyxaosnfuQG6JVuwzLKd+43VHP5M5m2nnnFPHKA28K5BI6Jl0ytzYrOlc8ljGwK8UpHSawKLYu/C/xKqEkARZfBy36uuwJ4ZXWI9Qi0y/XYtw33bHZsta5z/jN89A12FLlPvD7/ZgHYBA7GMUf2CJNJ2Iq3jF9OFlDpEoqT3VVLk5fSmY+hzmNMc4tsygQJI4kMSKvIvwY/hIPCogCGf4R+rb5S/0gAosCZgE7AOP7dvp+9f3xJfOM8P/3V/a39Ov0HOwq6pLdvNJHyNW/Vb8Ush2su6anliWek7CBxPbZrdv11njN79E+4lvsuf+/CDkSxR3+KJwxwDKJOH0zpC24KmcmjifbLYcxOy88KS8ZSwuH/V/y6+gJ3YDYNNQo1Wnadd3922DagNTlzy7P09To3Tvod/f4/w0KDxArF8ccOyBvJ10o6ynJMqc1oT3VRwBKE0qWQKc6hjDDLAUsbCqKKXooxCXBHtgblBcREvgLXAWP/gP9aQDbA20FkgTCAp/+XPpC+pb3T/j6+T34JfnA9zX1sfPk7b3mw9vV1CnMf8PYwTO3sqran3iUkJr5sYLJ0Nm/2BTQK8eYyY7bgOeV9s3+2QamEhQfECz2LoYxjy7GJ8wjViPuJowtajPxM+8tSh8eFMcHBf5e9vXopOI83tnf0OMW5zDmM+Cx1zPRos5f0vzbZORs7Zj13f7fB4YPVhQoFQUVoRfOGj4lqDBFP21G/UmFSJs+5zs8NNUy/C1JLAcrHCv0LQIuxilHIQMYbAv8BGYB/gFFBUcHOQavA/3/B/5n+Yv1P/DE7N/ta++99N30TfT671Pp3OSh20zVK88CynLI2sABvMizP6dco5uuTMEV05TdB9iTzq7LT9dY4BjuYffs+uMDUA3tGJUexCZXJLYdmBybGxsfIievLOYuSS1GJeUdDRKrDI8Fb/tv9izvSu3z727zx/IU8SHrEeP621jbsN1N40vt4/Nb+ff9gQNmB0gNNRHtEu4RKhYZGiskyzC1O9hAuDsXOkgv2S1fLMMqDitWKWQqyij5KHUo4CNOHfsVYwwKBxAFlwV9BhQGiASzAF38A/kH9PHvaO1U6oTrHOz+7rzuw+wj6iriS+Bz2onUyM7gzKDLR8WAxGO+JLvewyLP09Ql20/Y29CJ0rfVu9/a5OXu/vIq9Uv7ev/LBYELWhESEOIOlw9yE6IYeSA7IzAiayCmHDkbfxjWFecQ2whoBQMD3QJRBgUGtgO/AG77Pvgv9tv1ffU39Rb24/ZK+e388gAGBJ4FagZZBQAFMgdaCYEMDw/EEaYSuxKYFGwU4hSCE5UQZgxIDHAOkQ8tEHQPNQ9wD3YNeAmzCaoJkQgiBRUF+QYFCHIJcgohCV0FggMGBFAG4wTGBN0GmAUsAW/9JP2dANv90vcV9pvzuvAP7VvpBeqR7kvw4+zI6bPmquIW4o7j7OPp4zfiveDB3+HhE+be6Gzr/+ok6YrpAeso71zyrvQ+9pD2z/fq+Dz6T/sQ+6j6nPny+Zr6K/xk/mj/Y/9H/4j/sQB4AjwE3gWMBnAHTQjOCU8M8A3tDq4OSg7hDd4OvA8LEdQR6xG0EToRsBF2EuUSDBOfEo0RnhBREHUQ7hCFEE0Qrg9cD0APbA9FDyYP/A6vDnMOUg6IDhYPMQ8bD6sOHQ7EDUANewy/C7gK4QkfCfsH9AbcBYAEMwNrAa7/7/0I/GH6rfjZ9kr1HfS28jXxSO867WPrUumr5ynm1+TM4+/inOJ84l3iROIf4uPhEuKf4kbjceTX5VznFung6qDsO+667xvxhvIK9Jb1Jffz+Nj6ovyc/nwAKAKRA+QEGwZqB/AIXArqC7YNXg/8EHESrhN+FCYV1BVCFsMWbxfYF2sY4BhGGWkZyRl1GfsYXxinF9kWKhaIFfQUhBQYFKwTCxNdElwROxAsD84NdgxoC2IKeQmpCOwHAgcuBmMFLQQCA9sBrQCD/6H+mP3D/O37N/tf+pH5uPh893j2afVZ9FLzifK88RLxfvD774vv/+5Q7mztiuyX68LqU+ov6gvqMupJ6njqc+qc6sfq4epd66jrSuw87X/u9e+N8QzzePR/9Yn2uvfM+C/6a/u//EP+6/+FAS4DsgT2BRkH9QfbCJQJgwpdC0MMRw0KDscOUQ/eDw8QOhA5EAYQ5g/TD7wPzg/kD9EPmQ9ZD8UOIg5kDaUM5wtOC84KQgroCZEJHgmaCP0HPAeMBucFPwWxBD4E3gOVA1oDAQOsAjsC1wFRAdUASwDa/3n/H//S/pT+VP4H/rD9VP3d/GL8+/t8+xH7vfpu+i767vmU+S752fhm+Or3affg9mf2/vWi9VH1DfXI9Hj0FfSP8xbzrfJM8gHyxvGt8a3xw/Hj8RryQfKN8sjyJfOR8w30tPRY9TD29/br98r4yPnM+tb74/zk/QD/FwBBAV8ChQORBKQFnAZ6B1AIEQnKCXAKDguSCw0McAzUDBUNUQ1tDYcNgw1wDWMNNg0JDcsMhgw5DN8Ldwv7CoMK+wlyCc0IJQhoB7EGBQZGBZAEzwMcA2ICrAH9AEMAnP/o/kH+o/0W/Zj8IPym+z/73PqG+jv64/mZ+U35A/nI+Iv4ZPgu+P33zver94b3cfdh90j3Sfc09yz3IfcY9xj3HPcp9zP3Rfdj93/3l/e+9+f3EvhK+I74yPgW+XH52vlM+tH6TvvW+3n8CP2t/V7+Ef/C/40AUgESAt4CowNfBB0F2wWOBjgH2QdlCOYIZwnUCToKkQrdChELNAs6C0YLRQsxCwAL1gqVClAK+wmjCUQJzwhiCN8HYgfkBl0GygU4BaYEDwR9A+ECTQKsARUBiADy/2T/2P5O/sz9R/3M/FT86ftz+xX7tPps+h/63Pmg+Wj5PfkN+fb42/jT+MT4wvi/+Lz4zPja+O/4Dfkk+Tr5Wfl++a/50fkB+jX6bPqu+ub6GvtZ+5z73Psn/Gr8r/z2/DD9i/3c/S3+hf7Q/jP/iP/g/zkAkQDmAEIBjQHbATQChQLeAiYDdAPGAxAEUwSZBN0EIQVQBYIFqAXHBeQF8QUGBhIGDgYXBgwG+AXlBcwFsQWNBWIFOAUEBcMEkwRPBBcE1gOVA1QDEAPaApACUQILAsQBegFAAfsAvwCNAFIAEwDg/6D/bf9F/xT/3v6t/pD+ef5X/jz+M/4l/hL+Af71/e794/3l/ev93v3o/ev96/3z/fb9/f0Q/h/+I/4v/iv+R/5P/mH+a/53/n7+hv6U/ov+nP6Y/q/+q/6Y/qz+t/66/rH+qP6u/qr+sP6y/sH+xP7A/sn+xf7S/tX+2v7i/uX+6/77/gr/Df8T/xz/Mf86/0b/VP9k/2f/av93/4X/l/+k/6//uP/H/9L/zP/a/9P/2P/p//D////3/wQA//8AAP7/AAAEAAcA9P/y/+f/7P/i/9X/zf+//8f/uf+o/6L/n/+J/4b/dv9t/1//Yf9Q/0z/Uv9H/0b/Hv8h/xH/FP/+/v/++f75/gD/8f7q/u3+8P7r/uD+5v7k/vf+6/73/uf+7P7n/uD+3v7S/vH+4v4F/+z+KP8U/0v/O/9h/2D/bP91/3b/pv+I/8L/n//o/9v/DQDl//r/AwAuAE0AkQAhAfwAoQG2AckCBQNeBFwEzQW2BfEIQAipDD4TDi2pPl4jRggiB78ZuBAv/cwDsRN/EsH9j/Yg9d76mv0+7g/vr/++DvkJZP59AnYMwApH/Gj6aAbVBqACuvZ0AI8Newag/nz8Gwu5AWP1CPWHAgUL9QIrAEIKxw+S/8UFSA4iBQT7ZgQXCiYUFhaGCjQFbRK+EbcBKAv6EBkIVwBEAY0F4AZYC7wCpfpzAiX6v/Ac8lX4HvZ59yX5dPMG+g367vai81H3fftO9YHxjfVuAqEE5/zz+iP/0fyx+Tb99P2DAaz+of2e/kf+QAHo/xv9s/hs99/59/gf+eX4UPnT+kv2ffU59cL2zPkE9gr3/vag96D4q/jh+db53/rH+sL4JvqT/O7+3ABX/Xb8tPx6/gwAUwD//s/93v97ANz+H//q/+oBcP/x+Z36CPt8/P38+Pzs/Lz7l/l09/330vf395z3U/lu+OX4l/mi+VP4i/S08k3x+vKX9LH3EPcw9MX02vQs9MXwO+3f7QXx4vA+8E7uj/E98xHvueun6BLtRO8N71rx5/Jt94b2GvGE7zXzUvYJ9uX0gPP79d372/6b/Mb5Pvmf+qb5YPl7+1oBmwJdAugAlgE9BVcDmwE5/T4A8wQ5CR4KfQkyDKgMWwpsBnQFpwZ8CJIJrQoZC38NGw5IDO8HBAVLA9ICSQOfAzkHbwvRDioMwAafA5oBGgE3AG39of8GA8ADYwR9AW4Ahv60+bn01fEb8+T3K/t0+fv2sPU79rn0uO4I6kDqKe1V7dHrFO5N8sHyc+8h66bpI+wK7tvsLu4Y8Gr09/d1+Mf36/ey+fn5mfhx+Un9MgTPBhoI9wdtCa0KRAk6CQIKcQ1GEHkP3A84EEkTuhWPFQ0UIhNbEiwV8xXrFiUZeBvrHVsbjBinFm4ZXhowGQYYMRi9G48aAxkwFaYVNhRKD1sJ4gSiCCkLLQt1BaT+Pvpo9jjywvAP7WPtf+qx50rnIubv59njI9/v2E/VydbG1enVfNRs0zfV/NKh1/LWzdt43rze7OL+5NTt5/Et9kD3ffrn+mP8ZP03AAgEcwWABYkE0QdiBvMIugbwBrsFfQYeBjsK0g7hEaMTCBG1EEMR9xTVFnoXVxlCG+4d6iBUImkjSSb4JHMkfSPFI1smTSdGJockNyQSJGgjuSFAHrYc1RraF8AUKxKXD0INMAyXCLwFmQCL+831ZvKX8S3xivGh7LTms98Y2s7WQ9S805bQktG4zfTM0MxiyQ7JecXTy/HSxtx34JHiSudl6jHyBPO59sb6BP8/At4EnglAD3cSEBAXCdwEega2B6gIlQVMBhIG3Qd3BQoD2wJpAgkCKwHvAbAG3QsCD5gRDRKWFXAZ8BqMHCkgQCU9KnMrwCx7LeguGjDHKxwpbCZYJoEk6SJEIh0h1h73GrsWJhV/FdsUgxKeDpkNTwyCDbQNZwyeCgkHzwJN/sb7D/ve+a/4N/eB9HvuzOpC5G7gNN112unVMdS008vRY9I7z0rQy8190f/XidsD4KfiEehd72L34fnE/M0A7wUcCkIN+w8EEjIRHREjDXkMGQ7nC9UJQgTJA/0CPQMHAND7W/kk+Tv7rfsX/JP9Rv82A3sFQQnSDYkSQBV/FRQbgCJvLLUuZi7hLp8u9jP/MHMuTCrsJ4coHSXmI60gcx/YG3QV9hA0D9oRzREADq0JWwieC1UOnw1aC4EIwQclBVsAav17/R4BaQBE+t/xhu3H6GnlRd/n2h3Y/9Zx0BjIT8V8wwbID8YcwdvBOcj20WLarNxT4VnqaPMa+1H8zQGxCPsPMRPFErUUCxksGlUX6hGeDzgQ1w6OC+0EAwMjABH+R/lU9IHxivFT8nTzbvTV9mn5Ifzl/dcAowY0DH8TMRcjHSUghycALnIxSjRaMEIxsi2rLT8rfimRKb8lpyIrHeYaNxduFKMQeQ2NC/AJKQk9CNUISwjvBy4G6QbJB3YItQYgBEID2gFA/9T5uvbR8XfugOZM4q3eTN+a207RbcjHvkG/474ouqO2ZbwcxezPotXD1rzZiuNg7aXwbvuUAqkOehYaGFcZOB1+H0EdERjSFBYWhRYMFy0PcQpvBTsAnvp79PHuwu1y7drrhOx+78vwcvLZ8XXz3/msARYIVAuHEVkTvBxeIe0o4S4rMcQ0EC49L6stDy6/Lyss6CU8JUgiex9kG7cUIhK5DocOVwpHCeYI+AipCXIGFgWVBN8FBgUbBUcFyQcACPAEhv7M+B34r/UI9QXuqeqp5bri1NmxzVfEBMBGw1C9GbePr5+xYLp0w07LTdF01L3ZDePZ6Bv1YAEoDgQXNRnmGfsdfyDoITsgeR6RH20cNBg9EJgLkQc9BF39XvR87fzoB+jG5ZDkaufP6EnqM+ru66bysvrBAc4Fggs1EKUXtR+KJPEuYDNVN6k1aC3aLjYwOjTsMhAraiZcIeQgyRqYFk4TZw7yC4IFuAK2BMUHiQnBBawBHgAiA3cFcgbVBYoHEAlaBdAA5vqE/pT84fhu8FfquuhZ5SnjHNiHzgbG5bxeu7e2rbQztH2zGLr2vw/NqNL/1xffQuW58Wj6JAdKD7cZ5iC+I6AkpiQuI20jnSK3HrYcSBaQEI8IIQMP/lD6yfOV68PkO+G04HnjveRU5cfltejz71T1n/sXAEQFPQ6/E64dDSZGLLM0YDWzN5Y3YDzyPfA5rjWKK2AqSCiVIxscixbVE74Qvg35B7cExQP7AnoBGwGUBK4HqQl0B90GrAlpDn4PcQscCMMESAQXAW7/s/n99eTwBuq/4rLe8NhP0GvFQblntgi3LbRwrQixk7Q5v37NkdPy1k3e2elR8qv/2Qm1FKcfzCXSJyAphSsAKgonSCUTIhkeyRoUEtEJkwNa/Q33l+955VjfKNxr3IbdK9904Mvh9uS36A7w0/WX/fwGzAs3FIob/iMjMOQzUDWVOLU9lULFQng9+jREL34sNSlfI8wfFxz4GDMSfwlXA1oE8gMz/hf7rfuNAbEIHgdkBPEFoAkSDU4MVwqLCdYNCgwhBEP9Kv+y/E/29+v85MrjSOXg3PrISby2sPKy37PYqcGkM6zJsl++u8nZzerXreKn7JTyZP5lChAWPSGFJRcp4S85MsYtqyg5JQgjhiBFGkgPygmIBbb+iPd57o7keN+Q2qDVkNSA2RvbCN1H307kB+6f+Hb/PwCXCFAPZxqoJUksfTHBNjM73jsCP01BkD62Oo4yyynXKO8mQST9G4wWTA5KCrAF1gDhAM7+P/9I/Sf+XAOmB1MKXgmdCPMKDg+uD4MNdQ6ODFYLGQciA8L+K/u7877sDOYS4UjcDdJqxNC19a9YsC6t7KMLpY+rALfKxSHOR9Dm2dDmB+0D+8gDnA12HM4lCyo6MF4zczMCMEEr6SWQIf8gvxjME3gO1QaY/+X1BOtB4gndiNgM1bnWf9eN2pnfpuJE6Y3wOfZ6+3IAcwgzEgwgAioZLpczhzZoPfJDRUabQTE+HDr0MkQxRCtUJ48jmx3tFCkP/AoOCTUHDAKL/uf9DgCEAv8D5gQeBkAIMgnZChQN5A5nEKYNKgqXB4sHnQSN//v29vLZ7Cjov+BR16jPAMI1uHCx+a0jqtilMqhbsLW5hMrw0I3Vqdt24sDv/PiaBhcPgB09J/crXDCGMTExTy7HKigliyJEHp8aXxVDEJsI9/8v95rrA+Ps3RLZCtlP2a3ZXduc34Tj7ejm7R7yPfcc/zsIBhLfHZkloi6+MtM4Az7VQtpBkD0mOI40kDSsMRUw3SbkJJUdNhlfEjgMuwsGB4AEMv+YAF0EyAcZCEMELwRNBVgHmgipCRkLFQ1kCyMIjQWkAOYAxvkV9UDvD+t86dfhidmhy9O+lrn1stWuxqaVos2rvbKmw+XKmNAU15DaBOYx6kn3UAP9ENwdZCSyKvAvbi94LsYqOidQJh4hIR1VF10W+BG5C5MDLfhK7yvnbd/g2zDaPdvZ27zdRuFB5rzqk++88XX31/4zB4US1hkCJe0qCTI6Nqo8yjqfPK471DZsOzI2Lzf9MagwRSr3JGkdVxb+FNkPBAxwCBIJQAkQCqsGeQRaBkYG1QQHAp8C+wXwCbcJ1AXEBHoD4P+6+8H4TPa79Z/usON821XTI8tsx0e+Q7f4swasEaZhrye8RsArziPO8M9G22TlZ+xH8w8DCAnRE8EdpyBmJL8uQCwIKXAoPiItIPkbpRywFUYTXA4FBM3+bfc27F3nyuJm3hHe795u4ATkjen47LPucvOS94/6dwXqCbYS/hpdIe8mQSw6MwEzvTV1NAM3ETtbN4E0GzJfLnwtCSlpJQ4fpxqDGH4ROQ8UDvEN3wrkBssEcgOHBqsFswIpAXkCYwIOAkAB2AAc/RX6IvmO8mzz7vC264bmhd+v1WHPYslcwLC6s7lgr0OrhLeFtQDB9MgxzLzS9NrB47niMvAD9+f+UwlyEQkXOyFIJ4cldCRxI0ggWR40HuYXPBlRFo8QDQwcBSb+sPdk703oHeW95KPji+VX5rbqUO3N72vxKfFG+Gz7NwJJCgERoxdtIOciWicuLjQyhDMrMq8wnzCVNFwznDIbLlEsmycxJKEdrhrFGmoVVxEXDAULQgw2DEcHngPtA2MCbQG7ACsAJQPzAfn7KPn29y73A/Qb7zHqjOgp6GHfiddVzyXHZcNqwQG3qLInt6a0r7rZwKDFe8kH1PzVjdhI5rvmcPMz+4n/uQjIENsW9RvkHMkcsxwtHQ8ekhpxHUMZvRe3FPoMuApDBu3/1frs82rvJu//7TPsDe0c7inwNfBt8mnyrfZX/TP9vgQtBmgMSxSFFZEb9SClIjsm8iWPJmYshzJKMysvly+9KDoqMClgJUQj8SBQHboYbRfhE24ThxERDkQLUgoFCaEHWQeBBZgDQQLw/vb9sPub+F/0wPKn8HvsKut/4rHbvdbTzcDLGMdGwsK/Abp0uDC7zsF/xLHI7sixzavWBd344xvlUu628X/4YP9YBBAN1RJcFNYU6xXxFn8azxc+GRgXRxZhFEIQcA4fDGcIhgO8/xb8WfsL+Zv3Ifj49mP4Ivk6+eT6MP4b/hkAIAIrBMwJrgwBEmwT8xf2GRwciiDFJAYotyXHJ50lNyfWKG8nVSXtJFIjESFcHxEdwhzIGioX5hPpEVwQ9BBvDgcLnwltB2gEwgHp/5r8kvrO9/TxSPEX7mrrO+j/4m3hJt1k2l3U7c5FziPH5cYbyebGuskAzCLNxcwD1TzX8dfx4dfjh+ek6hTwIvLT9+7/VAFnAwYIVAiuCV4PZA3mEIoRpRDkEFoPRw+vDuINkQqwCOkHFAYRBW4EXQR+AwoESQN3BAwEPQb2BUoGiQhVCJ8LEA0JEMESmRUEFioYlRdAHFEdvxxMH2cdfh84HZQgph5FH3gf6RxDGwMaMhqWGLwXqRXWFC4TERLhDn0NSQoICv8FDQOFAFz/uv1B+Xz4APXe8hjvie0L6t7pVOdv5HTenNqN26bWmdaA0n/RT9Hs0lPUHNNK1rnW+Nid2cveouHo4QvniOXL55LtF/Pl9ev3A/lx/J/+k/46A1YDPwaDB8EIUAqDC34N7QumDQ4M4AoACygLZQqaCgsMRQuSDCkLqgxeC/QMAQoqCbUL+wkVDYAONw8YEbkSnBCCEh8TWhPyEycUbhN4FJ8X6RX9FxwYQhmLF6IW+hX9E3AWmBNxE+kSmBAGEWgOAw2/DOEJUwcdBRUBhP2PAK/9SviX9yD8yfUi8i3xbOtM7rPrC+j/5LLjYONl5SjdKdiw4q/h8tgl3oPfm9wa4DLijOKN5HfleeIL5iflOeag6aXtze8p70byFvWE9U73ivzS+w783v0HAVoDqwT2BvUHGgguCaQIiAjAC8kMGwx5DlEOvQ33DjMPFhCSDVsOXg+/DZYNAxE1EX8PaRFNEAgO7g44DwEPWg/lEP4Q9Q/3EPAS9hKYEp8TkxI/EmQPghOKEfYO7xVIEQQPIxI7EF4IUQ0fDqUJrAF3BwoMCgCxA5wBMgLi/oH92f1s+tv2Ufhe+IfxpPWg72Pu//ZY5ofubO8r4nzldexb64PeC+gL62flkueA5lbkuehN5t7nSOdX537p+eu67C/uI/KS8XzyuPP5+Bz2XvkS/OX7F/9yANsDTQVPBacGQgcBBnIIcwuFCk8KvgypDQgNyg75Dp0O3Q/aDQ0NsBAGEG0QQxBCDD0RhQ94DTIQhRA8EnYPpQ8aD1MO5RJGEJMP7xKhE2EQEw3UFIERsgraCasOxAkaCr0NFgv5DaIHRQcOAsMOBQF0AnwE6gVmApf3Vwty/ykDpfmF+6/6ovUs+dXyvvhn+OL1Merk8Wz6qeir6e/0Gun+5v3sLO117GvqDOzB7MXmCep08aLopuYu7Wfwi+qB68PyLPJx9ZnvifKW9vb3kfju+LP8i/1t/0n/BQKEA+oETQQfA1wDgQVmB5YM+gYjCKMMzAtAB/QM0w+9C3EK/gyvDlUJCRCWEOUNHgswFPUJdA6uDr4UQxISB+8TcQuuDBIPgBL0CiAX+wsxBQUMuA8KC0wFOg7/DDsO0f51DccOhAT+/N8RYAtu+MUJbAbiABQB/gKWA4z+Ov4A/jwB0ftZ/dP2xfUv+Wf5LPfY8az6kvTV7cj0X/Y66fr2quoe72r1iOtY7nzxjfHJ6Obt0PT07UnqAfGu85bwJetW9RH43+zf8Qz61vQe+CP0Rf8H/OH2FP/P/+oB+v04/yMFsABnAwsGYQfeBTAGQw5eBCgI8AoKDxEGIwrUDiMPKQV9DBsVwwL4ESYRXQXdDtIPpgp7C74I0RRgAsMWfg7nC0QNWQKRFUYGs/9KEY4WgP79/7QHEBeo++oKygq6/nwHbwDkA0cByQfn+90D1PxPBv/8Dv1N/gb8Xf/Y79X/nvfW+73xgfzB+Ufsm/RQ/lPy3+yw9/rztu+I7O786+fr+LLxfusi8kv1h++z6OT58vMQ7DPulwBw7T/uyPup9tvvDvNB/Xr2avk9/s37hvj6AI8BGfoG/2wFy/5bA28HCAAZClAHjACFCzcFiwjkCBgFFg2wAXIOHw3ZA4AN4Q5tBswM0A/RBY0NFhPECgYEaA/PFIoJo/y0GZERj/yDDIYJsggpEFgGfgKhEWcFQv3aB5oODwiC9PsGqRAp+B8ABQkvAh/9fwAWALQCTf+b+O39tf/Q+ET4nQDB+RH7Se9G/jkByulk9Dj/0fKY7oP7MPha7mL2k/vT6db5yfxM7K7xePss+Pzuhvax+bzzHfDl/Gz7Cu4H+jwAi/Ql9ZUE8fso83oElv6s+fH8ggZ1AO78XwNJAlkE+/5lCRwCdwWMCNQJFQF1CcYWlvuTCKIQ8QpYApwQ1go0B38VQgS/EhkJiAZ2EVUPEhDT/agSSBa1/FENnxBR/v4SFP9FC3AJY/4mHIf5tPyyD8kAUwL+CjIIevnD/X0Qr/QWCD0B3vqfBWv44wGp9kgCTP6U89n7zf7e+IPzHwEi9tjyMf2n86jxCvsF+7Dr4vti+vvp0Put+5PtrvaM+v3wLfmN9AP2WP7c7zv74vil9Xj81/R9+kn+n/VOAXj2IQHX/nr7mf3BAcEAIQB0BB3+3QZMAWMLUfq0DFIGiAb//k4JTRFr/VMGgBW3AP4EtRyx+0sSEgSoGJsJRP3SFhoQYQK9DJUPjwmTCjsHEg4MAMYJGxCIBuAJ5AWWBvUCoQgvD1n6dwgqA4P/9QP6+ggCSAfc+aUBAgC9+M8Ge/kT9b8DMvxj83j70P5H98f1UwGm8YL6+vhl73P8W/Rz89b6evPX9n/0WPQsAY7tLe2dBcrwaO7F/V36QPQv9M0Aye1AAWf8fPCV+kAE0fc378sKD/i//u75O/8RBO/9bfzG/FQMev2K/h4IvAMkBxYCIgg8BrEE6xFEA9ABngxlEwoDqgcvFWIHlgoEDj4JOgQJFfAMxvwLFl4NvP/FErwD1Qv5CC4ByhGVAN0EfwnPBvv+rgFaBz8BhP+GBRz90PyUArsBHvVEBCz7rfwvARnyygBN9r0Aeex69j4EFPJq72P6kflJ8/HvGPon+KbrYfjR/IjpzvA6AjLvvOxq/y3yQunVB7PwqOuy/eX8afLg8SME6vZv8qAAPv/L718CUv9q9KwIS/t//gcDh/yWD5745QBkEdD2sAYWE1v3yAHFGlkB3vnkFkkNfAPwA8APERDA/yIS/QC1C4URJQC1CRgPpQZSCDsB6hAUBtn+Rxdd/SID+gj2C+8BUgUCBIMGY/sDA8MG+fdfBUT8PARV/YL43v7+/b32lfmG+AD9w/hG9Tj8Zvf89C32ofVo91n2ZPDx/PXxlvL1+Mn12fQg90/1p+9D/EfxqfG2+Sr4mfGq9uYCRO249dMD6PhP7YYEbAAn8TsIF//3+GwDqgS2+o4CSgfe/1YA4QYXCw3//wWBFl/5UwV1FPcEMgXGDpYPhQH7CZUT+AjzAxUURwshBE0OTQkFEaAF+gcDDwcGmBA2Ak0CQhNbCyz7sw0rCfj/EgOqCCAEQ/6UDGb9a/1xB0P/CP0+AM3+Jf2Y9Y0BgP8P8M3+ff459h7yyPrk/wvucvduAPrusPb8/L/wePOu+xr2cPED+R36h/EY9P78OPff7Xr8cfva8Uj7jPpU+/P1sf+I/I/4wwD5/cEAOP4iA/wBlAPFACMIrgHBBfsLwQG7CHwJaAwpBYcLpQwJCRAKuw1HC4ANJAtxD/8MngppFIII9A6JDu8M1AyEBfQP6hBXAusPrAu+BZcMOAAHCpwL6/5bAGEJngfm+OIBtwxB9RMAJQMz+r/+yPt2AgDzx/b0BM/0Yvc4+tj2bvuL8Dr5kPQN8+n56u0H9In9P+zx8ev5xPBZ7ony/PaZ8DPx0vRN9S3yk/Zr9hf6UvT5+sH4PPqI/D/8fAIm+7YA2wN9BNkDwAUaCQwG2gV1DEcKtwsVCIcMJw/3Cg8O6AvIEM4LXgpXDngNmgzjDcEQsAuMDRwTVAn1Cf4RWAnSBq4PawiNAlwKCQx9BtYDXAkrC8YCPP3lAFkIBf+y+JsDDQFr+eMBRPzp74IDqfgM8bL5KvWH8RD0M/7R7KPsZf0R7JroifPL8c3shu9x8h/s9e7N8K3vsuuN8UnxN+368wzxJPQw9eT2FPcz95z5PftZ+sn5SAKiAd7/CAM5B6QC2gZACskFjQkhDBALkgsfDCIRAQ5hDk8QogzbE38Uiws1ERYVvgyxDl0RGBOdDfwNrBAgC4wOoBFoCBMM6A9+CtcIZwm+DmkH+QuYBEYGcAVG//sKQf80A8kGpP1X/sf3G/wa++3z/f5t8nj4ZvZZ8Wf2buhz8gPtO+ik7e3vEPHm6P/mdu1v6bHkNuJB54XwNugD5RXvW+/D64/sBO2H8Ift4PLl8gvxDvmK+u/6NP5/AU8AQQAnA7IGhwWrB3IKmwpUDvwOdhBYESkSehKPEFUSPBSMEvcSjBU/FG8SdhMHFTMTYhI9E8kRnA0dDsAPRg4pEFgQzA4QDEgJPQ+HDLgHBA+VCLcI3wfoBJMJ4wnsBGD/cQQ2Alj+kf7F/x/69vyP/+f3e++89fz06+pk8dDxDu/H6XTloe6f6J3fweIk4VbiYuFi4eTnuue64XvkwuXY5ULkqeet6ErpxPBd7z7xEfVu9Vn4DvrF+Sf88f9dAtQCRAg7DDILLQ3XDwUPDRGSEdwQQxWvE+MTBBc5Fd8X1RbeFPQVzBT8FGETOhLOEq0RfBCzEKYPnQ84EEcOlg66Ds8Q8Q+YDKMNsg9PDtsN0A3xDDAQGxByCNkJAwyNCY4HawX0A7ECowMBAg77Qv1Y/mr2u/Go8NT16/Ks7EfszeqJ5a/jQuai4qres91z2qrcJt+i26ThkOGs3E7eoeII58viEuQV50Xrce8/7RHwLPY8+H75DPsh/oQBzgTsB9QH0gybD14QexAsEp4UDRYTFvAVShlqGTgZdRhMFvsXCxhtFiMX0xXOFSkV1xM5E0URlBD2EKgNpg2lDiwN0g9nECwPkw/OD5UO6A9tDzYOew8sEh8SaxCeDhAOKQ/uDL0HagWECCgGKAPeAiMAG/0Y+y75mvLA8P7vl+7z6hDosefk4zDhUeCE27XTw9Gw0CPRo9c83Rvgn9zW2vrdnt1E4iXifuV4577q5PK188j54v7eAK7/oAACA/UGhQtDDtURLxX4F4wXxRfWFzYXZBa8FRcW4BcqGb8a/huuF/4XsRfIE58TbBTYE2kRXxBjEOgQcxKqEkUPpQ07DW0Mcg2UDy0SuBJ4EnISLhOiFHgXXRUOE8cSSRQiE1ESUxJ3EEsPsgu7CLMFzANhAlj/Nfzu+Zb2a/HT7mvsSOfH5FjheN1F2VzTVNDsyODH/su4zRDSk9D+zsLTy9Tg0zfZftmR3UPiOeRa6rnvNfYL+7n/2wDpAEIGuAqTDY0SjhZWGRQc6B3PHRkfvh4tHSwbbBqEGrUash07Hhwcmh2SGWgXXxYQEzETLBLaErwRiRFWE18TwBLFEGoOrAytDSoP4hHGFJoWoBekFtYU3hV4FdkVWBZlFcMUAhVVFV0TiBEODbYILwV7AhP/4f1G+xL3zPJh61zoFuHZ3XfazNOL0SbMm8kKxGq7ILhjvHjDrsT7x7LIXckKz6fTVthb2oPg++SB6TjxG/ef/0cJPwzpC4MP4xKNFpsaoR+KIm0layYBJgkmZSWcIiMfXBsDGRAZAhkBGv4YQBhNFzsT1BCADkYPdhAvDiQNngzsDpwSPhQAEhMQtRBZEMsQvhIzFgYa7htXG7QZ9hrmHZAcpBlNFwAXKRh9GKYWehSwElQOoQdLAVf9ivpB+ATyDe116D/mOeAB1wfSRMyYyNHCcb+Juq6yGq1nrQ20gLzVxmfJ0MbCyb7QQ9bZ3z3lo+ni8lH75gK3CpUV8RqOHEQedB1eH8IlwynILIouvy5FLUMpECbqIHYdExlkE1IQ1g7GDvIO9A57DWcMDAm9BtcEIQScBPoHEwv7DIkSqRNBFuIYPheoFYIUphVgGmMfMSPCJDUl9CWvI0UhHB4SHYkcZhueGWoY4xjwFgkTQwtLBOb9Cvku9BbwJO376FHkI90/1cvPS8lRw/m9hbnRtCaxEay9ox+o37Qwv07Fk8lax/fKzNeU3HLmc+939rv+Vwf7D8kYiiOfKP8npybpJmgoyyzfLnwwMDDqLC0ovyKpH0AayRMJDfkFrAEhAlsCBQNQBFUDNgAU/Yj9T/67BA8IzAe3Cj0OPxS1GXMeCh7FHcweix4fIUolJirbLHssISqjKG0oECgdJUUhRR6nG2AazBmiF50UhA/RB98AgvmT9avwYO7g6Yjjmt8n2WjTg8sExoXBk7sTuY62zLXrtImupqWMpdiuTblrxmnQZ9QB16vd+Od87176XwB7BqMOUxeGICko0i9cMdkvdi0GKScmKCenJxsoDCbiIHUbkhVwD48JKwJZ+972gvP38nH14viL/M/8ifqY+PL6aP+bBSsLIQ8ZFLcYEx9fJAsqcCzcKtcoYiagKe8uFDPsM/QwaC5IKyYo/SRiIQoeRxqoFvYTaxOmEhUQAAl+/3j4UfJ978Hq4+Yv5IXgUtwg1XLP58m7wzS/OLiOtoO4TrcZslKufKbPpQ20QcO9zv/TIdmJ22Pip++89fH9PAdXDZsUUB7HJnst8DJvM80ukCpLJzAl1SWpJWgl6SH7Gk8UjgtRBeT9xfWd8aftaOvD7QvwcfJi9an1uPRU9C/4mfvqBNMLbg/fFXEcOCRvKqwuMCufKaEqTS3hMMkznzZPNho1Ly8xKtYmvCRgIQ8ccBcHFQIW5BUTEucLtgQB/nj36PGF7lTskO1O6vDknuBm3VTYntG5y9/FTsLCwMa+870bwAa7PbaQseinzapku8TLLtu34lHiQeZm7TP4Mf8vB2oLQxDWGcMhfys5MYY0OTCdKWEkFB8JHtMd2BwEHD4ZcRIADU0EC/xB9OLsv+jO5rvoDOz67yHzh/S98xr1nvYQ/IECRwg5DWYVKh+ZJsIuhjDCMDowSC0nLJct3jGbNQc3nDWBMDQshyi4IyEe6Ri2FXEUIxSmEvIQ9w00CuECNfyn9ZrxZPAT79DtX+u36tjleeLu2i/Vo8/1ylDIasTuw+DBssLDwS26YbRfq2+ojLdxx+zVJt+m4S3jIurI9VP7jAJaB7cMXhTBHVAmZCynMQsxpCvzJLcfmhz8G3wbPholF5wR3wyLBdb+bPc47ibqxuWo5BPnUeoz7+LymvSw9Hr1GfnY/L0DtAoFEbYaLCTaLPAz5jWUMq8udysILXUwGTVdN7s02zODLlQrzyWLII4cuxXMEuEO5Q8NEmkRpA1RBQb+i/fA9BDyCPAo8Dnvcux06PblneFD3pTX6M+iyvbIH8jyx4zHBMN0wAO9xrFRqgasGrgPyXnbZOQC5BjqeO979Lr95wEcA8oLGBVAHW0oSjCKMWsv7ionIhQd+xs+GnMYixgIFl4SzA6uBxf+5fW67VDlxeQT5LDkL+r97tDwZfNT9AP0k/YQ+oz+EgXqD1YZXCMfLgEzrzZhNp8vPSvqKlAsEDDRMRgzejL3MDEs8CQVH2IZdxSVD04Nbw0qEIwSJg/xCSEEmv9++pj2dfPm8bnzafLO7yLueerD5S/gYdni0o3QjM/VygrMRskVxfvDl73lsKCubqvTrHjBBNQ54nPmB+nX6DHvyvtX/m0BowhYEMIX9yPdLBEvGzHxLr8kAh5vGqcXiBhsGA8WAxPKD1AKov9J9xXvfOVp5EHiR+Gk5XTryO5E8hv0iPII9RP4uftzAJ8KSBXTIWkvmzY4OW836DJELMMrTy2OLroyPTOlMkMyLS5hKDggKxpiFboOIAzBCsEMJBEsEP8KXgROABz8x/ib9FPxOvGD8oPwee0L7Xrr2Ocg4FPZ6NF80WnQocypyc/F7sKFwAi5q7Jpqfmsl78VyfDXNN2E34LiQevv91L5NQEJBQYJ6RHGHD0k6Cp6MMUtQSdtIxIgORvXGvAX4xR8EgkPLAqbAzf92vT361XmQuK44N/kK+nS7J7x6vIX9AL4zvvsADIGdA09FgEehik6L9w0nDYRM4gwEy0qLgUvmDHaMUUvei4wK/QmnyEDHEQXIhJPD5UMCA28DmkNxQo9BbEAuPuo+dX1s/L98erxNfDt7jPvTevO6LnhKtuR1P7Rq89UzXrMQMcowcW+1Lm0sjGtoakktZvDd9Hu2infD+De5D/x2/WX+7n/TwVaDHIWkB6AJNUsCS9bKfMjAiDTGhMa5xh2FkwUrxLKDy8KXgRI/T/1Ju/36ELl2eUV6uHtT/H49Hz2Dvk6/UcAHAS+CPUMqxKZG7Yl2y3OMjYzOjBWLtkuji4YLzwuWyzgKwAqKyjDJe4jkB+qGZoSyw0gDfANgQ3pCp4HJgaYA2IAOfx+94j1OvP67uHrMuy56wLr3OYW4NzaQtc/0vvOvcusx5nFZ8Tjvxe8y7khsyiulLfBxvfNf9z63p3d3+ga8m75Ef1SAnMC0whnEiMZ2R9PJ3Qp7iWGIlAeMhxQGm4XXhINEK0P9A3wDJkI7AAZ+0vzTe166rbqhOxG7+7yiPXj+CT90gBcA/sGrQepCm4SQRp0JDcrMS1tLpMu8C3WLEwshiv5KzwqHSdWJkUlkSbWIkwbwRSpENYPfQ5fDHwKcAkiCc0HnANx/2H9R/oM9sPwDO6e703wI+6s6UXlaeDO3fDayNWc0sHPb8p9yEvGDcL7wDy+7bemt0XDTcth00baLdo636LpaPKJ9Jf50Prx/bgF/QtbEecYNR8MHsUcbBuwGZoZEBgnEwoRUhA2D0kP3AyOCFID5v4y+nH2+/XR9ED2lfYc+Gb6F/6fAT4EKQacBlsHmwoAEDgWaRyrIB4kMCWxJR4ljiWhJmQmYSVcI2UivCKkJIQj5R93G6EXtxWlE4gR2Q+iDhMO0Au2CDMGOgSNARn+Pfj687Dy9fKE8Z/u8OuY5+nlZuJQ33Tbf9j41cvSuNCtzd7L9Mt3x2bEUcFcvkvFcc1d1I7Yz9304I/m9+888nfzIvXu+NX69v98BgkLFxLsFq0VpxQKFjUUxBNdEkgPbQ7DDloP+Q1+DcALBwgkBrkB2f12/BH9a/wJ/QAAtwGoBG0ISwknCmwM0gyfDg8S7BV7GTkdzx3sHO0dCyArIHkfuh4kHbodchzgG7oZRhozGRoWsRNfERYRgBAUD+8LRQrmCUIJ3gaLA57/YP1m+pj3kfUq9HHzKvDc7gfsyeoa6a3mZeOt3/Leidtv29DZbNjA1QLUUtFQzVPOYs7G0dPX6dv93HHhMuX46N/tqe7f7kDvPvJZ9c35DADgAwoHrgssCx0Lug09De0NVg1LDWUO2Q8ZEUMQZRBqD4INiQudCHgGbQW+BcYF4QXPBtsIdQl7DC8M/gsuDr8Nmw9lEFMT7BTXFRUWpRQJFswWXRd+FyIXZxdAFycXHRcsFs8WKRZzFAQTCBJNEmISlhEpEGYP7Q5DDjAMKAklBoYDagKS/yf9G/xX/ET5w/Yx91D0KPJA7qDtJ+zg6Pjl9eOh47Phad3y3Bnc5dcW1kPUXNRR1+Pbgt2p4CfiaOTq6ETqw+oO6i3tkex47xL0P/b8+pkAYwLJA0MHmwetCSYKKQqbCqYMFQ5FD1oQ+hB7EaAQbQ4sDUsM5wuwCw8L3govCmQLNwxUDMsNxQ2kDfgM9gsXDdQNExCdD+kP9A+KDy0R6BADE5USZRNQElARPRJ2E58U2xNjEscQLRAVEIwSjBLWEC0Pfw0LC/4KNgoGC2cJPwW3A1cCsgClALsAk/6Q+7v51Pp9+HH3Dfkj93HxZ/DF7xLtwenj6WvrCemb5lDi7eI74HveieDA4hvjDuPy5/nnmehz6y/rmuya60/qDezI78PxAfUx+aD7JvxU/nYBVALFBCAEwAS1BQUH6gidCu8M8wyMDI4MVAthC5QLUQuJC/MK1wrXC/AMBQ5bDo8OrA3zDBsMrgv9DDIN2Q7TDk4O8Q0BDjoOxQ4vDowO+w1yDRsP7g0AEcARehLAEeQPoQ+0Da4NgQ5qDv0LTA00DXgMHAtwC3UJYQZhBNAGwgN9AdUEOAMDAHn73P6gAlT9Wvn7/Gn9mPg/9YrzWfJi9DryJO818lnvFOtt60jnl+Sd54PoBeZu6IroxOjG6nnrC+zh6t/pjOiq6wDtLO0R8N7zmfIF9eP3A/i1+1f8ef32/SsAvwC8AcoE4QXhBq4IIgngCLYJownsCX4KDQo6CpgKBQtSCz8Njg+0DgoOxg0lDbwMsw0CDoUOEg6gD/cOSg+OEE0Qyw8eD0UOmwyWDTMNcg4ODqAOHg77DZQO+A2fDW8L/wqBC0cKYwgpCRgKLwrMCBkICQjWB/YDMwPPA78CMQExAOMAKQDX/hT+R/6M+/D4xvgx+fv5lvZO9Pn2IPN48CbwCu9B7KHrkuwV7b/s2exY76vu1e377UPtr+wA7ZPtUu057c3vGfDw8PLzi/WN9nf4qvnB+FT7w/vm+x3+if/OAAUCgwNiBZ0FqgU8BskFYAaDBvEGlQeKCLYJxAqLC2YLbwxeDJsL6gzGDIIMlwxQDBMN1w1nDiMP+w1HEJYO6Q6uD60Ngwz1DPkM6wtMD0cNLw1ADR8OcAzbDYwKxwdDCmkIFwwsCpILPQp+CtsHSwZmCIQHCgIGB6IDpwFICLUDgP+5/DACSwBx/2/+Y/yA+sn6r/qT+Kn2i/ZI9v7yq/Qt9GfwoPAn73HujO9H8H/uaO7l7d3vWe+q7nrxDO5y74zvC+/z8XXxnfFb9PDyoPTP97z2E/m6+LX5C/tW/NT9OP5Z/6wAywG2AdACgQNWBOgFsgbZBp0GLAgyCtgIAQvRDOALfAy1DZIMlgwBDhkNQAtVDTINnQ1eDsUNMBARDjAOOw5fC8wNcA1EDKoMwQ0WECQM+wtxDC8Lww2NClUKJw/DC8oHIwooClsL8wiWCPAEQQRCB6MEjgLbAbMCJAMK/6b9tgL3/GL8ZP+F+GH4lfqJ+Rj2y/gy+Ivy1/QZ8yDz9+568VTxVu6I8Wvv1++V7yDw7vBs7wnvWPB/72zvcO4Z76XwXPCG8SbyQPTb87X06fU0+fv2lPcB+bX6p/o1+2L+6/4pASAAHQIlA0wDLgWNBSIFSAY4CBcIuwh9CR4KXwvaCgAMsgwXDPQLkwt6DT0NsgzHDdUM5w7BDpwP4Q/CDu4MmA1vD/0LwQxODsMOEwxgC3AOnA2wCtIJtQoNC+kMcQp5CdoIhwjDCOIFEAclBhgCEAWrA4ICXwDbAE4B6fwsAHr8EP67+Xz4Bfn598D5jfKP8L306vJL723w3+4z7m/r5Odu6CPpdeqx6sHqMerU6QvsUu4s7oTu/e1m7TXv0e5T75ryZvTF8372T/eL90n5jvlq+8T7+fzZ/UT+QwC+AJcC8QI4A4oEIwUsBmMG7wZPB04JmgmWCWkL7Q0oDp0NjQ6BDcwNXgwTDYcMUg3wDi8PAA6NDqYQ7A9HD0MO+AxKCzMMXw0dDkQObA+gDg4N1w0KDngMHwvuCn0L+wmyCdAKdwq1CvoHBAnvBJgFvAagBMsAnf+XAYr+Sf6n/eT+Ov32/Nv4cfXP9Tj0k/Jl8fPwpe/+7avtOOr356/nOOhI5ybngOWR4z7nXem76W3qyOoU63rr2+uN7I7sye4r74vwsfC99Lj20PWQ+bb5z/wz/PL8jP6i/u3/FQGzAjUEGAbvBHEGVwYRBk4I8gePB2YJJQqPC4ALnAsYDgYOEA6HDmkOhg6kDnQP6hCSEeURrRG6EH8SHhHwD8wRtxMIE0cRmxFkEJgRQhGgEVwP5g3cD+kOkA2ZDUQNzQv8C+cK6glnCQ8KxAYPBXsEVQI6AuUBCgC0/c79Gv1Z+2n4P/c+9XL0DfI68M3u6uxy7cTpuuWW5gnl/eAR30zd0d1v3wfixuKm4TvjC+Zg54jozOkD6/XrbO4s8M/w4PO/91b5GvvP/J3+3v+gAEsCAwKmA4sEJQSLBUwGGAdIB3kH/QfTB8oHjgeHB0cHgwlpCUQJ1gpyC6gNfA3jDTUOnw9rDzcQaRCvEOISiRLAErcRNBNaEzAUvhR3FKUUWBUCFisVUBX5FB0V1xSMFLsTpxNXFFcUxBK0EWEQWw9zDQcLewmRBwYGEQSbAnIAfv9q/VD89flo96n2gvMH8Q/vWeyB6DTnbuUz5A/gnt1L3BPZY9jr07vPS9Mr1gbXedyI3JncweP+5knmzuZd65frK/CP9LrzSfjh/gsA6AESBQMFzQZYCJwI1gdECe8KSQqOClQJhwizCJAHPQa1BEwEBwSXA0cDqgPeA5QENwbrBiwJJwonC8wLnw2cD6ERYhS3FBkYuxp1GhQcUx1fHEsflh9jHlsf7B4aHssdtxwiGu0ZhxlVGJgWFxbdFUYVEhTuEb4PcA+4DQ0KAwhQBbEDgAIOAAn9yvoL+Kz0x/Iz8LTr/urT6FbkBuL+3nDb1dWe1SvQEs2jzbbH6cMPwj7ER8sU0OTSdtkv2sHgBOqJ6JHsg/C39KL3I/11AtcEfQu/EEoNVA62EfwOwxBHDz0NDA13DYEMOQnvB60FSAOqABH9WPqs+C35sflh+oX7K/36/qsAVwKTAykFiQb5CI4MlxNhG7AgSSNYJDklbSVkKE0o3SamKPAobSiZKP0nqCffJSAjfh6vGaMYXhjwFzMWFRWoFLQTAhI9D6QLKgmMBxAEKQGO/6T/cf6w/Hj6ZPaJ8kTwl+xP6fjlHeIf4KnagNdc0CrLycpWxXPCA78AtIay57ySwLbGbM06zk3VLuNZ5l/n7PHX9JL7gAU8CEAKWBVvHDUcPRyYHN4YOxoiG00TaxXGFaIS7hCaCyYGxAIL/3b5ofQE807w7PHn8U7y6vN19cP4Yvmj/Hf9fQAQBR0Kwg4eFr4a2SGoK5wrECwALbct1i05MMwuhCyILg4wGSwtKZwlWSJwIeEc/BgTFb8WfhiJFpgT4hDPEIUPOAyUB7IE/gM3BZgC6//0/dD8dfkP9r3xJO5I7ojqoOeA4dXbM9hX1QvPcMmQxOW+FLmRtqmxjaqvtLfBXMYLzDbTTM8n2srpaOnF8nb6VwBwCQgUIRcgGqciniRbH+ofPB70Gbsf3hzxF3AXgRSBDccHFQI5+vD1OPMY7Znqeuqa6orrbOyT7ZfswPFM9FP2W/zYAdYFWAz3ESsXYCBqJ8wpvSxdLWgukTcqOYg1fTRRNVsxEjFdMNsqDigRJ1ghsBkWGIQWUhXOEpIOMgyCCxoN9AsJCXUHmwUXBbAEegMfAzkBWv2u+of3wPYb9ETyc/D46Ffmzt6e1kfOMchuxtO+TLw2t5qu86LRox2yfLWbv2/FVsF+ymLfUeVY5/z2+PcNAEcNJhJ/F9UjSCnXJmAlxiMfJJ8k1iScHEocXhpIFNoQCAd/AJX8nfWb7kjpMej85szo7ugm6LLpWuv07Cfv2vOT9jD8+gFKB8YMmhSCGOMeziXFJxcoKSr7LScyFDjuNqQ13TSMMzYuripmJI4kMCX8IFEcARaDFlkVFRNODVwK7wl4Cf4H7wWHBZ8G0AbAA20CEADB/tv7rvbl9A/zQvFh79fpseWh3hzap9Ug0O7H7LtXuKexpq3CrVqm3p+Dr3K9Mr2LzNDOu8uY3CXsLOw78dwCUAV4EXAh6iC/I0oupygDJAMlqSCWIiEjSSI0G54aFhbZC+0GO/zm9FLwJOrR5VblCuaY5bPnRuX+5WHneegy7P3vbvZY+8YCeAcZDfUSDhzsHC0iQCUPJRgunDKGN3k3JDlJNuMz5TIEMfYp4yerJc0iwiJ5HkQclhXCEz8R6gxZCkUJGgnYCGwHiAWiBCMFSAVEAaL+7fva+Zn2F/XV9KD1+PPE7ezmR95h12/UqdEDzh/IfL2/t0ux060DqLekIa6iuX7E9svW0WjQMdsR6dnsyfNM+PQBXAvJGLYgViGYKIQqBiTJIsAg0h62Il8gsR5IG4gWAhKaCZ0Cf/ta9VXw9euV6vzoYesT7MLrNeus6uzqg+1l8nX3qv08AkoIhAseEnYXQB2lH/0h/CGUJAks2DE3Nh02PzdGM9IztTE6Ljoq6ihLJWUjYSGnHaAcyxfhFP8P8QvSCYEJyAgyCF0GXASlAtYDUAFLALr9YPq69jTzefIZ8QL1w++y69rjydxV2OfTHdTqzCXH97+juva24LOxsOCwm7nSwQfIDM380SnVbuBI7BXukvIt9sj+EAl+FM4brxx8IX0hzx7gHnAffSCnImggRx+mHW4ZsBZ3D+EI6wLt/WH5Cva49Z70jvXs9PHyke/P7qntFu8Z8pT1X/kq/fcBxgSIClwOFBJVE8QTaxRdGCAfKibSKo8s9yx0LEwupiyRLKcpZCjTJ8onaicfJsYk1CAoHWIYlROaEdsQPg85DSoKFwgABnkF3gFm/Rv6Cffx8/LyoPB37e3tHemC51DjcN8S217U/dR60RbSS862yXXFWb+8wDfA7762wN/JDNEm2lTe+dss3tTkCO808BT1rPdA/eQGsgurEL4UURlqGeIVRRVRFr0YQx6qHRIcOBpTFxoWRhBIC74GOgEi/9b7Evmd+bv5kvgy+KP1AvSr87v0OPUh9rb3bfnX/D4AAgOLBI4FugYpB3YHTggcCpsNShDlEgcTxhJBE3ATvhJvEVgRVxDdETISAhDDDx4QcxBPD14LGgiNBy4JAAmnB2sHpwrvDHAJ/wPCA58EPgNgBUkEAALoBHYGxgRFAMT/cP/q/5D8dPRK9hH6PPlz9VjxzfBG8zHw5uj56H3qlefs5Q/o/+qu7ZXsYepa5/fl4emZ7GDrdukV6rnrRO1m77nvOfBA8gPxy/Bm89H2I/nw+ej6D/vR/Pv9Qv5I/8j/gP9s/0IA/wGxA7EFfgbTBlMH1gewCL0JtApUC/0LNAyaDOQM6wyEDagNKQ3FDJsL6Ap7C8ILAgyzCwgLuQqSCoYKKQqaCSUKVgoyCtQJlAknCt0K/wpyC/oLDwxyDBoNPgygDScOkw2VDVwNbA6KDb8MngyeDE0LAwlBCHkJUwogC3oIAAb+BhAG6gMaAcL/HwHQAF7/wvz4+4T8+vpp+Vb3L/Zu9oz0hfKX8nvyCvOk8ljyifCf7pfs3O1/75fsVez27V3uRu0S7W7t6O5q7x3vkO8F8gL04fNq9PX1rPh3+jj7vfuE/ST/XgBVAkwEWgZ4BycIUgi5CfcKRgz6DGcNAw4UDzUPew7RDssO8w78DnAOww1kDSYNmgzHC24LagsTC6AKLwkLCS4J7AnjCfoIhQj8CBYJjwiFCVYKQAveC24LzAoBDO0Mdg0VDW8MoAxLDQsOdw1wDSoNsgxhDMYLvQvtCg4KSAhtB0IHeAanBCcDZAJuATT+DP0r/qX7zPgA9xr2rfVP9VzziPFD7nHrievG6YLne+Yi5ErhueKI5ufqFepW5Hvgd+JF53/oUech56/qi+2H7bLuGPM19Tr38fec+If6ffxAAJwClQbDCJgJ7wpqC3sM4w1RDvQOkg98D/4PsA9TEBcSGRJND0ANEg0zDc4LFQqrCGMIXgn3CJUHzgbsBvsFnQQJAxoDcwQWBqkFTwTvBlYITghwB/MGfgn5CUcJXQm8CR8N9g5yDiMOSA5VDqkN3A2ODokPYBCyEI0N8w0hDocNtwywDNsISAdUCQ4IfQUcAYf/Tv8M+zj5+/x/+zv2lvIR7+ns8u1z6+flW+Je317fsNq/1SnYTNxZ36nfGd0u3BjbHNvK3u7f2OIe5u/nQ+nU6/PwAvUW9zD4J/gX+rX+LgTpCO0JogxTDwgR+RBFEOkRLhMqE/AQMg/dEFkSKBJnEboOzwwaDDsJMgZTBacFgQVqBIYBJgAaAOMAIgE1AHL+if7NADUC1QT/BZUIfAtyDA8LWgtwD5wRNRMZEqAS7hSUGBgZ/hfXFnoW9RZXFpYUERQNFfEV3xLFDrIN+A1gDTgJ1wPH/+AAhQHk/Xv5gvff9KzyHO7q5g/jPObX5ojhUNss1kTWY9Th0LfMu8lpxcfFYcli0HPXzdgU1bbRhdRN2J3fyeKf5nfqG+7N8rf3E/1MAc8EEAVmBZUGQA0gFGMZ3hq/GqYaLxp5GSIZWRkgGcMWVxT1EggTTRM+EfwNzwl2BvUE4wIsArkCYgJ1AZj/Df4e/f/8lP6o/10AAwKqAycFSQhoDKUOuw+hEJMQTBS3FggaexzUHdcf+B62Hj4fWR8sH48dnhuVGqwbmBtTGXIXmBTYEL0MDQp6CJwG+wPs/478XPjT9ALyke4x7O7m0eM44Pnd1tlA0xvR2MyTykDGdL/jucPBDsqt0MjSxcvpyR7L6tON1hrcLOA45Mfofu3T9jf9ygVVBc8DKgZTDJASZBldHbQgYCM1Izwi6x/3IFghAR+XG5cZ+xjrGc4Y9RTZEH8NfQkKBZgClAGkAvECOgGz/gD9MPyF+5v7Zf1f/6EAYgF0A10Isg49EokTehN5FP8WpBlJHX8hKCaTKNAo4idpKHYp+yg7KPYlGSXFIxEiRSBOHk8c5xc/EjcNYAkHB2YDT/4e+vH12/Ld7arqLuWq4BzdvNld1iDT685SyGDEar7Cu8K4xrb1t7O/ZsWjyDTHi8Icx23LZtSr11jdeuPq6pr0BflIAdoGdQsVDaIOMxOSG5EjbilRK9Yriyt7KW8o3yZXJpIl9CKAH0QdAxwMG2MYGxPtDPIHmgPGAPj/3v9d/wb+d/ot97/15fRr9V32q/mg/aUB0QWECYQMARCsEtUTpxUQGa8dRyMjKQctyC+FML8vKy72LbEu+i3vLC8qwCcqJm0kpSGiHLUWtBDPCdoDLgDI/Bb5cPP27dzmj+Jl3c/YbtPhzvrKJsX6wSy9Nrrtt0Kyg6o6qHStsLlUwpPFccM9wcbG/8t61eDZ5+KW63by6Pv5AWgLLRIEFgQXuRn9HgQnwS2jMmw08TPrMUYtrCmmJ7EmmyOXHmYZ3haBFGERWAxFB5YC//3z+cP1+vTS9lX4vfdD9PXxLPLY83j2Ovl0/dAD3AnRDM4QMBUkGgodbB0bH1QkDywvMSwy4TF3M5c06jNdMF0tmi1eLWUq/CRjIvMh5R8CGXIP4AjTBbwDpf7J+PDz6PDh61zjINu+15PWftPGyu7CIL6Eu0i7VraSr3WpIqeDqL2wO7x+xnLHVMTNwr7EsdLM3F7mMevO8Fz4UgChC1gRARfuGiYdJB/zI10q5DDBM5o0EjG7K+coLSYOJUQiSR4lGTEUUhD0DPoJmwcwA+r8IfjB9I3zvvRu9U/15fM98prwdvCt8y/5Qf7OAT8FngomEb8YBxygHCwfPSCIJjUqbS/6M501PjdBNI401TPPM2Ax2y0WKzQoNyfyIy8gahvEFekPZwm9BGcAwPzP9kbxU+yW5nbim9tQ1aPQWMs2ybXC/723uoy2rbQxrXump6N6qaS0aMIUyGDHZsRHwn3M3tRa4lzpIu//9P77YQdkEHcahR7sH2ofpiBVJeMsETPlNno1EDEeLN0m+iRGIrEfiBtyFtQRJg75CoEI/ATi//H6ufVf8gbyYPN69VL2qfUj9CfzEvOS9df5mAA5Ce4O9hK0FgsadB6LIfUjyCbkKdIuWDHuNP422zikOZw1nDEwLTYsRSxxKownviMfINgb6RUxEDgLKgdzAgz8LfX18YvvTO2A5j/eLta10P/PfM25yh3HZsBlusq2+7GOsTWvgajPpkOvar3HzHzSts3RyYbL9dhQ43jtlPK0+C8CDg1TFhUbESHhIRkicCMzJjArZjIKNXU0+S/dKmsngCGhHkEaNxZsExkQqgyZCRAGwQB//NL43fRh8nvwPPFj87j2KPmm94z1k/SW9d77DQakDrETGBbDF3QbPiBvJRootChuKwAtDjLHNXU4yjlnNpsyXSyiKrMqgSo7KY4kQSAPHGIYKRQYD70IxQK4+031K/L2797uMel54rPaz9SO0c/N78m9xW7BYL3vuYu1kLTer/SnMKWpqy+5r8gI0VTOW8rFyWDTid3H6Hbv+PSN/YkH0hAgF20dGx+rILwjEyaaKcQuBTI/M54wySz8J4EhEh7sGLYUFREODu0KowYrAtP7Lfib9fTyIfDj7NzrMu2g8Yf1v/bp9OryQfIQ9if/EglVEW4V9RarG+0hxCcOKqsp9io2LKkyRjZSOTQ6gjfiNO0uLyy1KJAmOSWqIfgejBumGA0Ulg5/CDoCJ/3b+Jf1xPHN7qTqw+Yk4dvcJ9eh07bPPsxSykbF2MD2uYO227WBsyavvat1rQa5osTMzTXQTc35znnT6N1S5dvtI/MO/MsFgQ6IFeIY7h3BIIsiXSMbJdonciwDLrwtxiq6JX4g/BqLFjoSaA41C/sI2QVLAmH+mPq095n0aPHr7pTtDO++8UX0VvVT9W31r/YS+qb+rwP0CMMP1RZ0HTAidiOdI0oliyjMLPEv7jGMMjQzWzPmMmEyLS/EKqMlDSITIfUfzR44G7QWGxKUDEAHIgJA/jb7c/dZ8izuxOrN6IPkH+BJ2R3TYc86zQfMusgLxLa9eLiDsxayO6/trNGxD70zx+zO9c9tzD7QqtbB4GHmtey78tj7PAi/EHEV/hjLHPkeRSHgIpUkaicBLA4tcCyzKdslQSLuHLkXJxHiDMcKvgkGCNIELgA4+y/4bvWA82Lx5u8O8Mrw+vMC9/r4NPr5+dH51vwaBTEOQBX7GHAaTR2fIeklBihFKccqmizeLqAvyjCUMlIzRzKCLSophiUZI8ohjh78G4cZyhbfEoYNkQcYA9/+1fln9E/vaOyE6hzpEOYS4oLc5Nl41VzS080lyOfC3L1wviu+W7sMt7axHbGAu27HC9EW1EvSDtP810nhhec+7ZjyjftzBYkNZhIRFB4Y2B31IA0idyGwIVwk4CaAKGon3CTeIWMdVRfsEKcLCAmsB1UG/AJO/0H9Afx7+g74U/SC8Xzxh/I59Gz1C/hM+3X+UQE8ApQC/gWfC34SURm/HnciOiasKVYrrisPK0Ur1iuLLTcuzy0pLp4tXyvTJ1kjeR5jGqMXbBSHEu0QYg4dC2AGGgKY/Sv5O/Ur8dXtiuuQ6Ojm0+Lo3uPZYNeg1LPRYM4ax/nD175gvI68grfDs1K5J8IZy2/Q9M4VzqzRxdty4gHnIesI8OT4OwTNCq4NaxKUF6AbxB99HxIeTSDWIk8lhiV7JJYi8B5LGgUUAw4eC3AJOQg0BYgBRf4Z/Q79APwF+qz2ZvSV8sPxL/Ms9V/4zfyn/3EBfwPcBcgI7gshDxMULhpAH74i6yM/JkUpZStZK2UpqieyJwUojCfBJmclxiRCI2EfWxvDF7gVehPkD0UL/gZRBAYC0v/H/G35LPY887XvmuuF5zrlOOIr3+DaAdee1EbS6c8CzAzH28BIu4S36bQOuGvAn8dwzjHRzdH41MjZIN864pDlcOkj8HP4eQBrBcMLnRLlF5kb0xs3GrUZzhuVHVYfliCuIC4gFB4eGksV2hBmDaQJjQaKA1IBbwAYAML+ZP0r/J36Nvkj9x/2M/a09x76Y/ww/5ACkwZfCqgNvg9cEpwVIRjXGU8bQR0RINQixyRpJZ4l9yVLJT8krCKnIYYgpx8vHnwbthltF+4UyBFWDrIKuwdgBGEBO/7c+4f5C/ch9Yfy8e9N7HDpaOUN4o7e09ux2bjWW9OPz9vLHsh/xCHByL34vf3Djcpv0WfV/taM2Q7eF+Mt5VXnNenZ7bv0BfzlAH4GSQ0VE7MXLxp9GVgYzxhlGZEazBu1HGgdbR3fGxoZ5BXAErUOywocBzwEDQKhARUB7wDNADwAqf8u/v38qPtE+6D74/yi/jAB7QTHBxkLGg7WEGQThBWlFtgXTBkOHM8eLCFRIoMi/iIRI28i6iAAIE4f2B4RHtIbNhnHFl4VzxPKEesOAgw2CYoGIAPe/1L9Tvvw+fb3o/Vt8r7vouyx6Wrmf+Oe4Gjdytq31iXT4M57yrXHucO1wdHEQ8pN0FbVhdfN2AHd4+G15LPmIeeI6fHtivTe+Kn8yQGnB0cO0xM6FjQWIBeJF98XThgGGAwY1BhYGb4YNxdDFVUTZxGoDj8LiAfsBBIDxwHdAOv/W/+K/7j/xf8x/1D+Iv5k/rz+l/+XAMgC9gUbCloO9BFxFBwWaBdVGDYZMhrBGtMbrxyXHcgd7x3oHY4daR1sHO0axBhwFk8UTxLPEAkPbw0bDEcKIAi2Bb4CEgA5/mT8VPoX+Nz18vPk8ebvw+2a60PpRua+44rgytyV2ZPWeNOQ0KXMK8qvysjNONJ81uXZXdtA3mXhc+MJ5Tnleebu6F/tv/Ae9Mn39vwLA/cIVA3wDmUQ/BAkEkkTBBRsFCEVuxWeFdUUBRT+EiMSQhH8DlQMowkxB1AFeAMZArMAGwDT/zT/zP5O/lH+y/6P/xgAEQCTACEB9QLnBHsH9wn9C2sOYBBIEukTOhVgFjQX+BcwGDgYzReDF04XPxfZFgYW0hSrE4kShBGGEAUPXA3DC0AKvAhYB+MFGgRUArwAIP8a/R/7L/kv9wb1e/JD8NPtY+v56L7mm+Rw4e7eKdyv2IXVOtHZzeTLusxZz4/SH9Zg2BfbBN744MHjS+W15rTnY+rm7PPv2/IH9038vAFoBzYLAA6fDyURIRLPEssSoxLnEtgSsRJlEtMRQBElEWIQ+g70DGgKKAgIBk8EhQIZAev/cP9x/5P/i/+E/7D/KwB0AKoAzACVAR8DhQVfCMkKxgx+DksQuRHaEq8TyhMSFNUTyBNwE3ETvxNsFEcV0RWuFaMUehPvEXwQHA9iDfgLcgo+CUcIJQf7BfUE9gP7ApkBiv+J/ZL7f/nX98T1nvNl8SnvkO1j6yTpS+eT5JriqN813GDZvNRk0UrQK9JG1RfZ49u23Bjf2uHR5B/n3eeh6B/p+OtG7n7wEvO09q77pAFCBwsKYQygDRUPVhATES4RpRC8ELQQlxCOEEgQOBB1EBwQ3w7/DOYKKgltBw8GSQT3AhUCvgH5ARICfgL2ApwDXARwBIoETQQUBQIGUQfICLoJOwuzDJwOvBAhEj0TuhPiE48TVBPmEhATdhMKFFcUShTLEwkTkBLlERsRLBDFDiYNTAu7CQAI3QZdBrEF+QTbA8QCrQFUAJb+5vx1+575Cvh89iv06vHq74juTe1g68HpZefZ5e/iGd9D3HLYTtfr2Jvbst4F4QniK+OZ5TroC+p7693rBe0X7j7w1vFx87j26Pra/4UE7wcPCcEKlQtYDA8NAg0mDe0MHQ0oDcYMIQ0aDuUOaQ++DusMFQt0CWIIggfHBuoFvQW/BSsGfwaABuMGHAdJB28HvgY/BvEFkgasB+gIbgqTCw0NSA5TDzkQbRC0EH8QixANEMAP0A8rEKgQQxFcERQRYxCBD4UOeQ2JDIYLVAooCeAHsgZ0BWgE+gOqAyADBALhAKb/5P1U/IT7UPqe+MH2IvaA9H3ykPAN71DuKewG61noYeXA4kbehtpx2XLbL96b4Pfiz+Po5Vno2+kV623rHews7MPtmu7I7iTw5/PM+LL9WAKHBEwGPweUCEAJ5wnuCV0JdAkmCdYIyAhvCfoKyQzpDboN8AseCgEJrAiUCAAI6wYJBusF3wUxBosGDgebBxwInQd+BicFwwTeBNEF3QaoB5oIvglZC5YM9A3bDm0PhA/cDpAO+w0mDnIOAg/aD4sQSxDoD3IPuA4ZDoUNjAxFC7wJhQigB/0GXAbZBXcFKAUCBMECGALoAGr/+/3g/In7DPoC+Nb2H/XM82bynvCV78XsBesN6RzmbeNr32zbAdoc2w7dzt974gjkdubI6C/qyOpZ6kXqXOpy7KXtP+7I7+XyL/cu/IUA1AKbBHYFwwZLB8YHPwfZBmUHmQeiB8gHOgg5CQsL5guWC0IKhwilB2MHcAfdBgsGwgUBBlQG4wZJB88HYwjkCMIIBQjiBnkGzgb+B0IJLwqKC68Mdw0zDgoPmw80EOkQghDvDw4Psg7/DkMQlREeEiMSVBEaEFEP5w6GDiIOyw2dDBoLUAp1CVYIWwhGCd8IYgc+Bi4FngPqAWEAYv9Q/iD9C/sy+mf4afaP9GLyFfHY7Y3rTehh5JvgBNx62ArXWNh724DfH+NY5d7mXuji6QHr2Orc6m7qU+vq7PzunfFR9U/6WP8BBHYGTgcTB24HjQfLB24HLQdzBwwIHwmTCd0JHQqfCrUK4AleCDcGEwXNBEgFpwXkBd8FLAbLBowHEAh0CMgI/wjcCKcIFAggCH4JwAtQDtEPORHIETgS9BI1E5ITExNXE/8SUhNKE/kTXxXuFjsYaRgBGGcWIhX/E3wT+RIuEmMRCxD/DuQNqQx5CzgKDwmRBxwFFQKl/zH+f/1e/cv8qvsZ+Xf26PPZ8GXuxeuB6eTmmeNt3+ra3det07vPtMyDyeDJIc9g1Q3coODB4xHoS+1A82z2J/iI+CP6dPyiAIEDeAW/CX8ObBJ2FCcTrA9vDa4LIgriB1QFLgPjAQkC2AE2AGb/0P6g/nH90PtD+WP4b/nP+2H+HAC/AWMDHgaYCHgKCwvDCzwMPQ3XDt0P0BBTE78W9RlGHLEcGRzlGVgZchiIGBAYKBi4GN8ZGxvmGzocjhzlGygaxBiAFQsU5RIYEz8TWhJPEekPQA56DGgKeAc5BYACnQDn/m79qfze/CT8Avuh+IX0GPBs6wToBuM634fa0dX9z3DJhMPkvOi1MrA+r1azHr+2yxPYzt4H5IrpM++t9hL7zf6nAZwGHgzHEe0WGhqcHYchLSFqHQsW4AyOBkMCDgBj/gH9mftS+nP4pfXg8GftvOlx53LmNeZr6GTtcvQL/G8CegffCiANsBCTEjQVtRYvGIwa6RwlIRokCif8J8MmcySaIIobQhfzErMRwRD/EMEQ+w8EEEUQ6hCIEE8PyAxWC4sKOwsbDa8PYxKMEw0USxIaEKMN8wtWCjYJYQhaBw8G6wRFBFIDCQKc/i/74/Wh8dHrwOZ74uLboNe10BDLscTivKa4cLIxq0KkDKRusO6+uNH/3kPljurd7zD5sAAGCFMM7BGcGekftyT6J8cp/ipQKrQksxkRDDIBjfp8+Jz3+PR68n3vRuyz6LbiP92419/Wk9if25Lh/+iI8nD9EgfxDTgRDxMJFfYXahyXH9ki8CQUJzYqYSwjLa0ppSSVHiwYSBOIDtoMWw3CDV4Pgw4gDZkKoQnbCHAJEwqSCvsLdA50EQ0WIRrfG0Acexp9GPEV2xOgEl4SEBPdEzATEBGXDSUK/Qb3At7+jfng9E7xHe3h6mvmE+F22ojTj8yCxZ2/T7mWtJOuPKsypqaoNbTnxA7YAOZ77F/w7/cdAPELcRTWGdge7SXdLfkxhDIPL3kqtCQQHIUQlQJk+FXze/Kt8ejuH+ou5JXgfdv/1qLRGtBq0nvZKeIu6wL2lgAhC6gTKBhrGqkcnh+jJdcr0DCcMwU1OzX/M68xSS1/JXkdYBUZD2cM7AnlCSsIQwZgBDwCuf/x/s7+mQF8BZYJiw34EbwWfByyIUckRSQNIpMgmR/mH3cgWiDYH08enxsHGKYTOw78CBMEWf7A+n327vOQ8KTt9ulJ4vfbw9NTyyjHcMK/v9C7cregt8GxB6rWppKrCbrY0FXm4fLE+Ef83QRRELwaYiF+JSQqci9XMtgyJTHuK8klNyCqFD8GDfjP7eLo8+cG5jDiiN1L173TmtAQ0OXNytDn1cTeNOrI9DoBNQyhFfEcvCDbIuMkcyirLtYzLDeKNtkzwS9EKxwloB7CFYENuAa/BFoEhgOYA/8AuQAE/Rv8APqB+4H/YgWEDNcSVhj5HD4gdyPOJMEkAiXXImEiJiLqIzskuSO0IF4bhhUmDwoKbwSyAPz90fsu+f71LvMO73LrU+T+3ULXbtJvz3TPx8+cy93Fk8D2vFa0sK+2q1uvqrrGzJXhevHI+MT88AOCCgUTZxcVHJAe9CS6KzwvLzFBK2gkOhyrEEcE9fbH7bLqtOk/6hXp7uT23kLaT9SG0LPMc81M0n7bdufM8k/+dgdKD+AUABiwGaYb3x9dJtEsETMsNcQ0rDCPKwElaBt0EgcKpwVDBNwEoAdKBo8C+v/I+Qf6Ffiu+FL9CwGACgQS5hjiHyQiMiWQJRUlTiM/IpEkhyVsJ4knziUAIqccbBY7EBcLFAeiA+j/p/zU+oH44/WF8TDsY+UI4onZLNar0ijO988IzF3K6cS0wNm7DbVksVisU66vvAvQzeZJ+HMB3ATIB10M1xHzGQodgyL8J2ctvTE6MQkt0yQGGlEMI/6F8g/q5OZu597nKeer44Xeeti1073Oos24zsDUKN/46+f4zwOKDfoSQxc6GlcbXx5cIjMoJS9dNFw1nzIELcslSh/lFQ8OjwSXAsQBFgaDB1cGagb//V7+dPjH+Mr6DABlCPsQ5BiuHL0g9iLtI5UkUyQsIykjCyPVJgsojig7J58i3hxcFu8PcgpxCMEFXwQDAsf94Pqz9zX05O/N6ljlpeFp3CvbPdbU1O3Sms90ymfDJcKGueu0arFQrRmzrcXW2q7vQPzNALkCIQYIDPIP+hgCH7AmyS1KMm0zAi6qJjAdERIZBnn63fL27gbubO286zvpiOIa3M/U1s+xzHrOA9TZ3H3oD/M9/okG6gyQEVIUaRh0HDkheChhL9g0XDjONXQwKSqSIb0a0BInDIgGDAZaBq0FCgVbAhL+T/nT+PT1Jfpy/8AFCg5SFeMbOh/PIKIiMCJtJIYmcyf+KAkqHSyyKoMpniWZHzoarBRuD1oLjAm6B5EFBwJT/P733fOm7oXpbOX94Tnfj9yP243WI9Sgz0jLf8cowAi92riFtHyuZLBVvdXSeujm95T8cfsC/HQA4gltE0cd9yPzK/QwGDI3MC8pNSChFsMMMAJE+fPzTfJ58o3xFOxj44DaHNMPzn3Py89l0/va1OIp7Br1Kv3sAnoHLgsuDkUTxRnAITMqlTDWNEc0DjBKKo8jARziFh8RfAw2CWELWAv9CcII4wDt/bH2tPZR+Gf6nwOsCRkQBRdTGvMbVB2KHi0ffSHRJCEn/ymOKzwtBitCJxYj1BtEFs0R4g6BDbUNSwumB4QCVPpW9jXv4uoT5crhXuBP3Lfbedfl0znQC8ktxbq/i7mktsmya6/OqR+uKsCX1QznS/Hz8a7u9PO5/FoIuRSYHagiPCxoMkgwuC4qKHsfkBgNEOwGeAHY/Y/8vPu792nvRuQP2+fU09Cz0bPSCdcR3FriX+rK8RH4QvxL/+QDCQdoDb8Vuhz2Ja4rHi08Lfgp6CQeI98eExskFooSiQ7GEAQQBA0eCsACeADn/Ln9nP4zAtkFQwsND9ESoBVVF4EYOBqeGpobhyChIR8lbiWIJYcj1iBqHYQZERdBFLgTGhJAET0Pegy8CK8CFf0M9p7xnuuP6TroSeRN4MXbr9bG0t3Lg8XHvc61/rOIsHmuqawqqtuxQcR80YTgOOZG5kPrA/P4/zkKbBXYGmskJi08LwkwwCpvJRoicBuNFdYP0gm5B0QEXQHy+DbwtOg14Tbcndn01dvW39pT3trjQuo9763ytPf9+gb+mwSWCyMTdxzmIp4nqCktK6wp7SgcKNkjuSDSHjMemiDhH9Yd5RnsET8PSgluB4MG1wTvCHsKIw+nEoUTahUcFIUT0BLUFD8XNhpxHiMfRyCFH64eJx1WGn0YmxVgE+URYREbEJ8OZwxIBvsC/vr89XPvFuyY59jfkNoM1H3Rms65y3TGFL7ns9etnKzzqXSsPbt/xpPRydou3WHgsuh79ar8wwVNCzAQvxrcJaMrQi5OLXwnXCAyHMAXyBKXEgoPTAx+B1QBefp588vsreVu4THfoN7R4XTlnemS7dbvB/Lx80T33fsUAioITg4DFcEaWB8yIywlbSa4Jr0mwCRWI28jESTrJwAqwCbaHwEaaxROE50RRhGlEPoPVhKWENQQERHQEAQSahF3ETQSkhWmGREb5hrgGdsYQRgEGFoWVhTRE2ESIBBFDxcN5gv/CaAFpAIq/Y35L/e77lLrA+TD3X3YiNSv0GnJVcUrwbu7yLiTtoywFLCcuQjFbcfa0pTVCdbv5KjtHPVf+igBswRHDFsWXBqwG/Mgnx3eG74cdxhdGRgVVhOQD4sKaAgKBOD/s/yd9kXzku9C7QjuCe/28LHy/fKc9Y33N/qk/uj/4QTHCIsNsBJcFXgXExs0GucdPiBIIe0kdSQYJBUjQSMSIJ4eExp+GcsYixilGMkVMRUnFI8Urg+EDzkQLA9sEpMRjhEjE9sUYxTuEUMRixAcEGEPjg1EDDIN8gxpCsAHswbtApEAa/+A+kj5L/e98gDtUeW54izdptqA1wjU/NB0x5zD28F5uzq+rcJYwDDIdcxizPfPxdld3TnhMO0j653uxPVk/MQCjQjODRsMhgsqEKkNOg4FE/cNAg+zDWMMNQx1CEEIaAOS/6n/XPsX/OP7E/tX+xH8+vvD+xT/6wCwAk4FgQXwBhIMoA5EEiIUFxYJGYwbzByVG4sbaxyHHTgc6xuxGpwaxRpZG4IZ1xh6GTkXmBfnFhYW4haxF3kXohddF5sWxRUMFKMRtBD2DvQMPAybCXkI5wWBBOgBtP9W/cX7a/o7+Cn3PfQK86rvEO/G6Gnnn+Vi3x3dRdmH2D/WZNSn0wzOkcxHyvrHHcw40I7URdRw2HbYSdyf5LbmoOhq6NbuSe/j9eb8nfoW/0gC5QD/A/MF9AUNCJAHTgfCBsMJJAi+CGQJagZJBxwH3AShBgwHdwcQCHwH+weOCM0Mjg4ZDp8NFg4yD9gR4BTcFDIVrBfxFQUVFxeDFuwXfhlvGM0VQBfIGCoZEBqyGMYWWhc+F7EWaBfUF24YwBenFokV5hT6FKsTchG2D3sOQA18DPEJFgjEBuEDcQGz/oD9S/uD+s73vvWe9Gnzn/Hq7wvuTuws66XnXuhJ5nnl8uSR4sbiGN/r3l3d09uZ2kjdpN6t3jfi8N+44HDiQOXd5tbnyurv6SHu5vB38fD0YPee95L4nvrx+t372v3L//r/jwALAusCHAT3BGYEXwUJBb0FRQe+B5YJmAqeC+YMdA3WDxAPCBKiE7QSUhU3FXUUmBZGF2sV0xVyFakV5RMMFZITmxKhFfIT6RR2FOoUGBVJFccUMRSBFGQU6BRxEwoTDxM1EpIQhhBuDmcNzAyPCgIIjAf7BvoEwgJpAMz//P45/rr7hvn0+O72nPWF86ryDfAn8OfuUuqE62/rJ+rv6VLl4eT05b/h++I/5X3g8eEF5UXgdOIg5f7ibeHQ5lnl9OHg6Nrmgudd68Ts+OsW8Jzzy/Hh8VP2GPbr9Qf7nvlt+8T+hACt/9UC4QOzAoQEswQcAocF5gYCBqsJLwnACtAKGQuGDY8Mbw0sDVgNmBD+Dg0Q9RGMD5YPPxJZDwIPcxBTDiIOgBBIENAPbRJpEN0NZQ/aDwUOmg56EV4PnA0lD5wOcw2PDOwL/ArSB+gHzgYjBDYFiwQQAugDMAEXAHoAM/3V/kv7Yvnm+Tf5zPgn92b29vix8JTytvN67Zzw9Oyk8b3q/uYG7Mfut+sE6Gnq/enp597mIuux6Zro8OpR7MXpketj7fLs+uy47F3uTu/576TvhvLX9sPycfPi+Afz5/Y/+Oj1X/yW/Er8c/1y/v/9gP8VAt0AbwIqBasFcgVhBfAICggOBw0JkQlJC+wLPQsJDd4LpwoLDHsM3wrvCOQM5gqJCTENRw0hC3cKTAvUCAEJuwxTBzkJEQoFBhkLDQtrC9EIsAJjBiIEcAnmBS38Owo6Akf6kAaMCKr+pvmaCOD47fZDArD+jvqD9xIECwOv9c340/ya9jz0qfKy+6IAvfDD9BH9sPxu9w7vYv3X9Qbo/fSD/0b0we1T+//64u5v9ef9sPJh60r7x/NL8oL6VvgE+wv2dfjO9av4Yvyb+bX29fag/q71gPuu//z3Bfsm/kMAw/NN+9IFA/az+oYC/QXBAPL8ywWpA0IA5gCeBgYAAgKjCnoGRwNGCaEFugEaCngGNAEZBWgL5QKx/ygPQw42BOkGEwi5B0gDxANPCR8CIwdfCNUHTA71+VYHugxpAm4GKfmyD6MIPPqmBHUKPgfp+hEHXgegAJP80QKXBbr8if9nAO4I9/yV/gIKrvcOAbMEK/aX+h8J1fey9tUK6v5K/G73MfrM/pT+/P6H+KD+jv8p/mn6F/4zB2f3Ju6rDPL+Au3JBiYAiPas/Lb3Uwc8AOX01ANf9o/+wP+1/a4BFvtOA/n/b/zO/OT+7AoJ8PH7AxB391r97f3dBsUKlPAl+jgRKgPm7c/+TRHc/o/ypw/3A4LvyRaA8d0BCxId7jMIPhPv9gj+FApY//4H9fI6/cwSav3M+VwJegNDBvr3cAaPEVrqywRkDOH8m/waBwoNWvVr/EISy/N3/vIN1/QBBfX94gNeBR/76wSzBA/8//6ICBr30wGZA879CwF3/iUINP+eA8L4k/6tC3/tjxB6AmrtMBQ8/Er7gwWMAo37fv0SBVICaPNKCE4IXfGlAZkD5wV3+X4BHged82YOdv3G9xIJvAF2BcvvXRCmCC/u9QhHA5L8ewE2Bw75AACrBnAAgvq+CO4FV/YfBpL14Aa4AXn79wPL/Cv7Agop/aD37hIg+sD4RQZWCPj5jfg3FN/8lu24ECUC+ffn/uICDgbl9RL5VBSq+J72hgnl/9cEk/tY/VMLAAMl7wkMNgiZ86QBswRzClb1C/xcFebzmfs8BuL4mwrG+/X7NwiMA93tgQ8KBqTwbg3zBM71N/nEEHD9KfvbA4oE/PkxAY4BKwEFAOL+2ADF/jAGwgK++OoFLANS9gQM6Pb+A6AFvfygCP714QHXBrf+FgbC9rkK4AaM8ogE7QdPB2rzBwEaCtr7ov7K/RAFxwMB/xf88wByCC/7svnmD2H67QBBAFL7hRDR9Db/GQh7+CMKpfz59ycQNwMS8dAB8g1b+/AAjQJ9AmEGXPlF/Z4Me/5G/D8GeAG6/+IAAwSp/mkF//qU/WYIsPrT/jgPNPg+//4KKviiAwAGP/kAAK0M+vwm+pcM9PqV/PcGOwJa/jUB//2vCH79gfemEID1Xf0CEZT3l/4eBgn8jgny9f4CnQyy75AFHg/Y9u77lg2p/gIEnf/o/TkHXAFaByL2UwdFC+3zdQZOBf36bgbv/vX30AtbAXz50gcTBY3yAQQBDXz3GPxUCQMFcvvr+tgNPQAd+DYL1vd9Be4FQfduCFkJYfl3AnAAWv6OCRv9GwOTAUUBeQRo/N4HYAFj/MAJHvtBBKT7BgPiC7HzIf/YCd8B/gF2+ZIAgAxw9U79og1K/aH88QohA9/+l/sBBwz/LgKh/oD/+g6586UBNgvpAjb5lvrtCqcBR/XNBuQLsP0LAKsEDP6o+u8KTf/o/pIC/PthB0kB2v9r//ACgAMK9zQLZQC8+bIGXgMiAQz8WgWS/QECzANh+BALQf7M/Qb70AV8Cu7zJwEqBYr8UQNvAbz8Dgf3+y0B1AEh/S8CngH1/Db/8gKo/y4CHf+v/QQBbfzDBmP+y/XDC0f+4PV6B5D5/QQtAJD5hQLK/XQHW/f4AHoDj/y7A57/Iv0vBaf9cP0QAwn30gMiA0D6UgLAAYX8VQGH++n+9AWR9/IABwkh+Xj6/QMOBCL40f1DADv8iQQV/EX9HAeg+U8BYf0x/hkFzvohAHz+7QfO+tH9lP67/MkERPlVCXP3dfmbEIX1of5xBEP95vX7AGMGV/JwBNgF4PI2A/oEFPnWAcL6KwHYAjD5y/6BB5j5Rv1UCL76x/glAUz/BflFAm7+Xfuf+58Cyfzv/nAHx/RhAaT9t/9c/QcB1f8G+8QHzffd/hwF/foh+wsBWPygBFn9D/pgB5b9EvdMAtkCn/kr/cr7dQDwAfz7TvktB4/73/aPBtf4m/1+A334M/0mB8X2zf6HB2b4owALAAr6M/6E/Uv+vABr/MD5QQfq9Nf9/Qaj9cz6MP3VA+L3tAVq+LP+Swn08j8D7wNh9DL+lAW6+tT/7/+E/QABNgCR9l0J6P1B+I7/pAKQ/kf85gk3860BBgS89+j7kwml9/76oANw9ET+/wcj/G/4GgbG/Eb4wQFABW35HwCeARb3MAjeAoLzeAQNA0/33f4vCXP9XfrZA2ACCftnA8UDQPkD/WsHN/kG/r4FRvUEA/wEhfRqA6gIBvX5BTUB8fm3COT/tvpU/YgHWAAD8/4E9QX8+KL7c/90AEv+6vzm/tYDr/4j/BkBmwDsAqf45/rgBYwDI/QwCAYF6fUtAn4Bwf2C/OsFcfpdBSQCnvzJAs/8Rf2lA9z8kPoABub+gP3//NYDewA0/pv84P+XATL+vADv+xICJARO/H/2Lg/C+87yVhDw+Hz/PwFDAgsGPfWFAXYNCfbo+mAPsflE9W4SCPpJ82IPnf1E/EH/vAMT+dkAJf9fBgL13ACvDiHsugF5D/H1IfMsGeXzq/Q0FZr7nfrP/18EfAj99HT7Qhb58Qz4ohRu+Nr7SQao+X4EoAj49lT+gAU6Bnz8SPmiCc3/5/PUB0sFtfHXArsE9P0xA1P68AWzAkH3jwVtBeH+HgCtCLf9wfi4CJgIRfde/mwHAQCNAFf3CgsaB9TuaQX2CCT1gwa1C1319PnVCmn9EPf8CDUG8v65/wEBDffzCzL/p/h2BH8ArwFm+7QJ3/uDB+79EvM3EHH9B/h3CFYBj/18/Cb/3gbD/j363QEGApr44ADhA+f9FwN4/iv9uv8NBG/8JwH2BSD+PgCC/y8AIAPR/t78Vgms99r79Q5K/Rr1tA5A/onydA0u+n3+4w3h+HX4iwmP+T//CwYC/JX/IQi/90oDNAMs9t4NHfZH/cYBEv1v/7AArAIjAmn2EgZBB4ruAgUrDNb0LfvjDOf2mvyDEZ72V/AaF3vvhASxBU7yABSe+Fj3WAyd/OD5SQf5/dT+Ofv5CtL4L/z5C/n04vwGC1T3Pvt/B0z93wIk++77nwYn+bEAfwJK/u4CfPmWABv/xgKI+ZAHlfZFA/QAlfgdCo/rvgYxBPDsWQl/ANf5AAzq9ugDGP7Z/x8HfPjfAbb/XAOa/sr1ChHY9pb5ixIS67UBVAa09lMN8PWO/GX/owW7/2D0nQXFCAHzlgMJASD4ZxSn5+kDKgea9QAD9AKO/wwDjfOBAEAE+fdsDUDuEgtZB3TwwP8bCzT8yf3X+dwDsf4+9xEO+Py6/3b4CgSdAhD06v56B1/yWwnIAdHueBBG++D7kwLjAGL9Kfh2BK8CVv8d/dz9nQT+AS77PwWXAD70WAOkA6P2fAbsAgr8EflAAiML1/Rg/gEH5vKe/yABpQSEAw79kAET9lUKUPXwAbL+5fuJC2TucQROCdn+FvVI/ekG1PkS934LtgFt9NsEiwNE9oQHMAJ18ocMWfj3/MoGePy8AlwBy/1gAq//Ef2DBTv8mflUBBkDB/yj/xMDFPok/54E5ve2AbYE3Pz3+qACrghB8wwBEgrP/P/1pQs9Ah7qthQNAHzyZQw4ATL7YPppB4cG3vSC/pkP3u8m+fAMYACm+ygBhgLa/Mn/r//iAND6Jgs4/Zj3wREJAKX2jf4aBDUFKvLZCWkLLvMjATIIwfp7+yL+jAMq/av8zghc8QoGgQul9rT1Xgg8BOPzZwECCfb5nP8ZEE3zCv5NBJsA9PgZ/zMFxv1n/7T8FAlh9YT+6Atc+PT1qAREA9gBNfqS/58C5PMWCJD/dfdgBGwEgfm1BJABjAE0BWz8VgHcATYAk/wKCLoBU/0GAOH6VwRV/xb+0/8Z/+sCLvPpAwMJOv2s/sX8jwcw92T2gRCF+dX37wl0/1b3+ghoCW/utgE6Con3wPdGCIEA7fzN/OQB+QLT988BOwA7+dn5uAeS92f/QQkc9pgAwgSb+535YASw/7D68f/3BCMBygBm/koC3P9o/MT7TQEVBO/4WQAoAZ0BWwHL+coAnv+i/B4AxvkUBAX+yP52BJD6KvyMATv9lfrBA1QCRP3a/uAGsvh4/vgFcvgb/WEAQAOT+3r6twft/Nv8egBA+tX/d/zYA8b+DfxjAiv+tgCr/tb8pf/i/Jr85P8q+zQAZgRcAJ35WPxP/8r8GQHY/hT7Y/2R/6/+u/jXASADPftw+qD69P7n/I4C/wAK/qr+BAGl/TIABQCG+07/9/2yALv8IwMKBX38jP6r+gf7APvf/+UCdP5X//X8Nf2aAKL+Vf3a/K7+Rv26/lcC2ACA/lT+b/9T/kn+ogAg/h8Ab/82/sT+jv0TATf+t/xl/3v+I//h/an/gP5n/p7/E/+F/Yb9kQE5/j3+OgFC/Xr96f9f/Tz+9v7X/m/+3vv8/db8Sf6O/af+4f6Q/Xf96f6L/Zz8GQC7+4X8UADW/6z+Vv9iAIr8vf2y/qn9gP00/yYARgCE/6n+ov+B/77+M/2s/oABFf9pAK0BoADh//v+pv+q/Vn/Yf+h/x0BZgCyAWYBrf5p/vIBTv+x/38BtABEAZwCrQGtAPgBJAH4/in/jgBV/un/qAEPAT//9v+YAdD8dv5E/7X+nv5W/uD/3QDt/qf+qP5b+0v+gP5g/f/8hf+DAEj+G/99/wv+O/1X/pL8qf1X/wIA/v7p/NP/DwDx+17+RP45/mwA2f+l/jH+NgH1/XH/UQBsACECRABFAUQBlAI0A/QAMQNjAokBHgXkAOsCfgX/A6MDVgPPBBsEzwWABXQDHQZoBucDwgVsA2cEpgWBBAYFXgVXBZMDxANKAs8AXgQkAjwB6wL6AMwC1gAXANMAIQAL/+f9aP11/cH9o/20/E38sfwm+9v5QPgj92v4mfhb9nv3WPfq9Qz1XfSh8g30B/M484HyOPPo9Nvza/fg8yT2h/dI9nb4bfjR+gr8P/4K/m8AawC0ATwDEgS4BmoHQgrxCPsMMA3ODUoQpw1QEeMRnBG/EpMT6RTBFbwV/hTKFcMWgxaNFtQWhBeEF+UWwhW/FTYWOBWjE70RZhFwEC8PJw2BCY4JYAbcAz0BIv3B+Xb2B/RF7mfuyewz54jll9wB1jfUc8yRyCbJ7Lvyt4m9P7xQyNDOTcpvxvTIhcpfzf3ZsNlF4d7qje+A9iL8BAMbAywEvwb7B8IRsxxjHmgnkiGcIEcjcR48JKoijiFYIYog+B8eI3glmCNrIMgaGRkBGT4fCyITISof7hzTHJIf0iLdIu4iBCDyHbscByD4I78maCUeIDYdrBwMHiYghByAGCgVMBCcELMNEQ0YB9v9yff07SPrDedk4hnbitDNybnA4rPerW2f1JZPnKmq/LNqttS8NrD7tom/P8OAznbTb+IF7u77cAjUCzoR2BpUF7wb5SHDJOAxYTXRPrE9tzfbM38rgibOIAYcOhYJF88VdhIpDX8IUQGB+r766/Xi9Tr5Fv5B/zoC4gc3BpUK8w2FC7EJ9w+yFzMhMywILKAnQCd5KUopbCvDLo0vCjJ9MzsyjS6pLgouEShYH6AZ+xddFysY1BIqDAYHQgAQ9pTtounY40rgQtiE0TDNr8lOxdm7t7VTqo2kJKXsmI+Y46bsthPCnsiDxjq9Icl+1wje8e8w+hcCpQ8bGlkeph1SJdAnciWeLEMugjDNOD84qjTyLoElxx3HFO0Q/AxJCbgG6gL//Ir3DfTs6x3pC+fE5H7lLune7CLvzPSS9kD5jPrK/ST7mwFADFYWyh/oIrshlB4rJOAjFycpKCIuITOUODQ4OjMyM8Evzi1yIrYdHyCcJX4q4iV/HLQVLxAkCQMDPP60/1cCnf8Q+5f1v/Ou8bXrAuR14OLfOuNB4WrdYtvd0wTRdMSavgi74LAWsIi5PdGM33jn8N6+ycLHV9fv5Lz1zgB1AbQIvBOSF2gRMBWbFP0SxRtcIEQhlCdgK1om/R6mFP0NNQgICE0Fg/9W/lf8aPrS97Dyzeqs5vXlGueY6snwPvb2+NT6BfmL9+X43f6hBB0LiBOeFp8Ycx65HBsetyEyIBwjpiZHL4QyPDUBMkMqDCcPJmomRyQCJKkiTSFoIUwgoxu/FjoRrgptB64GcgnTCxgLhwf8AE39kvmG+B75hPdr9V/0vvCw7f3oX+V33mnWSdbTzkLM2MbLvOyxSbCzyBDam+Db3BvJer68z/bj7vAC9+z5MPqsAkUQZA00DhwRkhI4E5UZsR67IfQnlChcHXASAg60B/YJVwplBrsCnAG//1z5/fN17vLqnemg69jrkvC3+FT+jPzl9i3yGfH8+FICvwnvCkEOJA94EN8SQRUVFgMZIR4lHeMicCcJLuotfykxJIAecSOAKZgruyjlJu4juCK+IP0bbxdQFQUUxxD0D+YOCxCMDk0KYQOX/c79Y/9J//L+kvpW9jT28/GS7czlF+Ml3/HZ2dqg0W3JIsDTs22riqqMwvDVkNo72QLE9La2xB7YZOaQ7Wf0dvQE/lML3wjSCM4LgA/2EZsZXx/uIsgqKy1AIskW+BEEDe4QJxOiDTAIdwSuAJf66fW18H3tk+zk7dvs8O9Q9vb4pPZ98c7tcu1d9hj+bQQrBWAFHAXpBxML4g6PD5cQ3xUkFJIapx6tKZYt8CqMIxcbRSEMK4Qztiw+J2YiYiAUJHUhyRwXGqQY0xOhDkoMmQ6yEdEQCwm2AFn/GAVyBWkEKgHV+2v9kP35+or0ZvN87VXoT+ei4H/aOdHVzA3BmbO/ssWv+7kt0B7Yv9DKv+6yLLvXyUHgHeuF7Rv1h/pBAH3+7gBaAQIKoRboHLcfqCM9J/Yk5B6DFt4RChGAFpEWIxKgDUAGbv91+wf4dvJa8zDzhvHD8NjyXPQD9bz1i/Ol717x5vfq+3EDTAczCPYEygawBd0HSRAXGDUdvxz6H24biSAVJHElFCSOI6UnXyU3Ka4oTyYsJRAjsR9vHLQdAB/qHAMZFxPfDs0NXQ0nDNUILgnOB6IG4wTTAAP/Yv3Q+/X4HPgT9/Pz8O776mrmM+Dw2+rSX82QxTe/h7oTrG+vFcDIzMzUC83Juq2yrrwf0B7c+Okl8A/yV/gh/Sz6C/h4BcIMKRXkHQ4eESCBJKwjCh2aFzkU8hR+FkAY4hNJDYMJ9AFu+4T2mvSu9QX5Dvl29hX0cfQr9PPz1/Ow8iT03Pdj/b3+wwJfAlQE1gQwBf0GWA7FGWweLiLJGl8XQhjlIkwoFSvoLrou2izeJs4lbyMBKAkqfSVzH6Qdhh0BHIoYihHHDMELHQ3lC6sI7QjEB2gEWwAa+wP83fzz/ln7F/kz+K/ztPDL5zvjU+Dc24jYPNHfyR/BwrbWsNKzZr+mylnO1sSxt5Cxv71xzRnd++Q56O3sbvE+9l7zL/iZ/gYKYxSMGXwafhqgHfgcAhrkFYMVjBbNGc4YIBOnDUUHYgLm/aX7hvoz/Pb9jP2K+jf4OPdq9573sfgi+qz8WQE+AsMCdwGrAmUE4gehCs8MTxBoFm0boxsMHjobax2bH2oiGiSrJHMoNyn2J7sjrSJjIOUh3SKfH0ke5hytGmAXTROuD/QNLg1sDeUJJgibB3wG4ASdAe/9LPuR+4H7z/ij9ZLzBe6Q6m7mPOKM3sTY3NXBzX7IUMHAth+xP7MvvrjHGc2yx/a7p7XivtPJZ9Z/4SfnZesT8T30HPDk81j6ggQ8DeETHxbwFY8aGRt7F7sUxhRiFGUXUBiHFMEQNg0HCXMDLAH5/1ABbAPlA4UBxv+I/9//AP/Q/qH+8P6qAQADzATvBEMG4AVJBrUFxgjTDAITnRkdGtobTBikGnwbBR5/IV4jgyapJ/EmKyMwIy8ivSKJIQAf8RzdG54behmwFSoRMQ80DMQKWwcXBRYFlQIWAf78g/mm9+n1o/ZM9CfynfDm6yPp6uRA4Kfb2dUH06TNIMgFxrO/9rw0xKnHbMXFw4u/Wr0Vwv7MU9Ip2Krd1+HX5FznKeoq63D0MP0wBK0H0AvGDu0RlxUPFpkWZxZPGPUXuhegFswUixPqEKsNqQk6CFcI0wgZCXQITwerBvUF9QS3A/gD9gPmBC0FKQW8BYQF1AaEBgAHmwc7Cl8NuBFtFEgWOBjmGO0bwxxjH2MgMSIpIwkixiB0ILIhqCGZICAeNhxRGtgYbRaiE9wRpRBGDv0K2QfwBAED5QBt/qD7FfnV9iX1H/Ro85fxrPBr71XtqOsC6groHeV74nDe69oy2HDTANBUzvrMDc7Fz4DQBtF90CHQRdIw1J3YUNwW4cflq+nx7ITuE/ES9Fr4SPzOAB8FcAgeDFMPrRB6EsESYRJKEoYRDhECEMIPpw/PDtoNNQ2nC88K7wr2Co8LRwyzDMoMmgzzCzULvwnyCEcIoQfmBwYIWAgiCVwKiQvWDDsPixB4EewSkxS8FXgXDhnqGZga/RuEHAwdOB4+H+UfYSDqII4g6h8hHx0elhxxGwYazhfEFaITgBHNDncMoQnzBjwEngHo/mj8Bvrt9lL0+/DZ7Ono0eRQ4JjbMddr0mnNIsn/xP7BzcDNwGi/VsCFwZXA0sJCxizIrcyf0R/VrtnK3YrhD+Z/6gDwCfaE+8YAPAUwCsANHBHXFEcWcBjNGZoZChkEGQ0YrBaTFk0VZBOEE40SWRHxEK8QfxDUD80P7A40DrcN3wwADKYLFgxADCcNKw7nDp0QKRKaEx8V7Bb5GJAaZRzrHdQeDiApIdkh6SHEIjkj7iI3I0Yj8yJWIsIheCBTHl4cQRqpF40VWxN+EPUNHgueCIwF0gLq/xP9ffpA9/zz0fBN7Q/pAuYy4m3dmtmd1dXQOszQx1fCqb4fvJK4VbgQuRy5Irr8vO2+M8D/w6PIasyB0g7YTtzK4Lzlf+v68Oj2cv2QA7wIGA5gEq8V2xmCHGMecR/fH9Uf6h4IHuUcRBszGhwZoxcHFi0V2RTAEz0T3hKkEUYR6A/7DhoOPQ0EDf8MiA0PDvEP/BBtEtkUkxaMGFUbeh2fHncg3yGJIbEiOSOyIi8jDCNtIhQiqSGRIB8gdB8iHqMcyhqgF6IU7RGSDrMLNgnYBqwDdAEH/iv7XPj79IPycO/I7M3pv+Y9417fZNvy1tXSi87fyAfEzsCEvRa6G7rouXu5IrvSvLu9j78sw0zHactx0WDWDtp+3pDjkeik7Xj03fqIACUGDQseD0YTaxd2GqQcFB4MH4ceDh5JHZ4cghszG14afxhaF1YWgRWxFOMTSxOAErMRHhAyD8EOUA7HDiIPvQ9kEF8SdRP+FIEXnhnXGx8e1h8tIbQiWSM6JBUlESUuJZYlySTUI1QjgCJxIesgOiAzHsgc0Bq7FwwVOhIxD8cMLgrOB9wEUQLT/7X9Lvue+I320PPx8M3t7erT5n3jlN8+2/XWYNJbzaDHwMOov8i7/roqujC657pju2C8j71kwP3DdcdszYrSb9ZZ21fg0eQ/6r7wbvcs/RoDGgmdDcERLha7GTQcoR4QIO4fvB8vH3Ue9hxeHMsbIxoJGSQYwhZ9FeoUPxQtE+QSFRKQEX4RNxG7EXQSHRM3FCsWphcUGY0b0h2cH48ixyQOJp0nZylHKWopKCqvKfUoMSiLJw8mUCTdIich+R7YHMIalhd/FFsRWw7vCh0IeAW5AvX/Qv1++rv3MvXn8czu5evC6FTle+Lr3pvaedeP07fOkMpTxhbBVb0JvAi5Wrhxupe68LpAvmjAKsHixZDL/s4H1d/a6d6M43DpIO/l9Ef7twGTBxUNwRGxFT8amB3cHyUiWyJHIhkiLCGZH0ceUR3KGwUbHRqeGGoXtxbXFTcUJxTuEw4TvhJpEjISOhLjEvYTIBWOFmAYKBqPHKsfTyIMJDUnWShrKRwqHSpIKuEpLykgKC4nMiWoI84hqB8xHfgaDRm5FasStA9aDBcJggbPApr/s/yY+TP2y/IC8AXtPupr5+TkheE+3kfblNdQ03LPJcvvxc7Ck7+ju6K65LrXurC68bw7vm6+1sFdxjLJHM7W01vXXNvN4MPlg+oa8Xv3Af1FA3YIFg3UET4WExolHS8fWSDTIIogkR98H6sezh2jHbIcBxsUGisZzReHFkYWPxVGFJoTqRI4Ep0ROBJ9EpITsxT9FVAXJBmWG6wdaSAQI7ck3Cb+J4UoOyn8KNQoICg0J78lTyS9IpwgDR/zHK8a0xcUFQMSsQ5QC2cIDAXYAVj/hfx0+cv2JvSE8Rnvl+zs6TjnmuRF4YPeYNvY1yLU4s8azPzHAcP0v4e+KL2nvHi9F71IvXW/DsEcw03HqstMz6jTHtis2wHgzeXv6/nwDPey/PQB5waAC4gQlhRLGIcbCx2wHd0eZx/NHr4eFB44HZkbKBuxGigZ7hhVGBIX/xWTFfUURxReFDMUxBMxFPYUnBUXF7cYvBnyG/cd3x8VIlwkCiatJ0Qp1SntKTcq5ymVKMMnbCZ8JIwiryCZHo8bUxmEFlgTRBAaDXgJBwYgAxMAh/yH+Zn2nPP18MXt1uo26EblAuKL327c4dhe1vPSys7eylPH88Lbv6u+YbwPvDS9SL1Wvay/J8G+wgTHf8tXz//TuNhg3D3glOXP6nnwd/YT/LkB8wbrC4gQ8BQtGdwbYR7UH1ggGyFeIdMgOiBfH7oeCB5tHXUcFBvdGokZBRjqFx8XbxYqFrIV7xTjFFEV3BX3FpcYAhrhGhEdDh/tIBwjoiU9J0oowSitKDYojycMJ7AliiS0IqsggB74GwoakRcbFTIS/w6/Cz8I+gQsApT+ZfuH+GH1UvK677vsyemW513k0uEB3wjcRNll1nTTxc83zBHIssR6wuS/L7/Rv/O/CsCxwVDCy8OWx6bKRc4i02LX3NrZ3lvjlecc7Xfynfi2/VcC+QcuDIUQIRXJGLwb4B2UHxAggSAtIeIg/R+dH74euR3gHOMbIhsNGlgZ9BgaGI8X/hZuFqgVahVMFaQVpxaHF9EY0BmpGxQd6B5cIWwjNCV8Jm8nwSc0J4wnFycCJsMl0SMUIjYgqx1mG7sYlhaUE2UQSQ0ZCoAGYANYAHn8Tfk89pHyOu9t7Drp/+Xe4nHgVd0H2vDXzNRK0TDOt8qRxtDDmcKvwF7AbcFawRXBUMPZxMPFccn5zabQN9WD2fjbbuDW5IHpD++882X50P5OA/4HXAyFEIIUrBczGnQbNR1wHvEecR+jH2Ifvh5zHu4dvRyMHDMcMRvyGZIZgRnwF84XOhc0FiMWgxaqFuoWmBgUGdsZaxuPHPQdNSAlIkUjLCTCJFAlwSSCJRYlSiQIJD8iUSB+HmMc8hkEGK0VOBItDzQMxwhJBWwC3/7U+gL4ZfTX8PTtIusX6N3k+OEM357bsdi/1drR+M7DyxLI6sWqxKXDm8IBw6rCc8Ljw3fFRcccynjN6tAw1FHX9Not37DjquiR7Ufy2PaS+xgAxQRJCWsNFxEuFEoWYBh5GuYboRyrHbwdyx0NHfocLx0mHNEbYRuXGrsZkBn0GAwYyBfrFtcV9RWoFoIWqxfaGB4Z3xlIG8cc8B3GH/QgACIqI3cjpSPsI18kxiMbIwYiVSDWHiEdYxvQGMgW0BTjEQQP/AuVCHoFpgJU/6371/gB9rPyFvAU7Tzq0+d75ffh+t9b3TrafNhB1ZDSANAhzb/KpsnAyK/H9sh3yLbHzsjAyUPLHs4Q0b/TFddh2tPcdOAB5Lboie3s8RT2Ffrr/iUD2QcLDIsPJxPoFDwXpBkwGp8bEh2ZHC8chxvJGtUazxogGpcZgRmuGJkXnxceF18XTBdmFrgVsBWbFTEWTRf0F+0Yuxm2GlQbcBzEHTIfkiAeIdIgIiHMII4gJCAfH7keyhwnG40ZERctFYwTjhGMDtYLNQmrBeUCBwAH/Yf6XPj89EHyVPDb7Ynrs+j75ZrkJ+Jt3+TdXNsO2QrXfNPXz3vO8s1GzNLL4cxHzajNN8/tz+/Q79IC1tTY/tqG3ZHg3OJ15s3qde7i8tD31ftn/wIE/QbZCuYOehDmEhMVahYNGCIZlxkKGhMagBkqGWkZthjNGJYYwBezF+8XjBe2F3YXuBZ1FhkWaxWqFXoWkBZiF1UXkBcrGEIZKxoeG3UcUxxRHNEbkxvUGyYcCBxYG4sZfBc8Ft0U1BPDEYQQDg/cCxIJKAd2BEUCqQAt/iD8JPl19lL0YfIG8GHtTutZ6T7nfuSc4lLhk9/N3P/ZtNZV1F/TYtJ40STRPNHF0CvRs9Dn0PDSsNT/1E/YXtoX22PfaeKv5Enpd+058df1z/nk/Y4BLwV5CFAL9g3QDx8S3RM4FcUWmBe5FycYOhi3F+0XShizF+EXKheEFk0WMhVPFQUVmxRHFHwUYRQbFF0VZBWWFvsXeBiyGREayxrqGhYbOhvvGnYaVxrOGSwYfRc6FmoVFRTyFZ4XRxitE0sPGAxKDM8J/AnKCSsGZQSVAQv8qvM79n72HfNx89HuDeuU6Nrg1t3C3Q3et9rY1XfVVtTCz4zQm9KUz1vRttDUz7vQ8dBh0fbVQdcW15/aotx437TjkOil7Hnv6vMo98H42PzV/x4DUQa/CKMLeA7BEMgRgBOKFLoVZBV6FfkVQxSpFL0UNhQqFA0UohOrErITbBO4E9YTIRN+E2cTuRMzFeMVwRiLGd4aCBwPHacgWCEcIVMgMyHkHv0cXR7DHcQdMBwsGnMYEBYzFZQTrxGyD+wMPQqAB0oDRwGI/1L8CfvV91P2FvQn8Tnw6Ozq6ALl1OM/4rbc9tvY2W3Xq9TvzjDMXcqWx/PGlskGzMrKO82f0BbP+NFP1sbV19Zr2njd6+B35QLple3d85D4BvuT/zQDsAazCroMLQ+0EYEU0hV5F2cZWRr5GgAb+RnlGDcYgxZJFYsUHhPqETAS2xBqEHQQHxC2D7QPuQ/XD6cQtBEuEtkRFRR5FVAWChn9GkYcdx7rHzshKiD3IdgixyAtITIfBB62HdMc/xvuGTYYSxZDE1sPeQwUCaAG3wO4/9b8mPls90z17fEm7r3qreYK5fXfEdxU2dLWyNN60MDMTcVRwjPDS8FGv9rCFsYmxuHIa8u/zJzR9tUm13nawd0B4YvlQOoJ77/0Vvou/1kDzwcnDCEPPxNHFYoV3hdCGcIZIxsRHFAcsxt/Gy0asRdgF2YV8xKzER0Qzw74D8QQTRA7EeEROBHIEUkSERLcEjoUdhQPFBQXuxnfHO8fPiJiJHUlXCZYJg4m/yRhJPgiuyFsIIcfJB8hHfcaBRgVFr4SuQ/bDFcJlAXvAjgAd/xf+1b4yfS/8aHuz+kw5obi297R2jvXLdSzz5XMYMoCx1i/rrsUv5HAi8JVxFDCy8QNzN/PftHq0yLYWdzM4YXkLecl7ZfzyPkL/wwB0AVCDSIR+RNnFecXRxofG3gbLhtOHDgdSx3JGw8aFBpJGcIXBhfPEzISohLHEDgPJhH8ECEQhBKwEUgRDxOFEzYTnxT+FYEWkhgkG8MdOyFSI/wk3Sa4Jo8nmCfmJngl9iOnIvggbiCgHnUd0RphGIQVexI3Dw4MqQhkBCgBSP3K+YH2GfSy8AvulOkN5qviWt3J2pzXONLqzrnLiMV6wtO/Krp0uTi9w77JwPPE+8XByP3PKNWn15/b4t3x4Tbm0eqQ74/0R/t5AccFYQqeD4UT3RfuGYQa0htmHEwcmRy1HDUcexsNG2QZRhf+F6cV+RI3EaMOLA0MDe4MzAtFDaMNPQ1lDrAOdhDKEoQTFROQE8AULRe9GbobVh4RIEcjMCVLJignMygiKFQnpCVgI74iPSIcIdAedRzDGr0YIBZlE5IPAAy8CGYELAB3/H/5Mvbp8rbv3uue6JTk3OGb3e7YrtVg0lnNNsmvxoTAhbs4ui649beeu+S+nb8+xPbI1Mu20k3Zs9u034fiieZw6/TvH/Zl+4wA1AaxC7UQshVsGCMc1x1YHSEefh05Ha4cMxwaG3sY+hfdFssUvRNzERkOkQsdCiIIYAfKB+oGdgdKCGgIMQoZDKQORBEyEsQSsROzFfcYIBzJHTYgICNuJXcn3Si3KWMqXyo/KYMm6CTSI+khDiBtHVgamBeDFYoSeg7lCkYHuwOj/7z7Wfge9QHyhe6R6k3n7eTk3/7cjtnl1LTRtc2bynrG0sLav9G5vrdouCS7VMDKwonEXck1z3XUKNsa33DfHubI6YHsr/F89nL8sAFKCLgNFhG/FvYbih3lHqYfAR/6Hr0dKhweG3gZrBjmFmIVAhRYEuUQrg74Cw8JzAeFBeEE6QQRBPgEVwa5B30JwwwkD5kRIxPqEzMVoBaSGc0bPh0AIOIhWyRnJownNyghKGEo6iaGJIciySBwH8cdwBrEF7cVpxNZERIObQpGBmQD5f+q+1D4T/Wb8jTvaewL6b3l9eFs4FHbA9bI05rPZcs3yNLFg78ju3O6WLkLu46/bcNxxazKHtAq00ra8ODl4XjmWumR7I/x0/aU/IIBYgdJDcUSbxiSHKweCiKfIvgg9iAuH7Mc/htTGkYX1hSjE+sRZhBpD40M+ggZB00FuwI5At0BTAHgAaoDrARSBgwK5gzODx4SXxNUFCQWfhnBG3QdAyCQIUAkbCbMJ7ko9yhUKQco7SVoI5AhXiBEHqIbYxgQFrETeREsDl8K+AbIA/H/1fu0+CD1HfLh7iDr2ufb5EDhcd/P2iHWrtODz3LLiMe3xJW+ermbt9u1rbikvR3A/MFuxzTM29H82ODdpuBD5cDn6+tH8C/0uPrMAD4GrQzIEmQYrx3MIGgjTCTnI9cjhiEbH7cdfRspGbYW1BQ7E6MR1xBqDfEJjQf/BGoDdwL6AGX/9f9lAKoACQNSBaoHAAtHDRMOKg8+ESMU+RYjGWob0RzRH3YiNiSlJZAmPSexJsYl2iOFIiYhXR+NHXca5he9FZkTRRA3DJ4IWwVFAa/9Pvp59kHzU/C47CHpJOcu4/ff9ty72JnUrNG3zofJeMa5w1O/Kb3xuMa15Lg8vWjAssVIyTvLgdID2rXdYeIk5fHoWuwg8pj2Fvr6ATYJjQ7zE3QZpB21IoMlAiZHJZ4jMCN/IIgedxwBGVYXCBXrERkQ2A7YCzgImAWnAer+F/5T/Nj6g/vA++r8rP95AykHQQpkDZEPnBHiE3kW1hcrGmAdKR9vIbUjwCXzJ1Ap7ym2KKwnwibiJPAiLyAQHhEcJxnJFYYSDBD6DL4JXAX4AGv9VvqS9l3y6e7H62bpROXe4bfeGNvg18/VJ9EuzJzKdccYw8O/4bzAtkO0hLmNvJbAksZqyIPMXNWJ21Xfk+LC55Lp7O2t8uD1GvssAtQJxg6YFIEaZiBeJbsnZSdpJ0gnyiMbIjcfSBuRGWIXlRNnEOkPeA1cCuEIggRtANj+vvzX+bf55/mF+S/8cP/TAWcF9QkfDdgPEhN6FMgUqxgmHA8ebyDWIislKCjlKsAqDSqcKzQrVihSJkwj0yBNHx4dChkNFUQTzhAZDjsJggQeAbD+Yfse92jz6e9+7TfqDOcT41Tg0dyV2SLWaNE5zjrLZsgxxCvAzry7t9W2zbi+vRDEK8coyhXPmdQq2+/g8+RO5mXsI/Bf88b50f6/BbYMuhM1GckdQiOhJ58oJymIKPclNiQgIXgdlxrfF80UURE8DwUNpQowCQcGrwG4/s/8FPo5+sX5evhN+gb8rv6yASUG5wmcDaEQiRGJEzcWHxoRHvQfLiKZJGgnbCoKLJAsqCySLIArWilrJtUk1CN5IXceVhsFGVwW5RPMD4QL3ge4BFYBbP0S+sD3ofVo8qfv6ex26qTncuUy4Kfb5tge1U/R+cySyZPFecGhvs25CLZFuA6/+cS9yMLM/c9Z1p/fa+Q95+DpCu+Q8ir4nf3ZATQJhhB9F4scxh81JNIo7SkgKRgnyiSuIoAf7RwAGusW3RNUECoNbAoRCVAGMwLX/r76SvhC+Ir4k/gZ+s77GP2JANgEbQiyDBIQNRILFDIWlhnWHJIgOyQHJ5woZysBLg4vpC/VLvUsISsVKgwpgSZlJLAiXCA9HREatxZVE0oQUwwkB3QDZQCP/WT7n/gR9d/y/fAa7QXrwecV5HngCN6n2X/UTdJYzgbKFsYvxJ29MLbrt6i5ir0yxTfJeMua0ZbZx97E5BXsU+x28A/17/ip/cgD8QsfEaUWghzJIKkkUifDJogmSiX1IP0dEhsdF8YTvBHUDkAK6AjuBm0DrwG2/gv6hveZ95f1yfb1+Sv78P2HAvUFOwn/Dk0TyxXZF50Y2hkVHW4hViX/JpgpNCyaLlgwVzDPLuQstSsVKe8lgCMvInAhMCDeHDgZJRe3FIgRswzDB7gCMQA+/Qb6PPd79cTz9/D67mnsoepL58rl3eCH2+vYzdXb0fbNU8v5xU/Cwb9IuT60hrUiu8TB7shJzaLOWNYW4PjkYekM6zXvEfPC+tH/MAOcC4QTpBnOHvQhrCOMJ2soTSXhIaUeuBtnGTkXiBMODwcMlwhRBN4Bw//H+/33a/Vi8QjwdvIK9BD1BPk1+0H9TAL+BmcLjQ9sE54VKBjfG4IfqiMzKJ4qBC3nLR4vsTCZML8vUi3/Kh0osCa9JYQj/yBDHqEbshdNFAUR6w1rCt0GLwJF/l/7tvlf9y/0bvHc7i3tROqd6MXl0OE73yvcd9e60wrSSM4rydrEXMFwvKC2frSksSa0t7ydxDnLf9CE1s7cYOWB7d/u+PCw83f5jQCmCFAQURb+HXYjgSYYKN8mqCTjIvYeghoEGIAVFxOlELUNqwjXA+z/NPvi9qvzbfCj7CPrkOwV7w3zTPcv+ur9+QG8BNQH9QuZEF4VURrXHIgfZyWfKuwu6S/lL1IvMTDUMBwuuCziKzErIikUJrojAiEbH6Ab6BbnEScOIwzeCeoFzAIpAIP9I/oT+ET10fJ38KftD+v76FTogeXy4+fgMN292HXWKNTIzzHLJscZw4e9jrcpsNCrNLFkvHvGCM/E0W/WDuD+6EnvB/FY9KH2Xv/aBpkMdxWQHT8ltCgpKs4ofSVQJNwgRRucF2sVcBNPEu0PGArEAzD9Y/ao7/vroeg/5XnlZOZT6DbtIPJn9Sj5I/yZ/fkAMgaMC3ASSxpeIBIlLCnyK80udTLAM2oyyDAAMFsw7DFjMaQupyuEKTAnNSRPIEkcRhkNF9ITYxDMDesM0AoFCE8Dlv9H/Bf7x/gM9jj04PK18RPwy+9X7lzsn+cy5F7fb9u+2MPVNs8uyUzGWb/4ub+1oar2pLasEbd/w+DNjdKC1NPd7unQ7cjydvXp+h8DQg6TFt0bPyVGLA0uXy22KGgi/SCSHlcZlRbjEz4R7Q6dC+EDfvmp8qzqsONb4TPgON454CLktuUf67LxGvUY+eH8mf66AXAIKhCNGB8iAShXK5stdjBVMyY2MTd5NPYy2jIANF41bTT0MDAtqSn9JTsgsxuYFz8VtRMNEZwNXwwxC8EIsQQv/xn7mvp/+XP4fvdQ98b3JPhq9uTyjfKK8Kvtb+hf5HTgrt7B243W98+AyIfDRrv9sdyoKKLpplu2JcJMy5jPa9L62J7jsu247mfzhvcpAcsNoxoWJCYpoi/nLzUtcSlFJHgfFxxyGYwWoRVNFEoRQAzRAjf3Curv4JzaC9gd2TfbIN7I4mXohOw+8PXywPMT9qn6WgBGCEMT0B4wKPQvGDJqMlQy7DLQM8wznDORM8o1VzfaN6o0hTB5KlokXB6BGHYVZxT8E4kTdRLoEHAPMQxTB30Bwvz++rT5kPsg/ev+B/8h/r/8s/rl+IX0gPHu7LHqsegP53Xket/Y2EzTjcqXv6m8YLIVor6d657FqRPAvM+Y0gHTdtdd3STok/D771D3qQKdEVEejymgMtU0EDW3L18nuCKzIEEeqxxaG8wYhBgsFssM9f6I8bjlatlF1lXUbdNl2OTc19+I5CjpIelX6//tWO/M9Fz+1AitFHIhfiu+Mdw1hjV7Mz4z4jMVNEc1SDcIOBo6zzqnN+EveymjIbka/BWaEdYQ7xJ6FasUiRO0EcIO+wmqA3D+svos/R3/dgLzBAsHdAbaA/YBA//8/BL5RPZY8knyv/HI8AvsieSu24HTkcg8viK6ra6hpEyf35pxoQm2bMjr0XPUjtKW0s/cxemI8Mv7sQVJEzEiDS9rNrs3QzebMoEtDSlXJh0l2iRMI88f1hxQF5MOZwHh8lXl3dc51MrRjNEf1tLYNtvK3p3h3uGI5CDo1usl8ob7LwW3EDMejihOMDA1yTXqNCo0wTVJN3E7Hj5NPxA+AT2LO9s0yi/PJDUdiBcJFfYTvRNGFOcS/BJBELwMBAjeBHQChADcAKwC/wbPCtEMnAohB14EYAMYAg8Anf5+/Aj8UPmZ9YLuMOgL3xvXicw5wnO/v7fnrgOo9ZzVk2aeMbVlx7fR29Nb0fnWFee289T80wK3CrAUiSFsMJU2ODpkPBk4jTDMKbomJCa8JXgh/BsMF4kSeQgs/Frv4N/S1uTRINAk0g7XCtt83WPfkt5X3yzknemz74r3kv/XCd8UCh+5J4ottTB/MbgvMzIUOFdBq0ajRTNCyDvaOWY2DzNAK0ojBx+JG7Qa6hn3Fx4TRw/CC1sH/AT1BTIHfgc5B50GqQfCCYELWghOBEcAggAjAccCSwQqAwoBPvua9kzuEOn14IXbLdRSzHLHXMFhvEazJq2wnVqS2pURpz6+3tJ915zSnNfX4DfwMfkwAagEPw51HvAqezXNOOA7sjfWMC4rRCa3JU0m4iPLHc0YjxLhCwwC0fHA4sTWKNCV0F/V2djV2g3cINtg2tzcIeCN4yXrQfTx/DgJBBSOHAckZSlmK1ctbTAsM1E4IT0FQk1ECkNoP3Y4DTPvLWYopiOuID0exh0KHJwWARCSCsMG5QRaAg0AqAD8A5AHpAgOCMQH4Aa5B7cHFQapBUkJbw03DukMKgdcAmr+ePlR8wDsOuXX3qDV39Izy4PAV7cGrLCksZf0jn2OmJyJspnJs9VF0jbN+81m3v3vbv0qA9cLyhptKsU2gDiTOf409y+5KsUmvSUuJ4wquSlNIiwX9wtvAJT1T+lB3XrW6dQY19/af91t3djby9hw11LYo94x56rxDvzgBIIMNRJmGNMduiF3IyEm6CgQMJc4mUGVRRpDID3JNKwyBzBjL8AqxiZ7IxMiyCBbHDEWRQ4oCaYFxwT5BBYHugg6CQYITwbDBX4FZAZkBTYGhAatCMYJBwvgCpEIxQL4+6r3IvM+8PLnP+UT3+bb+tMKyra73K41rril4aAZnh2WQpcgr3TFtdHC1GfPIM4+2jjvNPkDAxkNohhpKGQzejSvLk0u5izCKvwpZylGKdgr/ym5H2MWPw2PAnH4/OwU4QLbY93z34jfxd7Y27nX59d02APZcd/i6Qn1AwBDCKYLvQ7kE0sXiRxgJFYpXDG8N3Q9mUHBQXE/8TYPMjUu0y31LicwXS7/KmgnIh/HFy4RswvUBwgGlwcfCFoLBQ1BCvgFyAH+/if+pQAyA+EEKQixCUkK0QlYB1IEW/96+W3zyPKO8U7tiuYH4mHcMdf8zdTBybVfrWatqqcQo/Sh0KQ7tCHJstGY0LnO/c6P2hHr9vm5AL4IMxQyHyopZizIK8EnXCd8JvMlbifmKZ8q8yZfH9UVpw7hBuz9JfOO6B/izeFn5azmfeXv4Zvegdzv3TvhteYZ71D3Iv9HBv0LLBDhE4sWaBuQIpUqxy8KNWY4kTpVPJo56TY+Mgoxgy8wLzstXCw0Kq8lpiClGGwUmhE5EOkNqgyjDJEN3A2OC8oHbwT1ApgBkgLFA6UFtwUrBZcDBwGW/ub6N/cv8vnumu3A6vjlPd9l0wDSlcyrxdrADrqStfqwYbAcpv2fj6lJuOrDAs6gztnMu9Qz4HPsOPVQ/kYEAg3xGD4fJSKBJDcnjyUpI8si+SMjJlUmMSIvHPoXzhEcDFcF4/qn8i3tcOt67A/uHesc5o7jIOG54ePmAezt8bP5jv7dAskG2gmvDHsRhxehHlUovzCiNhE57TcQNQwxhi1gLIstqTLhM04y2y37KIUlICA4GvwQ7w1FDysR2BJGEXgP9ws7CAICc/z5+hj9vf8UAWMBgACQAKT+kPtm9qfxdO9H79juue0F6vLl9N/x2QnVgMtxxEm/xLzLvGe5pbcCs+etnrCrtpHBbshUzBDQ7tTQ3zTqyvNS+Ef+qQQKDGIUqxZ1GdEciCGdIpsi/SE8ITIhUR9GGloVZRIyD4AKcgMY/jb6FvjR9RTzAfC/7WDtiOy+7WvwH/UM+wMB5QQdBdYHBgvPD/cWxB2jIq0mlynnK1wtoS6RL6ctCyw8KoAqWyupLaArWCeBI5wfEx5NG0YZQhaUFJITpRGTD0IOLAz4CLoEHwCy/vb+/P7V/dH7rPlE9yn26/J+8b/uJ+wh6jTnx+Y04tDeQtlE0w3SEMymyMHF9MPOxNTEvMETvJW1D7X/wd/Qatyh3TLZdNms3/nsIPSY+Lv6yv9VCDUO8BH9ENoT1BXEFjAVwhRVF30ZThqdFlIRaA6cDkkMpgjhANT8SP2H/V79xPo/+U/5Fvqh+rz8Hv//A/QH7Qr2DPQN5REaFTgZMh2oH/YgLyPYJKomiCj+JzUnjiavJkkmmSa+JlclJiN/IGEdSxuIGtYYcBb/E30SUhAqDuMLRwh8BYwDzQBU/oP9Qf0N+1f4EvYN9APzcvEC78jsrev+6JvnHemL52/kUN+R3QTbHdlx2HTWqNT20VPSnNIp0QzRqdNQ1yvcvtzw2WzbP9/758Ts0e1H7bvu2fMP9534E/nn++n+uQFpAr4DQgWlB34HvQVUBTwG+QgfCToI/gUABS0FzAWJBHwFIAWKBYkHewgqC5UM+g4UEFsRiBLeE1MWwBjtGuUa3xo+GmQalRsBHWIdWR07HYkc5BwMHJwbYxrdGMUX6Bb3FTEW/BYtFv8UPxL2D3gPMA8NDpMM4wonCkIJvAhDB98EuAOmAVz/vPsA+yv7Wvk+9r3zO/Iq8IbtceoZ6KjkB+WV5AXk1OEt30PcXdoQ2qnYE9io13/Y7dlm2uzZft1032PeJ91r3nLj6ObC6NjoG+xy7k/vOfGf8kj1B/dO+H/5tvo2/XwAEgGSAcoBRwLKA9UE1AXTBlQI+QgoCXkJzwqGC8QM1Q2rDZwN/A4hEJcQ6xGREgsTqhSXFIsT1hIwEysUUxR9FKsUshW3Fv8VAxXKFE4U7xMLE4AUhhXjFAsTjhEPEWIQKg9PDZ8NtQzBC9QKkAucC8kI6QOBAWcDgwYGB4cBGf5a/q4AuP9A/D35//i2+YT5+PZ+9D319vRx9Yzyau9D7pXsluyw76/vCe396Trno+lN6pjqEuho5vjnauff55fox+kN69fpKei96HTrBe6r7bfsI+zc7bDwJ/Jo81/08fSt89n0SvYK+hL97fyp+oD5dvxzAMoDTgNdAvgApQFwBIsGQwjGCJwIpQjOCOcJ2Ay6DakO6A5zDJcMwQ2DD4MRjRFbEFIP/g+nDysPZBAbESwRahD8EC0R+w1YDbENNgzgC+ILagw4DIkMWAsiCWAIZwePA9IGYgngBs8FhgFaAzkEKQObAqP/cfs0/Ib9Q/93/Y/7OfpF9Z/2jvph+y31BfL/9g/43PMg8fbwIfQe+UD6p/D75x/vyvZ9+9v17e6X8SH26vaX8hLzOPRq9G32QPYf9uL2RPdz9lz2Cfrv+MX0PPcV+ln9lPy/+qD6QfvD/vP+Rvy4+yj9pwCWBBQFNAHG//oC4gYRBt0CCgIFA4EHZAvmBwIFhAWXCBIMUQv1BigGzwk8CsMI9whpDL8NAA4JC48DagXuCTsLsQpGCJ4JeQ3LCfQClf9DC/oSLAjlALoCpwf0B+YEmgR0CKUHK/9F/fYBVwYeCY4Dkfyf+qP++AFYAYIBRv91/hH/0/uv+ncB7QCK+rz24/mtATkAoftB+mv5r/hc+WD6//to/Nn8+vt2+/f6Kvdq+9P82vhY+Cz93f2n+wj6APtK/1L8T/kC+jH8kv0g/UT+1f6y/Kr3v/ni/7UAt/3t+k7+AQFD/hf8mvv5/m4B7v/M/HP7//uTAccHYwSE/sn4ePzvAtoD+gL2/7QC0wR9Av/9VwHvBvoF/AP//Vv9jAPOBd4G9wRQCFUHkv3h+/UA+QfNCpAEZwDnAH0DQgRBAscFXQVvAnz/lfsKAlQJ+gW3AZP+5ABeBCUG2AEA/ez+Sf7N/8cDhgV2BgP+i/kd+3MBgQPl/Tn92P6nAwoBcfvd+E3+KQUiAwj8oPq9/J796/tkAHoDmgK2/4f5OPX++KkEkwcFAQn5LfsVAAcBc/2f+X77PP7vAsgFcgF5+bX2FfntBGsKbgH29BPyqfttCooQwf5O7Vr1wAS2CNkEHvx39/75OwCgAqcE1wAn/fr/X/4b/YP5ovvmBjYFmAMBAKD4rfkH/rIDSQJ1ADP+d/pY/EICBwLWAR8CQf3C9pb5RwDiAuMEGwHN+9X8av+qAK4Bxfs1/Pj7vgJJA/L7G/rS/c8JqgUH+GD0PvxIAgUHlQWS/Wn1p/r2A78HdgbL+dX2Df6NA3oCHQLf/cz/qwYCBjT8KfTd+N8DFQkNCOT/3vqk/D/8yQHpAvQA1ADm/1T+LQBrAMcDZAuH/zL6h/zk/YgBdwO5Cl0DYvli+hQAOApNCF7+KfZe/NkD1AaFBG/8fv/cBLMC+P9qAhn8Qf2wAAABnAFcAuQCv/42A/gBAfxM91/+sAgZCugAt/pO+2j9lQX6BwMANfBO9woGBQ1/Bqn3I/Vd/ZUG8QQOAH/8ovZn+CID6A3oBfz0gPL2+/kG1gcg+oL4XP49/Zj+Df+1A+v8G/kR/JQE1Qb0+Rr1B/gZBpUKPQIj+s/1LvuTAHAGawf6/dj5mfvQACsFZgRL/1v7S/3bAz8E1AJL/Yb8+wUXBxMGAQGh+rr6WgHMCEkLXgEH+F3+JwaVCAIFgf3y+toAvQfkB+z7TP//A5EF6QnWA8/4BvmVAgoMaRLYB/b3ffFw/XYOJQ61BA38Efuo/rMD5wcmBiYBN/oyAugKSAKD9zb43AVfCpoDm/yW+2X9zQAsCJcG1QAx+tn3IgBjCIcJmP8G+ez9oQKEBe0CLP5r+uP6WgJyCTwHOwDR+Qr3g/9DB0MH1/8q+7L9df7rAn8DPAKHALcBeQK9AaT8B/5K/DEDcwZiAj0CPfsF/8n/fwh2C/YAXvX9+j0GaAYbBxAC4v6P/hsDxAT9AcsBjwIQAEUC3gT+BAoGegFHAKcC3v91AU0CGQGqBIEEvQFCAab/ZADyAGoCcwYGBLz+ofunAE8HVgUtADv+/vyhBJYEhgEW/xT7bAHHA1ACJwDHAUP9/vpr/esCnANLACj/9PtX/mT/5//M/uz+4P74ABwEx//O/E/8wf2aAa0AHgBWAED+N/7hANgBMgNX/wH/fwEEAZT9fPxS/83/FgHgADsAnv8EAC78NvyK/qgCsgHd/jf9Gfu4/yEAFAHbAID/6f9YAWn9Hv2AAPr/PwJrApAClP60/b3+qAKiAnj7k/s1AVgCKQFt/nf6KP9PAkoBWwHR/ef5vfyMAKQApwD+AZABzf6j/fH8gwCFA0gAiQHEAKwA5QGnA+cB5f0u/cv9AQNz/8L8kQAlApwA6f8AAJ7+AP01/NUA5ADY/0X8lfyOAmwFaQCi+yz8wv0cARkA8v/3/zAAJgEPALn/s/9gAPT+/vzPAE8BAgHh/B/+EAEbAuv8S/me/wj/c/+u/ZP9nv0OADH/0P6p/7v+Wvz2/aMAS/7c/pL5W/59AFsA3gAx/Zf8jv6gAR79Zf1R/5T/4/07/3b+OgH2Aqn+0P3Q/ToApP4r/QD/aAJDAGb+gP0DAO3/g//m+3b7aQA7AQ4BYvxf/0P+gf03+hT/zwE/AHkArP0z/1H/hv/RAS8EbgEa/4L6//spARwF+wE9/zMBRQCF+yb89AMyACf/eAFEAOj+U/7G/nj+UACNAhAAOgFOAEL+hf30/mcFbgOw/6393/+AAycB0QFpAhb/zP+J/iP+OP71/iAA1P9a/0H/ov4b+nT7Afwu/1UD3gFw/gv+vQC6/wn/uv6uAJ79Av7nAJj/swEk/37/yP8eAFf8oPwWAMMAnv5c/Vn+Fv9i/5v73vzC/ZL+ev5N/xT91f2J/pz9TgB7/2T8x/wz/db+rwAN/ev+mf4w/zwByPzA+/D9sAGG/4X8/f2dAEcB/v2s/NX8+gBAACL9pP0o/vz+w/wG/yL/M//m/P/9j/9S/5r/yPoh/roAJ/6K/oX/MwAIAVEBaQC1/moB8wBf/h7+MgAGAw7+Vv3pAcQEMgEh/vf8jv9+/18AwAFL/Q4BuQBRAJT/5AEoAXz8X/68/63/VQHbAcUCjgAEAW0BNwBiArkAEAD7/5IBrwCcA1cASv7MAU0EBgKW/TYBmgGfANEADf9O/uIAWwHRAJ3/w/8B/73+sv7pAJECpPxz/tH/rgCUAf38f/7//Vj/PQA5/4b/7f6X/mn/Pf7c/xwAuP9//5D/7/7j/Kb//f/DAHn+nf76/gkAG/4a/5D9of2aAI/9mP+q/lwA3/3c/0f9f/7VACb/Y//V/Z0A+v49/pYAsQD4/8H+EP2o/vf9WgC9/rwAuADq/sH90/6oApr9XgDk/r//mgAr/34A/f8TAiP/CwDDAOwAUQDu/7cAUQH8AV7/kwByAW8Cav/f/ogAGgCHAFP/bwD4AGsBrP8GAEoCff+u/gf/7ABcAMAA/wHHAJIBo/9yAVj/eAHo/wz+vP8AAGcC/P/KAQsAkAA7/w8B7v+1AJMBgv6xACD/1QHxAAwAmf9a/4AAof8U/+//pv74/sT/hv/v/rf/Xf9a/HD/lf/z/+/+UP/T/3v/H/8g/pv+kf86AJD+1/4T/5wBqP9D/c3+XQDNAbL/pP0t/jv/+/++/0UApwBM/8b+F/6j/3YDxgEM/uL+TADgACUCxwByAKwA7QHh/5gBXQJ+/mkBUgF3AcYBQwG3/9EB/gKW/3oB+gFDAeYAnAGxARsA8gFtAGoA+QDkAJgAT//2AHoBcwGFANn/MQEcACICcf+1/9r/BP/p/3P+bAEA/8oBq/99/4QAEgBR/tb+JwBg/wgAFwAcADH9dgBTAZQAZv8v/7j9wf6x/WgAVgABAA4BO/7+AOX+7/7L/ygBYACp/8EAt/9NAQ4BZAFkAfAACAIwAIL/9QF+/zgElQSbAPAA/wHRBUYCywFUAXECFAIeAkkD7gH5A58D1gGVAngE2AF5ANUBwQEzBEcBPABHADQARwPOAmgB7f+qAUICqQFQAXsByv/YAUICXwIJANz+AgB+AKUB9P7B/1cA4//p/ZEAkQAl//7+fwANAdgBqv9u/1oAJQF9Acv+egGSAZkBKQG4AbUA1gFOAzMDGgCeALgCXALLAXv/0ABDAdQCvAF0ASgDEAGAAUABOgD3AhIC9wIaAR4EkgE9/2AE4gG4ApEB3QIKAgUCGgCAAo8CkgGIA+sBkgL2AAABkwF9AygBOgHIAdz/pQBrAoABRgBxAXECfwKpA3ICZ/90ARAA2f67AZAB5/8IBPwDcgHGAVECNQF/AMoAKACm/7IBNwJjAQQCJAFIAxsBYP8yAY//3wEzBPH+tQALA1wAxQEhASEBI/6Z/x4B4gLWBKP+7gDnA4gEGAPr/SQBtQHTAbv/bf+JAL7/fwJaASEBOgI4A2UB5P9Q/2b+qv8D/xABfgE1/x8ELwKxAL4BqgL2AeEA5/2Z/aECxQElAhABFQL9AW0B+gDQAAIBFwJIBA3/AQCOACMAIQO5AQ0EGwa2/yADDAJ+/mb/3f6R/c/87v8QA/IC3wdXAiz/IQIZ/83+Pv0+APQAwwGo/m4BPQFTAr3/7P8GAuP/5/9x/or+MwCOAqT/8QAvAi8CuP2aAtb98/5wAA3/LgHr/l8JTgQQBUgC1wXZAt//yQFWAvf+e/1iAuMB0wQDAI4AyP3W/qD9Yf6bBN4AUf6q/v3/7v6d/lD+KQB5ADIBbwDX/m4ARf8g/7H+z/83AWABNP/o/dz/TQH1AUICCwLF/yn95vwoAWX/5P5LAWD+Cf+E/2UAkf88/3X+rf8E/gz9Sv5H/3gBzwBEAID/Vf+p/lz/qf4P/44Aa/yA/t3/of6u/oX+LgLsAX//1PyG/3kAB//m/kP/jv+d/4r+iv4d/pf9Ff8v/wAAqf6tAFoAgf95/eH9nP2u/rkAPv2w/8b+KADT/u//t//2/CD+tv9w/xAB1wA7AXMBNv6y/i7/RQAA/ov/ZwHWAeL+OwDJACQANv9r/BIAiP2k/3D+0v1z/xP/lP///tYCSwFT/zMAxf/D/tIBdv8N/uP+kf3o/vj9ov+OAB4B5AEOAp7/NQAT/9n/NQE3AC4Apv5F/97/cAAt/17/BgEPAD3/+P9e/yr/of56/vn/jP+a/9//sf8x/+kAVwHaAM8Azv+b/xP/pwC5/s8AcQJO/jAAGAAYAAkARALcAI7/1QG4/3P+RACT/+H+MP/4/qr/uv66AIj/yf+v/tf+eAAN/hL/IP74/xUA+wCiAPYAXgDV/24BCgAAAMkAtgDj/8IA9//W/2IBSwH5/tL9Cf/a/zIAWP9b/iz/jf8j/qT+Ef8OAB3/Of/M/5EAUQHNASgBqP5JAIcBEwHbACsBXgDiADMCpgBEAToCJAAFAU4CBQL2AW0BgwDD/+7/Av/tAnMCx/+7/0oAqgJbAjUCswGoANf/EAD7AGgB6wBBAYABRAGuAI0BYwLL/+n+JP9d/6z/OP/u/2H/qP4vAOz/fP+W/vb+Gv46/aj9KP2q/jL+A/4k/q79lP3A/br9u/xx/b/9Vf4//rT9cP68/uT+Mf4g/Yr+Hf45/Yz9IP5J/jX+h/7a/p0ATAB8APf/cf/8/8//2/+E/4z+X/9nAE0A9QHSAb0BRgGLABgBNQHuASwDKQNmA2UDeQMMBEUEoAS4A+ED/AObAyMEVAPnAjMDhwMNBM0DPQNFA1cCzAK2Ah4CiwJ3AeAAugDxAB0BpQGXAUoBqADB/67/Av8i/x3/2P7W/UP9Nv0m/e37B/yc/DL7L/t9+vj5zvli+V75o/ni+IX4nveO9w34pPdc+Gj4JPgY9+L2Lvf19wz5TflW+Rz5D/mP+Vr6BPul+678E/3Z/JL8FP3k/hr/FgCEAP4AKwKaAv8ClwONApIEQQYzBikG+wXmBlgHjwjDCNUJHwnCCXQJjgnnCOsJawtOCt4K+glJC5kLFwvtCv0Kmwq3CaEI9AfKB0kHEgeLBdQDdANVAokAGP+T/mX9Mvxz+Hr23PQ88XnwQO0/6+vp3eZF5ZXiIeHW3urdzdxo2p7XXNZX1H7TLdQE1FnUxddt3EPhAukK79Hz1vf++xX+UQOMBSYI8wkHCq0MbAyLD+QQ7hHeFK4VJhbMFVQTrBE0EdgPxw9lD6wOJg82ENkQnxK0FD8XqBkWGxMchhzwHWMf/h9yIIwhQiEkImciNCGrITIgbh+4HIoZ7BUsEecMFwhBBPD/OPx/91H0ku+y7Hvn4OJu4NHZcdbo0BrL7cdnxEDD/L90vLG7LLqvtzK4b7YOuczGbNND4c7q5PGG+EADQg1hEpwVeRSFFq0WaRbrFXAVxBgQHjseLBzCF/YQFA1jB5wB3/sK9u7xou3/6evp9ep67xrz/vQY90H30vnk+3D+AgNTBqMKNg6IEd0Wix0WJCYp0yvTLIYsUCxUK+kpTChEJkwj1B9PHEEZfhdXFvgTUxGmDUQKbQc+BQsF1QU+BsUGRAegB4AIXwrcCysOWg98D/gNuArLB5sFNQT/ArcBWv7i+jr12fBf637mseEw3UnXXdAZyW7CPb3+uMe3v7aTu/TDj82z1nngSOh78mL+EgYWDX4QpBLNFKIWjBgQGr0b/h7uH6MetRvjFooRTAsDBfv9e/ce8bvqx+aM5KvjCeUz5hDoNulv6wXuyfBl9bD5bP8iBRIKYw6RE3UZBR8mJNImzSjWKJ0oXycAJRYidB85HH8ZURamEkIR8g9PDv0MjAs2C4YL5gs0Cz8L+Au5DdAQ3hMcFpYZvBy6H8IgPSH2IZAiUCKAIHMdTxreF/UU7xEvDhYLqgZGAqX73PXj7/rofeEL2sfRtsuFxU2+frg1soevvK3EqTKn4KSvqR+3lcXJ1mHhzexw+Q8IOBQ+HTsieSMzKCUpfCm3J5EnKShvKwcqzCSIHqEVgw9ZBhf9Z/NN6tXieNxE1w/Um9Pl1ffXMtsa3i7hWuYE6x7xS/iu/+AGcw4pFJIbNyPQKa8vZTFnMjYx9y8LLfwpHyZoI7Mf8RoKF9sS9Q9KDtQMWArvBz4F1AL6AOD/zgB9AmkF0wcRCvUMkBEjFo0a4h0bILIg0SB6IKcf2h5OH5Afdx5YHF0YABUSEcENUgiTA8L8hfSt6sXhPtpe0vPLTcVyvp+2Mq//pwOiOp7smhidvahAtiHHGNcl5CvvEP/0EAggtiigKwAsWSwNMKUwFzDVL90xtS/NLk4mDxkHEVkGZP4V9a/ok90X1bzORcvCyfTKucxM0lnWN9nR3RLiK+h/8AT6wgJiCgAReBi6IAEqZjHdNc44mjgKN78ztS/KKYEloSH3HIwWTRANDMYIBganAk8AFv1W+0n6RPnT+VL8sv/bAigIOAuWD48VRxrLHTEidyW0J4coUSj7Jx0oCij6JvkjGCDaHHwXshKeDHcHvgFq+17y3uru4hLcP9UHzQfF87wFt26uhqgJonybuJxUnD6cPaYAtcTGC9qU6YXzJwMLFWsjfi02MSUwnTAzN983QjSRMpkxRC9FL/QlHhjgDacBzPeN7Z3hWtZvzwPLGcdQxM7ENMcKztnTTtjn3KTi1ulk8vH9GAfyD8kZHCESKSMwyTQgOGM6jTnRNtMyNS0YJ+YhshxPGGQSKgywB7ACZf5i+oX3FPX79Kv0ePQH9jT4sP0MAzgJgA1gEqoX1h1UIswlmiiSKoAs3ixpLDcrwSn2KK4nNSRwH4MYZBP3DaIIIgFG+ufyyusG55zh0tp/1NPN58c3wha7WLRSrS+pDKY6o5Oi/qFVpW6w6MCT1ODmrfc0BHIRLx9dK3sy2TYlN3c14TayNHsx4S0CK/QlxyJSGZ4MywAQ87joiN7n1SvOUslOxjfFxcRBx+fKjNHg2MDfJua87KT0Jvz/Bb8PQBiDIVYp6i5pNWM5PTsYPD05AjV+LwMp/CFaGmITIA71BggChP53+jf3LvQY8j3xcvQd9jX46PwvAMYHtw5OE34XlRz3InAp+ixrLoEvyTG7MtMxMC98LGYpiyZIIp8b4RSyDQwJmAL5/Jr1CvH46yzlD+D82KfRd8unxM2/Fbv9tWOwB6r+p2Om1aasqUCsnbKkwd/S2OSB9CIAhQy8HI4pUDFsNd4zsTNiNJE0VzHYLEQpRSY4IG0YhQuR/WjzwOif3zbWic1tx8PEO8P/wzLGKMvN0VnZCeCK5fLrEPRa/Y8IfhOlHA8mvS3RM7s4YDzJPJU7OjjLMXUrsCI8G0UUaw6GCBsDq/7A+tD1yvF78LvvQfCW8R7zffYX+zsCzQl0EncatSGzJzcrKi1iLzIyTTRLM/kx/S8/LmIsAikNJSUhah1ZF28PXgZ4/0L67fW07yvq5uY95WDjPN6O1vXPV8lcxIS8YLTVrUSpO6lLp8Gm7Kf4qsmyi8Ej09Lko/YMAxsNDxxOJ/8xIThON9c2wzZtNyUz+S0GJ7siWB1lFUkIdPhC7WLiLNog0ozKfMW0xHzEOcXZxrrKw9Hx2hPjU+n18Ef5OwMCD70acCR/LSI10zrHPrNAdz+/PBk51DCGKXIgChcyD64HTwKS/lv5nPRv8Ezs8uqn6qPr9O6B8n34Xf++BnEOyBa4Hm4m2is7L0wx0jISM/ozvjMWM+oxzy54K+8mnCHOHB4YOxJaDL4FaAC7+7b2mvEj7QXqSucD5MLgtNqZ1fPQNMzDxh/CLr1cuX63DLSAsQGxd7IWtgi8ucct15jn9PljBVYQARt7J5Au8jOiM5IwjTMEMwQxZyqXJAIdOhmlEagEpvdV6hngFdfhz+PIdMU5xY3HCsk4zJHP2tSs3ADlkuwy9IT9tAX/Dzkb8SSwLrk2wDo+PnE/Cz2gOdUzcC0EJjkeoBbPDnMHwgFd/av4RPNG7rHox+ZU5SXmEOvu73f2Df/JB0MSehvrIgIp0S10MK0y1zPSNJw1Wze7ONk3KTWMMYgsMCYkIFcZAhMlDVgG6/4X+bPzwO947dTqMegH5ijhf9r90a7KB8XLwG67BbiStcqyZ7EJsSWw/rF+tAW6tccX1ibn/PfWBH0MqRkyJgIwXzalNkQ10TVtNusvnCs1JLgf8RrDFAgJ5vjV7CThONmL0RbKXMVWxOXDLsTbxbPJf85R1+rgDOcL75r3kv/vC9oX/yJBLn02hzsLPwpBqz8ePJQ3ZjEzK6Mj7hppE0kMDAYsADn5b/ON6xbmneJC4GzinOYY7QL24PxqBfIMfRTrGrAfbCRhKE4ssy8QM0s37znIPBE9XTqyNpIwICo6I6ob5hSfEAsMQQhIBUICBwCs+6L1K+8e6efjIN+y2lDV5NGgzfnKjMdKxMHBTr/zu6q1u692q+GpxqxxsBi7O81U34vztQJLDm4WQCMTLGgyIDWFMvwxPjOANCgwxC00J68jzRwDEwwF/fJn5SnYo886yBnDxcHYw8nFfsexySrM/NDM2IfgGee27oD3ewFBD4kdFCqONg8+L0HnQrJA4j1sOOMycixPJSsfiheTECML3QWcAQT8OvSA7cjleeDY3zjhhemO71P4+P8KBzEQ4RWJHdYiYCcGLcUvxjJsNAg4WDsCPqs9vjlqNd4uVyjxIBgbOxavEp8PHQxECM8E0AA/+//0y+3x52Piet2r2o3XttUa0R/Nwcg5xFfBxLwHuZqzv66yrvqtyLL0uKLA+tCl4FbyvQJgDyoWPyBFKVYv5TPlM1syjzQbN5s0/DCrJ5AhoRhCEGsD6fJh5gjaLdNdzYjJ+8YlyLfJqMnuyQvKdMwi1Djdq+WG79n5EQQoELwdzyi6MnM68D1xP3E+kzs6N/szIDAwK3wmVyD0F4oQJwhoAKH4BvD36sDkReDb4NjhU+ie7iX17fubAhkLGhFDGF4f/CVsLqQz0jfFOYY8Lz+TQLg/0zy9NzExFStUIzodgxi/FHsQhQxuBwEEk/+f+X30Xu8o7DXnkeGE3cLZ7dfB1HjQhco+xXvCH8ATv2u+Mb3uu+u5HLZgtba5K8HbzsLgs/NWByMVDxwsIBYobSzRMUczdy64La8thi+nKxgmjx69Fl0PJQSl9GfkTtnN0DvMHMoDyETIcMrQysPLE8xjzlXUaNzM5UPvSvrABXcSjB6fKVAytjgaPA47zDguNuMyhy8WLXAoPSVfIIsY3Q+pBQ/7F/KQ7B/mfOGT4ALg5ORF6XbucfWG+pL/qgTZCt0S/BniIdwqxjFDN6k70z06P+0+8D0+Ofw08S5IKTomtCFUH+cbchhcE4gMIQVQ//z4GPO57gTrp+gm5sLiYN6K3G/aktjN1CjMxcU0wEq+r7yNu0O8vrvHvlm/ab8dxPzIOdDr2jLnWvURAr4OSRWCHPAk+SygMdYyvC/YK/0r+CZEIpwZJxMpDcQGCf1F7xLkWNoQ1y3TB9AFzgrNJ83/zO7MMs7r0vval+OC7Mr0zv5mCf0Tgx/5J7ovajRnNYg0RTPUMe8vai44K+on6iJpHBUV6QpFAen4tfGt7ETpo+X15fLo7upH8vXzFfpH/n0AVge9CfcQEhpMI4stPzTDN9g4HjloN8c2XDW+MkAwcSygKZ4mwSKvHvoaxBYZEvELuwRs/Qz4KfTa8Xzvwuvr56rkzOC23A3XS9My0NjIaMWcv1K+Nb+uvjbBpL05vQ+7c7xIwArBocnv2+PwCwB2De4QdhR5GwEltSkNLSou5CvGMIsxVys2IwUdZBViD1IFVvbP6anfztoU2NrWK9TV0UPQkM1syXvI+Mou0UbbH+WE7tD36gLzDF8YkCRLKgswNjISMHsxvDEjMtgy9zAZLQ4oIiF7GFoOmQRC+2r1QvBp6yboJuaC6BLraO6K8WXzXfZK+4YBGQyyE54cXSNfKK0t8y9ZNG01NzaGNxA2RjWSMBUsTylNJrAlpCG9HZsX+RGSDRIK+wbDA/MA+v1Y+zf3sPMB7z3rFOgn5YTiad1l2L7TjNEhz5PMVMlQxfHBjbzAuFi3vrrEwVfI7Mz0zoTTXOE48Fr+DQZmCuYOhRhFIZkmDywoLKEu0TAJLmAlPh47F6UTrg6lA8/3d+sn5RXh/d7D2lTWEdIszkTLeshZypjPqdfZ4Ybqm/F5+v0CDA3jF4EgjCZHKt4rTyzqL+syAjU/NA8wqyo3Ix8b/hH0CfUC2f2v+ET0ve+F60bq5+kq7D7tz++Y8VH1HPthA+MOuRgAIOUlNiiaK6Itoi9rMkg0mjaGN3M2xTKYLhcqmietI2EfwhmjFEwQBg1UCoUI0wZoA6QAovwu+NnySO+p7Nvr4erb6EbnZ+Ig3mfX29HbzLfIuMUNxRDF4cF5wce+jb+Jvra/WsPayG3Xe+il+OgEEAdXBswNVBksIpsovSkEJzsqyiuXKqUlzB7DF/0RDAr5+7Pwv+iL5/HmvOXX3//XdtJ5zbTLn8przEPTCN0r52juXfR9/EoFtA+AGOsbRR3rHhEixCc4LygyYjPhMIQptCIKGqoSggxcB2AC/P7t+d339PLL8DHvkew47pLpjutS7YH2JgbPEfAcqx9ZHwgglx9CJM8ohS7VMsAzGjTIMCgvjCurKL4lgCFQHm4ZCRXVEbwOHw9XD7UNSQlxArr8UvnR+MD3bPSf8AzsC+ql507kcuCt3ajcs9i40dPIqcHEwJvCmMVFxEu/QcDuwGXFzchRyT7Rv94a6971vvv9/3wGGhHFGkUhwSNVIhAlkCbxJuwh6h1qGiUWuBGvBx7+DvX38HvuUuxg5hbfntmR1b7SC9Gy0svW59yM40joAexd8SD4uP8BCCEPlxSnGd4cjB9HJDAoLyvSKtknVyOVHlIbzxbAEisOZAocBrIBOPo69ovxPfA68S7vl/Gg7s3wVfWo/ZoJqRB9FsAXUhhNHPkfHCZLKh0vfzPiMxsyyiy+KUUohigcJ4MksyApHbEZxBWfEdkONQ0KC0IHWAJA///8xfvq+RP2e/J97f7oc+RV4fDfIuBT3/ba3NL3yprDo8J6wyDEvMN3v1HB/MJgxm3LJc541dXjR/Gc+5L+4v7XBcoTiB+gJZ4onyQQJWkmCCTzH34akRZiFMsQhAZg/Jn0T/Gm7oDqe+PU3GDXrtPA0sbTaddG217gAuYB6l7tnfKI+IcAPQnKD8MV4RrwHTwhuyUdKaoqZyhmJBEhsB1aG7kXLxOBDkgK9AR8AJj5gfXz8UjxivEY8MHvxuwu8HH20QDIDMYSGhWnE4sTyxlZIBoplS3AL8Yx2S7LLBIp3ignKX8pjygAJAchRR2RGjEZAhZlE0QPowuAB6sEFATZA0YCSP7c+K70uvAy76vsY+kJ593jCuGd3XjXfNADyr3IjcdRx7XEccCqwlfDMMZ4yTTKG89j2Qnleu/P9aj5l/5YBzISNhz9IXAi2CKgIQojriJUIJwe6hqrFtIPUAgZAPL6dfbp8mntCuck4ZbcN9pe2WjZmtoS3anfMuNP553rVfGj9wD9SQMTCYYO8RP4F/obKSAAJLolrCV4IzcgDR+eHbUb2hf3EqEN2gmxBIoBifz6+ED38fNJ9P7wiO8P8Eb0Xf7KBcUK7wmeCLMNcRIyG0IfLyIVJgEnuCnpKO0pkiqnK70rgSmpJ74lryRyI2cggB4RGw4YSBOiDgYN8QoOCf0Ewf/X+kj4H/fN9OzxLO5U7A7oleOF3o/a69VJ07HPsM0RzQrIn8SMw8LBJ8C2wojGn8nvy+HRAeCY8O76ef9x/VP9dQY7E7kd8yJdIQIfbSNVJkgkLh7eFn8TTRBGDKUFFwAA+xf4PfPR673kt93X2ZvYCtlk2ijc5t5u4pzmIOoF7n/yiPh6/44F+Qs2Eh4XphwFISUjySO2ITsgjB/jH4QfcB4iGuoUnQ69CZoFKwJj/uD6TvnA9nH2BfT08v/ygvP99XL4UvtuAMoEQwlZDtYSsxclH+smfCvpL/ItDCygLNgsgDCGMPIuyCuYJlQjkiCUH4QdMRvdFcAQRgwGCFUHcQb5BCYDjP8t/Iz2R/Qk8zfz1fCo6+HjD90b2R3VK9UF1JnS6c8tyfTEKcDHvUW96L80x2HLuNBS2Qji++pD8hH2svew/BMGEhNMHj8hex5pHRYf6CEaIhMfLxsBFkkSdw2UCCkEuQD2+5v2Vu9l57jifd+F3tXd0tzl3dzfqOJI5kPpKO1i8R71ZPqpAHEHqg0AEzYXcxtjHlwfgyC2H9AfDCHuIDQfXBw3F14TWhAxDWkJlQR//3b7xPlZ+Xf5D/j29V30N/Uc9kf5wPulAeAH2Ax0EnYVwBliHeUevyFCJZ4oaSytLZcsACvuKWwoDSfrJAAkwSLqIHYd8BmNF44VohIiDcwHGgSvA+wD/QHX/qb5QPeW8nHwfesM57PkGN/W38Xdedzw2YzSB8/RyLXGc8Wuw5fEesYPzSDPys8d1k3dKuWP7qzy+PR2+QUA2QxpFk8ZyBafFf8XeBslHvcc6hpzFawREA9tDQMLwAcOAwr9FfdM8I/sn+oU6Yrm+uPp4sbjOuX95i7prOpu7WHw9vTF+poAoQUACm8OIRKVFIwWnBieGRQbrR18H9Qe6RxyGBEVDhOMEG0NrgncBdwCxgDa/iL+E/xN+bD21PIq8cHzKvyMBaQLnwkqBPYFQg3fFNYYzB21H28h0iNKJN4jDyY8KIwkfSLWIDkhtiMNJcwh/RxHGhMYrxXEEjMQvA1hDN0KkAcoBP//E/y/+Db2t/F+7kXsrupL6HbnOuJZ2hbYNdMn06TTdc/Jz9/NF8vsyUzJNMrNyfHNaNSl3BDkK+kb7fXw5/S++u0DSwqpD1MQ+RDFEHMTaha9Fu8WeBM3EOEMjgvjCY8HDgUbAqT8evhX9OnwW++Q7Yzrwek56RvpEeo/693siO6u73TxpfQ7+Wf+1gKWBaIHcQkMDOkO6hAlEqASmRPdFFgVdRTWE7MRHRB0DqIMLAvyCXIJ0gcHB9cF7AMsA1cD8AMwBf0FMgeLCAoLPAx+C00LRg3LD3USdBTRFSAXKRmwGfIYkBk2Gq0b0BwWHY4cnRxdHOAbJxqBGEAWUxMeEQgPiA0HC3QIWQWLAhf/2PoQ9+fyPfHJ7ivsOeji4tLfhd/83CHa6tXvz5rP6c950a3SvM9wzBPLgM3P0f3Xut4w44/nr+oG6i7tTPO/+U8C5wXPA+sD8AciDDIP9RCdDDoLsQr9Cv0KUQpWChcG8wNx/gz7fvnc9w73dPSE8kPx9/BG8STxMvCN8CzxFvLM88L1bvka/aD/hwCxANEBfwQtB2sIxQlZCo8MMw4eEDIQ8Q8NDwUONg6mEUQUfxZkF3MUSxNnEW8TMBNUEWgRQxCiD5cQPRBdD18Oxg2ODS0NZgySC0AMLA2IDi0OGA6QD0MQ0BH+EWQRIBKMElwSrxGKEfcQiBBDECoOmgzLCqAIkgWzAtsAqP53/bH6h/dO86vw3ewX6ZzntuRv4nvfvNv82a3YBtil1crT0tPV0yjW39fZ16bZbNyi33bjluV15rHo0uz273D0BfZk92b6wft4/ij/UQGGAXQDYgJ3A/ED4QJKBB0CQAFi/wf/DP1t/BL7tPnZ+d74XfkH+J738vdP98b3/viS+bL6cPy4/dP+z//eAPkBAQNvBKcEHwYWCDgK6ArAC+UMIA3BDbEPixD7EIEUFBRTFJ0TzBNAFKoSbhKFEBMRlBIBEsMR8xBCEJ8PrA5kDi8OcQ6cDrYOGw+5DzkQkg+eDvEN/A11DTQN9gwJDXMNbg0YDMUK8QmqCDsH/gW2BIUDrANPAogAff5P/Fr51fZG9bjygPHY8E/uTO0q6+/om+b25KrjD+ND4qXhH+Ik4ergv+Bf4A/hnuM75ZrmKOd5553n+emR7IDsR+/J743v8/G380jz8vVJ9nz1yvZV+LL4M/o8/Lz6pvqW+0D7NvsV/Uf9Ef1i/if+Hv5a/2AAef/8/xkBGQFdA+YEmAVZB88H7wbjBkAJ3wlPC6QMmgs6DgsRbxCjD5cQag9lEKYSFREKEh4TmxJCEU0QsRAZELsREBAhDsYOAw+pDrUNBA0pDL0Mnww8C7EK7woCC18KVgm8COoHRQibByoGYwb/BVcFmQSoA6kCfAL9AZ4ATwAh/0z+c/33+4D6a/lq92f2NvbW9G/0OvLq79Xvku6v7Tjt4evX6lfsAutD6pzrIunD6Afq9eh46fTqYus/6+Pr3Oui66rsK+6W7uPv1vDN8mTzIfU59sD0Bfdm9wr4SvnN+p37s/x5/jf9I/4//6X/v/9VAUQB6AGeBEQE2gRoBmwFEgVWBr8GNQjBCeAJjwqCC2YLigurCx0MDw0RDRQOlg51D18Qqg/FDigOgg7DDk0Puw8sD7UP8Q6GDUIN1wxUDA0MnAvfCngLPwtmCsMJ4AgKCOUGtwVEBZUFxwXTBZAEhgPLAtQB4gCGAGUA9P+c/4v+6v3I/Sj99/uW+nT5yvhq+VH5ufhP+Nn2fPX99IH08vPT81TzLPLa8pDyffLI8jzy/fFv8RTydfPK89fznvPM8krzTPRn9O70PfXi9GT1GfY69hX3t/aL94H3Nvfv+DH6pvq3+0T8CPv4+z397Pyb/X7+7f45/zUAt//1/18B4gHxAVMCLQLLArwDeQSmBZsFqAWVBVwGlwaMBxkJ4QixCJgInQhBCdIJCgqDCZEJsQrXChYLCAuGCkAKpgprCl0K0gruCmQKuQk8CfYIKwnECFkISwjgB2MHDAe0BpIGdgbaBQEFGAXrBOIEpATMAxcDgQI5ArkBXQHYAFUAwf89/7T+Sv4Z/pb9+/yu/Cv8Cfwk/PP7ufuS++z6g/qY+gH60/n3+TL6Bfpt+bj4k/iJ+D34pvja+AX5APm4+Of4gvnN+e35fPlV+Zn5jPkZ+pv6t/kB+jb6IvrQ+nb6ivpV+sn6afvz+0P8gvxA/NL7jfzY/Cf9qP3y/Xv9ZP0W/qD+Sv9M/0b/lP+Y/77/+v9nAHQBpwEdAa8BBAKGAlACMgLbArUDCQQQBPgDdQS6BZsFigVCBaEFWAbDBucG2gZHByIHMwfHBpkGugblBvQGAQcdB0AH7QZLBpEFewUaBv0FDwbuBfgFywUeBfcEwgTDBIQE5wOOA3sDNwMIA6IC+QGUAaUBEAF1AB4A8P8kAO//yP98/zP/iv7F/Zr9rf2g/TT9qfz7+8v7jfsQ+336RPpD+uP6JPus+j76YPp9+n/6g/pf+nX6Yfqv+o76ofr7+in7vPot+kL6Pfpl+or6uvoO+9L75PtW+zz7HPsX+6T7K/wm/H780/zC/KL8yPwh/eX9wP0p/W394P2e/vX+x/5g/sj+1f5n/q/+l/86APv/j/80/9//aQBoAMf/EAAZAe8AhwCCAL4AVwHLAVQBfQE1AnYC8QGNAYQBNQKMAlUCFwKSAuoCOALcAeIBigLzAsMCRgKwAhwD4gKHAoICDwNFA6kCDALkAWgC7QJtAhgCBAJWAhUCqAGCAecBDgLBAasBcQF/AUYBCAHjABIBFgHgAJgAggCOAB8Ak/8s/1P/YP8n/wL/8P76/tX+Xv7m/d/9Rv4d/tD99/1Q/i/+w/2M/XL9xv2X/Wn9Wv2m/en9kf2U/b/9uP2m/WX9P/3H/dD9of1b/Xr90f3U/e/96P0c/kL+RP4s/lj+v/7t/rD+ZP6d/sL+z/65/sv+Hv86/xb/+/7O/l//nv9N/yL/jf/4//X/1v+u//j/XQBbADAAVwDPAHAAQQAmAHkA2AD/APIAywDUALgAsgByAPQAEQFDAUgB5gAzAUoBYQFRAVYBagHQAcsBpQHeAQ8CxQGGARgBdwEtAgwC2wGKAbQB7gGyAUEBJgFUAYIB0wHvAcoBXAEHASQBFQEHARgB3ADqAOQAzgB8AD0A/v+p/0X/c//H/w4ATQAwAG//A////hT/fP+t/+f/uv9X/1//Vf/8/r7+v/4N/0H/M/8S/7n+qf64/oP+g/6W/sT+5/4W/wX/0v7g/sv+oP7G/nP/fv86/7j+l/7h/vv+7P76/pf/xP9Q/7H+L/+7/93/ff85/5r/1//n/8P/4v8FAP3/iv+4/y4AywCZABUA5f81AGIANwBFAFEAjAB9AE8AJABJAE4AOQAzACYAowCsAIcAFQAGAIEAmgDFAFEA/v9mAN0AigDn/18AowBxAFoAJAAXAEoAJgCE/1L/9/8sAOT/9/8DAMv/lf+X/6f/RgCkAEwAmv+f/1QAIQDX/3r/k//u/wwACgDG/5P/sf+S/3D/h/8NACAAxv9L/9/+N/+d/6z/+P5K/+D/O//Z/vD+Nv8U/wX/xv73/iv/RP/5/rL++/4Q/yb/A/8F/wP/Mv8b/w7/P/9M/0P/3f7k/gT/Of8K//j+3f69/rv+iP6L/sr+Gv/M/oT+nP7k/ub+0/7g/sL+7/7n/tL+Gv8b/9D+0f4W/2v/dP+B/4z/ev9I/5H/4f/J/5D/Rv8x/0//Uv8a/+n+Nv8x/+n+wv7x/i7/Vv87/w7/Nf+f/9D/Tv///i7/Rf9m/zj/+P4c/z7/3P5q/ob+of65/mn+Jv6f/uX+Ff9a/hL+Mf6z/gP/sf7a/jP/V/8t/wP/LP9b/zL/F/8N/2T/l/9Q/xX/Jf8s//P+Jv80/5r/6v/1/3//Nf84/0X/dP+H/4L/j//A/9H/hv9L/zz/Kf91/1L/R/86/8b/NABV/yz/3f5Q/5z/oP+m/5P/nf9G/zj/ZP+H/3f/NP8T/1H/dP8e/+v+J/9x/5T/Xf8n/w7/D//+/vL+6P4M/yP/+f6x/r/+Fv9T/y//+/6J/7z/s/9w/zv/cv+t/3v/Yf+R/5z/lf9H/3f/Z/96/5P/if/e//P/4f+2/8r///9iAD0A+P/e/zMASADi/7b/2v9YAF8AEAC9/yQAcgA8AOT/xP8nAEkADQDm/+n/PQAEAKr/h//F/x4Auv+Q/5j/9f+f/0H/Hv+E/+P/jP9z/x7/Xv93/zL/Fv+R/9b/xf+I/0//m//Q/6X/Q/9G/6n/AADy/6r/QP9M/5n/mP+2/9j/zf+H/3j/Yf+P/+z/9//C/73/FAARAAgA3v/q/zYAWwDb/9L/NABiAC4A2v///38ArgArAOb/KACbAMsAWgANAEIAdwDHAFAARAA6AHMAxQCVADcAAABOAE4AZAA+AHUAQQBQADwALgBCAO7/CgAuAH0ANgBZ/xz/of/3/x4A6v+v/77/2v+r/6j/p/+y/8D/5f/c/6H/rf+n/4n/a/9u/4D/r//V/6//b/+r/4z/Sf9k/5n/AwB+AFMAzv+S/4f/uP/l//3/AABYAFgASAAhACEAIwAiAFgALQBsAJ0A6AB7ACAACQBwAAABlgBWAJUABQGlAJgAmACmALEAfABXAIwA1ADaANIAZgBRADcAPQA9ACMATACYAE0A5P/Q/xsAYgA3ALD/Zf/k/2cAYADp/7T/l/+6/3f/K/9w/5b/sv9M/8r+nP7q/jz/Cf/x/ij/UP8W/9n+y/7D/sD+h/4S/lT+5v7E/mT+/f3K/Q7+Af7Q/Ub+w/4V/83+bf4P/m7+Av/u/vb+Bf84/1n/Gf+8/gT/CP/y/vD+//6d/53/sf9S/wv/Qv9J/4X/3//+//P/3v+6/2n/cv+K/2b/qf/g//P/0P+m/5v/zf+r/2r/av95/73/p//k/3P/Rf9G/yj/AP8w/2H/bf+E/zr//v7s/jj/I/81/y//IP9v/2j/+f7J/tz+3/4C/9n+pv6X/pL+tP6k/p7+1f59/p/+Xv6f/rL+Gf8G/2n+pf7U/jz/G//U/s/+Sv9q//j+6P7y/lr/Pv/1/gP/+f6H/xv/CP9K/8j+xf6r/sP+vf7u/hr/Uf+0/pn+uP4+/6//Lf/9/ov/t/8b/7L+tv6Z/7v/SP8V/3z/2v9z/wz/OP97/0T/Bf8p/6z////Z/2L/Ff/Q/vb+M/9u/7H/z/+Y/4v/FP/s/hf/f//a/8b/mP+u/+X/qf+B/3D/X/9w/9T/0f/c/97/3P/D/3n/p/+O/57/wf85AML/e/9v/5j/EgAkAM//ff+o/5//8v/J/9X/w/++/5z/W/+V/5L/HwD9/4T/O/+D/8z/pP/J//v/+P+E/zP/W//s/zUAlf/3/kr/BgBDAPP/mv9l/6v/qv+N/8r/BABlABYAoP9e/5v/XQA9ADMANQB7ALsApACrAGsAzQCZAI4ApwC7ADIBSwFuAecAygAjAVIByQHHAWcBfwF2AdwBIQLwAfwBywESAvABMQEqAcoBLALWAYUBUgGzAdkBfwEGAf4AkgGWAVUBIAGUAa8BGwFmANz/fgDBAJYARABKAN8AcwAVAP3/VwCfADMAFQCH/7X/kADbAFcA2v8MACwAXQD8/53/4//iAPEAewC6ADgBdAGTAAUA8v9tAfUB3QGwAfQBEAKAAVMB9QC9ASECygH6Ac0BNgJ3AlUCNAH/AO8B1gG0AZ8BwwHdAVkCjAElAaMB4gHeAVwBFQFfAd4BaAHaAKYA/gDnABsBXQAZAB0B9AAvAG//u//3/+P/3P/r/uz+yv++/2r/Zv9f/9v+tv7n/s/+df9I/wz/Nf88/2b/M//X/vr+TP+S/5L//f/U/2v/qP/r/x0AhgDXAIwAxQDRAJIAvgBEAZIBUgEhAaEAgAEcAjkCnwGyAT8CFQIQAlkCjAJyAkkC3wHSAQ8CWgJdAi4CsQHyATkCYgIKAs4BowF3AZkBWAHsAfUBjQE2AS4BbwB+AHYAPQDa/5H/yf8c/yP/p/6t/oL+m/67/lL+Qv4M/n/+yf50/rP9xP2r/pb+2f3m/UL+hf4d/lP9IP3A/Z/+VP6J/Rz98f0j/3T+rv1Y/R3+4v52/lX+p/6E/yr/gv5l/mn/ZwAnANn/b/8NAOgAyQDk/8z/wAAqAQYB8QAuARICIQLWAbkAmQG1Am0CWwLRATACAgM0AxUClQHSAWQCagIBAk8BuAHuAegBUQE5AAUBRQFCAUgA4v8NAC0BwwDO/8f/T//Z/0f/c/6G/vv+Gf/+/pn+ff1x/W39D/2B/HD8Ef15/YX9Q/y9+1/7/Pu6+7L7dPye/Cv9ZfwU/Kz7WfyB/Fn8xPzG/Hb9wP2E/Tv9jP3u/X/95/0S/nP+Sf9z/wT/Of+y/5L/CwBRAFIBGQGaARoBTAEmAgwCpgLBAecC5wJsAzQDLQO6A28DzQMxA3oDfAMWBO4D+QMZBAMEPwMNA6ADBwNnAxcDwgKWAmgCvQFaAXwB5wB5APj/sP+f/67//f5P/iT+zf1G/Vv8QvyR+9f7G/w2+476Vvpd+ff4T/lt+GL4IfhL+On3Z/gh+In3e/eY91/3zfbV9xH4A/nL+CH4RfiI+Q360vnz+QD6bvsU/c78kPw//aD9jf76/hQAwAA+AjUD2gL5Au8D3wQ1BrIG6gbJByIIDQpmCboJTAoKC2wLsAvADK8Mwg2eDdQNLg1CDkQOfw5kDtYNVg5hDfoNvgwrDLwLkQvDCqYJZwknCHIH8QXlBEMEWgM6An4A7P1w/aP8h/oX+mz4v/ZI9Qj0vvG48P7vWe7I7E7rj+q86Mfp5ucn5orm/eXg5oLnuOc/5x7oEekL6ZzqOe1V7j/x3PL689X1T/hQ++X8aQDLARoDiQa/CGULTg3rDn4QoBIbFHcVKxfDGCQZ6Bh4Gc8aUxwCHvUd5B3jHfwd5x0lHk8eyR6OHzoeeh3tHMccdhz9GjgZRBjTF00XtxUfE6oQnw6SCw8JaQecBYEE/QCY/R/6avik9T7ymu5q6kTpmuVI5C/hSt483FDZFtbr0c3QFc6Ry6TJr87P0vXW2NbU0KDRQ9Tg3YHhxeUi6IDqWPAT8qj0xfZE/Br//wARBLcJig6CEe8O5gruDO4RIxeEGPAXzxfoF/0XMxefFooZdhwYHIwbkxylHwgiIyIXH70eDyEbJKUlEyWFJRIlriRSI7chbSIvJFskViMXIv4gyyCdHq0cbhriGNoXIBWJEtoPEw46CssG/wEW/yP9FPqt9qrxye6q6dblyuHH3jzcntgQ12LREc/ozc3JicTev5fBbsZ0ztHQJ86cylHLMdLF1Wrd8t9o5UXsI+9Q8yH1Rvpn/MD+KgMDCSsQ3hSWE1UQBg/qEQoV6xYWGBwYlxhsF5YUNBO/E20UgRPiEloTSBYmGXwanxk1F7sWsRaIGLsctCD6IUgh/B6bHc8esB/BIXwiGyPWIx4iOyEXIXEgrx/vHNoawhmpGU4YCxXFEVQOnAuIB5wEgAH0AEn+ovkp9nLxje+U6wnpgOWp4QLg/9rU2DPXd9VW0iPOWstPxqjD08Aawj/IVc8i0i7PYct4yrvTRdo94anjjOdc7anwHPVf9o/81AAuBBgGvgldD+cUnhX+EhsSrxNaF9QXYRd1F+MXHhikFTgTTRKmEqwSLxJgEsITixbsFvMUlRTTE6MVhRbjFyMbFh2BHsQbkRofG9YcNx85ILsg0yD4IbggGCCSHjQe8RzqG4wbmBoqG2gYnhRaEHMNLwsaCq8IPgZJAwT/QPuP9nPzQvBe7fHqvuZK5EDgz9xB2wbXQNOkzlzL68mKwgXBMcQcxOrM6ND0y/rKD8tQ0UTY7t+F4hTlWexz8DT0RffJ/SYARwO0B1ALoBJzGU4YJhVZFJcVCBp6HOwa2hlMGq8YrhYzFeQS1hEKEvAOCw8YER0T6xOlEboP/Qz1EfwToxR3F0MXgBbMF3kZZxdHG1UdzR5lILwgSCHXH3Ai6yGHIbshcSG8ICgghx9LHGobihmRFhcUUBKOD14NyQtxByIDuwDn+4P5QPVi8drvqOo26Fbku97J2k3Zctb50bnPUswhxmu//7p3vPvENM55zhPKBMjoxnzNrNWF32zhCuXU6mXuz/Wy+w8BsAHWBNsHHg2bFIsc2h38HGkb9hn4Gy0eaB84Hp8dpxw6HEsZVRdnFJoR3A+7DkYPwQ+5Ez4S6xAIDj4N+g2DEIsSVhIiFdcVuxfhFTIX/hhFGucd0h15HqofxCKqI7MjYSImIacgiB+uH3Afth+UHscaVRWGEpcQ/A66DAcJxQPEAML+Gvox9vDw5+wH6F3jYeHZ3Wraodii0tfMushwxU3DGLjxsw25sb+SzBfQAckzwbbBHMtF0GvedeP85kXuFfDn9Oz3dwNEBfgDYwpED9YY7yL2ITMfRh2HHhogjR+bIQMh/SDzHzUbaRdxF68VKRBKDCALNgoKDeYQ9g1/DWAKIAesBv8I+A1JD4MR9w9gD6kPJxNMFaoYcxsCGVkamxtmIaIjVCXlIqkfUSCuIUEi6yBSIYAelRyKGAYVaRINEYMNCQhWBLUCmP/7+qz1AvHt7FrnQeXe3vDcjNnp1anQm8vLycHDqsFqu32zVrIxukbCOchsyMbER8Paw/bKl9WA2VnidOgT7F/ypfal+6T9DQRnBygN/hJqG4kf6SHgIkIg2yAzIQkjxiJHIzQj4iHLHngbgRc9FJIR0Q0JDMkLKA6zDY8M8QqUBywGVAZuBFgGigrYCqMMTQtqDTIR5BKpE18RJBPjFqwbJR4sIAchzSCpH+Mdjh6uIMMhXx+1HF4b+RqMGgcWVxAIDVwKhwcPBLEAMP6j+V/17+1P6HLmTuPP3T/ZLNZn0jvP6clzxkW+Ybobs0OvGrTEvxHIYsTkx6W+0r7VxyPMrddT3GzmBeg17M300vXu+soBEQSXCNQR5BYXH40jZSk3J7Uj1yN5IoUmSClDKNYl2yQUIqQcqBgTFt0R7A+2DzcNhQ2ZDtgNaAmrBB0DCgG9BD8I/wn4CHIKTgr8CxsQLxD3EPgP+RPRFZIbAiCHIdEgWx9+Hp8cUCAVI30j8iFtH98cnRsOGXkVoBBbDXEKqgUdBA4CWv8Q+wjz+ezT5znkeOPr3NjZ19Ua0mPQjcifxdu8lrjosk+yVb81wxnK+8XevtO/GsTey1XNZdfH2XffSunC7E3xEflQ/oz/IwX4CWISJBlkIy4ltyZyKCgm5id3KVMq5SmeKHEoYydvJOciOx/WGYEX1hOVD18RQxLPEbkPLAy5CPgGSAjyBtoEIQeBB+4J4wuADsEO1w0QD8kM2Q/HEgUY9Bk6G8Ec5hwlH4MfqB76HMwcvhznG3wb1hpMGWUWgBEjDV4IrQdpBaMAUvus9obzLe4g697mbeLk3m7aWdUu0ffOhcyqxwvAZbrYuKm7FsKGyZ7JfsbuyGbDwcOAz4fP+9Z33c7grOZe6zr0zfUZ+Or/Zv8LBcYPPBMzHYAg+iQnJl0jHSYNJp8msSmqKGgmryfAJ/skRSJTIMMaSRh2FwEUvhJ8FDgSkQ8mD10Mywu6C5IKEAkrB84IrgjfC70N2g+qD84NEBATDb0RtxKlFPsWuhe7GVAZGhukGngZ3xYnFDoTERMkE5AS1g+5DCgJFQSl/0P8JPh29bDw6e337H/om+gt4pTcktjZ07LSEsy4ykbIWsXWxbLLM8zozHvQv8llyeXKBM7e0MTTF9um2rrfyeUS5/7qNvKR8zr22vwg/zUGtQuQE9YWhhj6HHsdux+dIpgimyHRI5MkdyQRJYgliiRfIYggsRw+GU4Z8xdWFj8UehK1ENwP4A/cDV0J8wmxBsQG4Ad/B5kJPgmDCsAI6QhpCvkLAwuGDK4MOA0qEAAROxIHEoYRCxCkDi4NuAwzDEsLFwn+Bj4FkAPPAVb+lvoq+J30HfJV8PPvMe+A7Cbs0uc15fvjB+HI3+zcWdw93BPc09744HzeCuCM4APaENqy3VTc89zV4oriNeEK52rodueP6jzvsezr70L2WvaW/JL/uAIQBokHGAvlClENUxE3EegRXhT1FYwWSxiOGVQYJBiMGAUWORWXFmYVWRVcFJgTKRJ5EU8RLg4uDTENOAtCCxQLKAuzCysLrgsWClsKSgotCtkJNQrwCp4KgguEC4gLQgvQCmcK+wjsCPUIQgeyB4sGMgVWBMcDFwNfAoMBGAEf//v+mv5l/O78Xvpi+pX4LPet95b0jfSe87jyhfGT7n/sKOya7P/rF+7A7dLoIuhY6j/n0+Q85jzlR+QF5PXjGeV+5/nnJui76HPq5OrZ65rwS/Hj86H3yPj1+4L+PQAWAlsDWgVaBiAHLwrgCrcM3w/uDxgQEBBhEBEQIBDcECgPMBCoD48PpQ/+DlEPqA0XDY0MbwxADLQMkwynDCsN0QwBDIwMXgxEDDEMAQxGDMsLdwywDO0LUgx+DLoKTQt8CsIKngqMCYAJGAitB8wHwQbLBS8G8ASRBJ4DWQOBA6sCJgJSAQ8BRQBW/7/+vP5m/W38+/s5+jT5A/nw9yX2M/bH9ePzpfO/8pfwZ/Bn77Tt3uzI6yrrx+qp6k/qYOrL6urqTuvN65PsV+1J7nPvEPGJ8jL02fUq98r4gvrS+w79ff6S/yYBnQILBEIFsgb+B8YIqAkmCqcK6QqcC8ELNAyzDBcNXQ1ODZQNvA1yDYkN3w1iDbgNTQ4tDh0OoA6TDiIOQg4aDoQNNQ1TDdoMuwx0DB4M1At1CyYLnQrxCU8JhQjMB3UH9wY+BgcGEAY5BVoEUgSmA4wCSwLtAQIBwQAuALL/lf81/6H+F/6F/Z38BPxb+6H6GPqE+eP4UPjO9wv3b/bT9er0JPSh85Dy8vFv8bfwS/Di7z7v1e6U7i7u+e0A7hTuLO7K7vPuPu868JTwFvEs8u7yu/Pw9PH1/PYd+Gj5Yfo8+6b8gv2G/tb/wwCyAaYCiwNrBE4F9gV0BhEHgAcDCIUI4wg+Ce8JNQpmCvAKIgtJC1ILcwuFC2wLlAueC2kLfguwC4sLOws0CxALyQpvCi8KzAlXCWAJ3AhFCBoIsgcgB4UGCwaiBf8EhQRCBIMDLAPZAicCrwFUAc4AGgDI/13/yv5c/gT+bP0A/e38Ufz3+7j7ZPvm+qD6Tvrl+af5Vvnw+JX4Vfgm+OT3mfdg9w73uPZn9g72s/Vm9Rb1vvSE9Ev0NfQJ9Ab09fPa8+bz9vMR9Dj0ifTG9Cz1pPUz9s72dfcY+NP4dvlL+g777vvP/MH9oP5r/04ANAEIAt4CqgNRBCAF5QWOBjMH6AeSCAwJsAk+CngKywpDC0wLaAuuC9sL1gvqC+ELlwuZC3EL7AqCCmIK8QlpCQAJgwjVB0wHzgYUBlgF1wQnBGwDpALuAUsBeADt/1L/rv4v/qL9Hv23/Df8x/tg+xH7vPpT+hH63Pmi+Vr5P/kT+cr4t/iV+FP4MvgR+O33rPeG91j3Lfc49wn36fbV9qn2i/aG9nL2VPZF9lP2VfZh9pX2sPbd9iL3ZPeg9/v3jfj9+ID5Cfqa+jf7+Pu0/GP9Mf4Q/8H/kAB7AUICCQPzA7EEVwUgBuoGkgcxCOAIXgnrCWgKwwonC2wLvQv0CwIMIAw6DCYMDgznC6QLawsxC9kKagrzCY4J4whGCM4HHQdiBrYFBgUmBGsDsQLeAT0BjgC8/wD/aP64/Qr9fPwJ/Hz7Ffu8+jP66Pmh+UD58fja+LX4fvhM+En4Hvjx9/v30/fD98T3t/et97T3wffQ98b34/fl9+T3C/gP+Cn4O/hU+Hn4mfi/+AT5Ofl8+c35/vlP+qP6+/pS+7r7J/yp/BT9mP0X/pj+MP/A/0QAwwBqAfIBggIhA7QDTgTsBHEF9AV5BvAGXge/ByYIggjQCB4Jawl9CaoJywm8CdUJpwmRCVgJGAnOCGwILgi1B1QH2QZRBsYFIwV/BOMDTAOoAgkCXgGlAO7/Rv+X/u39Tf2w/Bf8mfsJ+3L6LfrK+Wv5Mvnv+Jn4ZPgI+Or3v/fE96f3UPeK94b3cvd/96n32PfP9+/3K/hq+ID4nvj0+BD5OPl7+aT55vke+l36oPrO+hj7Xvug++v7Hvxy/L386fw2/Y792v0k/mr+uP4D/1v/r//7/04AmQDmADwBkwHRATMCiALAAhYDbQOvAwAEUwSbBMQE+wQoBUsFZQV7BZEFnAWpBacFqQWFBYYFZgVABSoF+AS/BH0ETgQGBLcDbQMgA88ChwImAr4BaAENAbIAWQAAAKX/Ov/b/n7+IP7u/ZL9NP3h/JX8TfwQ/O37tvuL+2n7WPtI+yX78voF+yD78/rq+vD68Prv+vH6FPsY+yP7Wftl+2L7nvvH+8373fsI/Cj8RPyK/K38y/wI/S79Nv12/Zb9pP3P/Qb+N/5L/pH+tv7Q/vX+J/9Q/3P/s//C//v/JgA1AHMAnwC+AN8ABgE6AU0BgAGdAboB4gEHAiYCNQJcAm4ChgKcAqgCqgK9AsMCwgLYAtUCvgK7Ar0CpQKjApsCjQJpAlYCSgIqAiICBALeAeABlgGcAWgBXwEDAfAA8QDJAKQAdQAtADgACgC9/9n/kv+Q/yr/LP8L//f+9f7C/rX+rf6D/mz+av5W/kf+OP4r/hH+AP4D/vj92f3h/cv98P3c/eb90/3n/Rz+/f0U/gL+Rv41/lH+W/5l/oT+hf68/sH+8/4P/yj/R/+A/4D/uf/V/wgANgBbAG8AjwDJAMYA5wDnADIBMQFiAYgBlgGyAckB4wHmAfgBJAIgAjwCYwJrAokCsALmAtgCNgP9AgoD0wIoA4UCLgE4A8oJZglIAT8EJAStAOUEPgGTBIABgwJlAqACOQLXATUB7/8WAbIAtgEiANYBKv+qAdcA1v/5/yoAsP7y/ob/kv4DAFMAIf/q/pb+O/9o/3T+7f94/hn/xv+h/1j+3v5O/i3/D/9R/2D/mP4k/3X/EP/E/nv/tP6KAPf+bP+9/3L/GAC7/y7/7f/X/y//+f97/+X/x/9h//j+XQC0/ikAH/+3/9v/6P6EAP3+agDt/3cAEv+6/0gALQB7AML/0/9CAB4AlwAKASwA+QCP/xcBHAAmAUX/zQBGANP/mAHa/yQC2/+OAH0A6f+9AEMA5/+eANv/8QBSAJgAHQDW/zEBdP8GADgANgAe/8wAev+9/4T/Kv+l/6//9P9m/mv/0f9T/iYAoP5i/1f/HP6a/33+Rf8e/7T+2P1P/6v9TP+K/mP+n/+y/EL/zP3c/wv+Jf52/339m/+R/aj+if7q/Z7/XP6c/Xz/ff5GAPb9fv4I/m3+hP7y/03+2/1K/x/+vgAf/bT/VP8z/8X+1v8l/zn+n/9H/0r/FQC//pv+Qv9xAIT/cv9i/bf+HAAE/yD/HP63ANT/Pf9q/qkAXP6X//X/MP7p/w0AMgC+/hT/gf7YAFj+ev9D/6j9LwFy/23+4f5+APv/X/zy/6H+mv4F/2gAIQB7/lIAqv8W/rYAiP5F/Uf/Pf6CARv+cv92/uf8zP1///79of06/uH9If+//zH+vvyDAHT/UwDq/YT+8/1+/Sf/Y/90ANL/Ff6T/dP9w/+X/wb+Ov9o/bD/E/+I/ioAhv+F/+D//f1JAL39i/wf/8P/NP4z/skBXwAi/3H/ZP8Y/LoBAwD9/LkBs/8eATf+DwDC/yT+Nv5zAHT/Sf5jAer9AP/5/uEBTf9r/eP+0Pzv/uf9M//u/AT/0wDAAPb/v/4jAHb+NwAk/iv/bQCG/wj/YgCx/3v+zv4E/yoAPP69/8H+ywFn/fr9mgPq/Y7+7/51/5P+xwCp/pz9oAACAJD/mv5B/xL8j/xBAsr8/P0SAuP9LP5c/yUB3f1t/s3+YP9u/gIAKP/z/jMAsf+J/1j+//+1/1EBd/40AOD91/3JA6b+Sf/SAVf/zf6vA8oB1/5p/swBnP/t/tIAygCUABH+RQHf/87/Bf5dAEABgPx1AM8C7gBX/F4DsQKH/uABZf4+APACvf9j+04AiwL3+wX9PQE+/jn5jP7Y/4z90vzu/ckA9wB0//n9t//I+5v/mAML/n3/ggCq/7T8bf9mA+X+4fyV//z9GQD/AwH9J/2MAWkAof19/XUDzf0z/j8ARP8n/10BaASl/XH+9AEw/gf/xgEKAV3/TwFhATwADQCOAocAOPvp/sQA7v1OAt0C6/xJ/AsA6f8T/1b/rP1R/zL+aQA//8z+UQIS/aL+uP///5sApv7i/hUAoP94/2n/wP8wAYv+8f1D/pj/6v8sAMn/8/1r/xD+f/6j/4j+ef72/kwAEP9z/T8ARQDX/t/+fP4H/3X/+v7R/pP+qv5s/xz+MQB3/j/+0f/FALb+bPtV/yEBaAGk/ij+F//vAJMAbgDH/9j/JQGtAGT/Wf0qBeUTQA/lB9EFigglBsH88wa+AFgC8gLcAdIFHgHCAnEAL/g69v39zQH2/dv/4wQqCWUCwPwyBNv+D/3G+3f8Kv+/ADIGpgBvBFgAr//iAVcB8wMqAtoCowBOAHUBagIwAukEpf+X/sn+BwLm/bL9EP8r/Zj/XQHtA7ECyQGFAe7+H/yv/AcAZgJn/3sCsQzPB8EDWwN+ALf+lPlI/0H/cPw4BqcElP9N/vD/5/0S97L75v6i+vD7J/63/kUATwMQ/6P8rfow/Cv7dvjQ/j8AbwAI/kQAZgHD/Uz/4vxh+fr6Tv8fAAD+9ACcAycBagAQAUj/WP0u/y//Qv6NAMUCYAE0ADAD/gHI/0P/nP8E/Av++gHkAn0EzwORBKUAFgDvAOr/VwFcAPwBaANW/18FnwOb/zECQAHK/+T9gv6W/gj+qP8CAXX9GwAH/Rz6pP70+Cv3JPwq+j74F/uR+n75efbQ+fv1DvGu9XT1p/Or86HyKPNn8+HyevJa7u3wCfOs72/v6PIR8pHw8/Ox9GLxZfW09kr12PfK+hr7DPqb/qf9SALXA4wE+wbcBjEI0gX+CxcObw1lDmsPww/TEHsS1xFPEtsTcRU5EiMTsxXgEzATHBQrE18RfBP+Ej8R4hCYD5oNhAxnDQsLxwiTB+8FiwT0ABgBdv76+vP6w/e39N3wZ/CS7w7rAOhm4mreSN4h3F/WUNBqzyjLTcWIwhzHfc8/2Kjaqtk+0nXRu9rT2zriMuSt5UnrlPV5/m390QKNB/ABYwRmCicMGxTaGTceAh66IW8iHx8/H8wanxjCFuMXZRxMHoUfsSDhG5QZMBlLFYwTpBHFEl0TqxUpGXsYcBmZGGIVeRLFEZwRMhRgF5sXrhpFHKodNh3iGv8XkBUoFKQS2hKcEycTXhBvD1kJtQVJAWz6MvRz7VbqZOaJ5TfdY9f00orHHMD+um+yWLBTuHq9zb56xWjDP8Nyyv7My89u0NPWqtxg6ED32PoaAAULTAoBDq8SkRPHGU0eQikdK4EtZjEzL4otfCzkJvgg1R9HHS0bIxluGxgXoRLID4AGSv+Q/KH5vfUI9Yj2cPbS+lb/Kv+o/CYAqgAmAFoGGQzLDNAShRsPGocfKyiMK6goCSrlKeQltit9MAUwiDCCMNItMCuZK+glvSAFHFQVOg8NDJMLgAhIBD//UfZy7H3oxeGV20PXDc/HxTDAy70AtPKwYbBloY6dgaiRqmGwMr0FvJS1FsI3yyTKWNhk3yjjpe7++3MCgQe3FG0aKRrQItQljCZ2L4YypjZvNyI4hDYNMHktoykKInsfoRvNFDAUvRJmDUwKrgUy/WT4sPXs8abuDe/17QjuHvAp8Rru6PB98qPy5vds/S0DNxCHHDIdAiHWII4gtiKbKP4r4C/EOX88MzsaO8M6EjmBNjsvlCWII+4jiSS5IwIg+hk5EhgJFwAx+lH2XPKN6EXkd+GC3wDegNa6zB2/irsBtfarV6rIosObraRstQq8UsDFxMe5Erl6xi3NgtND3FblfOqg+iUJKQ0JEtAdshuuGqUhSSHwKkc0XT4NP2o7Mzr/Msgu8yspI2wbdBiBFeQUYRR6FTgQwwiWAWrzPetF6kbpmepP6gPsfevT7D/zofB87Ynul+pA6k30aAVbCOMTgiPDHzwkeCigKt4ptDOzOOky1juCQ/VDyETfQiE5QzG9LmgoSiV8JKAkhCBwGmIVtQ4zCSwCvfh18azu5Osx6ormxuKc2aTSU8tWv6m817i2s0GuXKiKoWmlEbdjvlm9i8JduTq4jsczz2fYwt1C5/jsY/ckBqAL3Q5eGVAaMho/IEwiiyv5MCw7TzseNXQ0ii5cKnon9CCFGWQWNBRoEn4QIBDlCn0Dj/xX8R3rzOp46gvr8+pZ69Hqte0s8GTuu+3w7VfsvPLN/ZIJxBAvGBse4h4yKBooqyuoLukxMzU9N8Q+SkFVQ6BAGzscNcUw+y4cK+UpLSduIz8g1Rs6FvcPXQmJ//34sPVO8trwgezg5RzgQ9k41ifOvcb6woK6g7obs/qtCaw1oJ2oGrVZumvCdMQTwcjDvtAw1x3ZBeQW5ujvAQGfB68PMxikGzAd4R6gITkjrynGMjkzBTZmM2UthSu7JOcfMRkpEqoPDwziCJgJpAawAPn8rvQf7M/pJeqJ6PPoM+u/6j7te/Es8iXw5/Fw8pHzgfuRCDkNphSXHuQgwCPfKH8u0itrM3I37ziTPn1C/EFRPGM8XjU8L5IufirhJ/Ql0iPAHqEajRYiD9wHbQCL+qL2LPMl8p7ui+oV54rfo9sx1wjQz8lhw7jD/L6Quum6vLAlpmCqg7RUuU/HqsmvwB/J0c7D0Zbcu+F65STuBPzqA24GQRPTF+cUPh9UHwcfFCaQKMYv5jCNMeEtZCTQIu8e2BeAFJwPCAnXCFEISAQ2AC38kfSH7b7s+ehB6OLqR+uc66Pt2fAP7jLyIPML8UbzE/eN/XUHExSyG8ccnh/pH4Afnye5LD8zIzgpPes6Pjj4O9M60ja7LksoxCOUIRMkfSLAIAMdIBS+CnYEkADQ/xP9o/eP887xOPIO7+vpEuSq2yjWqtPkz9PMz8rLxn/AJrhEs2quHKiftQvCEcEzyXLHJ8Q3yE7Tt9bJ1HblPegK8TEBJAOIBlkRexM6FHYYHRuuIJol5S5OLucsNiz8JbQjGB+RGXIT9w7DDeIK2QcsB9YCV/xJ+VPwT+kk6nLq5uqA6xTtAOsw7lnz+PHO8eDz1fHd9B0ACAk+DIkWuxyjG2gjfyePKe4qmjLLMKMvSzjgORA77zlKNW0trilwKD4k/CI9IpkefxmwFtMRlAwuCAUB3fkB96D1BfTN8kLxy+rq5XjiWdwK2OzUKNEPzVbL48dXwge/jLa6rJytd7SBwpnGksdlznLG38qo1o7VYdvk4vbp/u83/GgHyAgaD24W0xCPEv8ZNxkvIjEqpy2PLc0psia5IIQdehm1EvEMnwvOC+sJJglFBjAA+vld9XjtReoO7QvtGO6R7rvvC/Bj81j2iPWw82b1wfet+0MFWBCwFGoaViC1HeMiYCaaKOcpbS3iL04wxjWWNic2FjOvLrsoPySAI1IhYB8vHRIazRZmE0oN5gfcAZP8qPdb9ITydfJL73jrT+YS4HHe/9qw1szRPM7wzdvKlcWzw726j7KYsa+0rbo9x+PNmMhx0BLQ384A2Bfal+CU5ffvQ/nj/1gLew8WEDkVFhNcE5QYsRo9JAMo2iulLLslSyS+H70YeRNkDWEJPQk4CzQJ7wZWBdv9aPYH89Pse+vJ7Unt7u1H8JT0TPJs98L1GPQ59n32gPpiA20O9RQIGm0a1BroHRwkAiR6Khwuiy9QMVExtzMZNKQ0aCsfJdIhjR/nIZshMh+SHE4XZw8YC8oG7wRjAxz8pfbm9bz1TvZp8oLuN+ZF47fgjds/3TnYCNWy0wTMkcn9yc++ZLcMu1m7YcHAzgvNZ8o61WHPHsv32JvYfd4p6mru0fJK+RkFvweVB34Q+Q7ND0sY3hhNIKEkUil8KQkjzCVDIcEa/RnsFCkPSBBKEE8MDA1RC24D2/7r+x70vPL19DTxufKL89vzhvWM99z3dPeu9uT5cfux/x4JpQ3KEfYYCxyVGMIgVSOfIpgmVCjiKB8rFS9OLWMt/iubKPckdiEhIFoeoR2ZG7AXVhRIEUUOcwmdBAgAkfs2+oH3o/ZG9DPyyu1/5oLmdeOr3aPdINiw1qTUEM+zzkDFZL/+v7XAeMSPyXrNrcxf0+7RkM/f2BjXrNtY48LmsOqA8uf8qv9MA9MLBQmCCmcQHhDCGDkbjR/ZIs0hOyTxIPgdKBzyF68S5xG6EfgPkBAvEOsLjwdeBQb/Vvtp+1b4CPib98/4tfkj+zT+Jvy3+cv7DPu1/NEABAf3CocOfhagFKsYNB0HHsAeZB8pIXIglySbJ3soGShDJ0QlxSAIHykeIRtoGCoW3xKsEDMQFA1JCSwGpwET/Qn6f/jb9bfzrvHk7h3t0+v/6F/mtOJ330/cedci1sXStM+7zrrIN8ZSzmjSSM+n1UXVks7503nYLNn328biruWY6L3vp/O192r+6P6CACoFzwW5C0UT9RZlGisdRh2LHs8e1xwzGo8YLBdkFiMW8BW/FccUDhGFD7QKAgbjBd4BYQHc/34ACwCcACoCKwGa/xMAKv+b/rEBwgOJBrEIUQwGDlURLhSgFpcWhheNGdQYnBumHv0fCyHJIV8f5h4KHY0bqhlAF0QWiRI/EhIQiQ7ZDLIImQXzAET/Hfyg+vP5EPjh9Izz//FI7YvtMuom5/zlkOTN34Devt722QTWR9Yc2g/XadrV3eTX1Nrk3HjcCeAe3zvibOTu5d3plezf78D1bfb/9k37qfx8AJYDIQiQCrIMFhDKD4MSfhVzE3YU3BSDE9MUNhWwFKcVfxW5E0AS8w/xDsYNzAz0CloJewicB3wH9QbMBakGrgT7BLMFawZACNYIego4C3cOvxAxEdgSmBR9EpkUPBVZFbwXhRgLGZ8XkxdmFkIWGxbEFJsS2BDCD40OoQ2cDBQLAQn2BfEDQAH4AKn/G/0x/BL7oPcC9m75h/LO8uLwY+0+6Yvpx+6y4+bmUecK3bXcRuDy3ybdFOD94TTgJeEn5Irhbd8g5hPkFOIL6Lrrruyo7tH0afXQ82T4GfqJ+ez/2ALZAr0H/gtxDA8N6Q/8DxsPNRBbENwQvBKcE2cUtBLpEjETfw/UEDYOUgy9DBILiApvCbALdQqjCY8JlwiKB5AIzAi3CZgLBgztDLsL+A3ODgwPsg+vEBsPOw+HEI8PghJeE5YT+RC8D64QVA5cDYcQlg19C5ILnQp+B64GfAgXBhIBVwFxArv+SQAo/hL+6/zF+qP43vTm95LzOPTm9T7wcO4d7vjswuf95rfm4ec/5wDpJOpt5Izm1+Z95aHmpObp5ovl1uhg6i3pqe437ujtcPFJ8hfz2fOx+SL6efym/4IB/gPjBAcG0wVqB2gJtAnJCaEMTw2SDRwP6Q5rDXkO0Q3PDJoMLQwRDIsKpgs7CzMLNAvDCWIIjwdFCZkH2wloCiMLWgzxDAMOJwy+DJgNawwjDZUOkw13D+sOUw/8DkMOIQ1QC6IM0QqGC78LxAuXC8AKFwnhCScGiQZUBZYGiwI8A7cEAwEuAyb+VQFMAOb78PmL/bn5dvfp9073zPL78sP0Uu8R8MnvBuu765Xrley4673olOwY6bnp0+rl6VLpBekL7KTocepY713s+uwP8k3wD+8G8yz13PR9+Gf6pPqS/SUA1gA6AV8DKQRTBI0FNwfgBzcJxwoOC6AJIgocDI8JPAt5CgwJpQrvCS4K8AlRCyQK7gkVCu4KngnZCyQMnwvaCxMM6AwWC78MowxpDcELzA2xC1MLzAwADJYKTgs8Dc4ILAwWDI8K1QjjCv4IOQerCFsHugeXBYwIcQXqBWMFsQMdAv4CtQTf/7oBcgGlAKn7wwLV/vD6hf0U/jT23vb3/Vfzs/jV8031cfE58sPxNO/j8djv5u/W7jjuo+1h76HsVO9n7cLrNe4h7fTrkO8K8OLuYvFv8W7yFPPE9bb1hPYZ+Mf40vpd/BH+CP8MALIAYQJlAjsDuAQxBTMFCgckB/8FSQh6CNAHIggFCTkIxQfFCFAIkghkCY8JzQhzCywIuwu3CWgJ6wrSCUsLWwqtDSUKqgszC9sLOgsgCjcK8AhfCcEJughaCWgJsgr0B2EGGQsUBOMGNwXiCDYFqAYaCVEE8wZSAlIExwXvAP8CFQYiAFACSf52A4/+xPpFAtz8Nv2b+xv6C/s8+BD6V/Zq9T74cvW/8hP2t/FV83bxKvI58YbxzPL47m/xE/Gl8gDsq/OG703vk/Ej8Q7xVvMv9MjxjfUt9Vb3x/W2+dD4O/r7+079Jv2s/nkAe/9BAboBPAKoAZsEuQRyA7YFmAexBVcGgQnlB6EFJgkKCEQJ8QkgClQK9gebCtIJBAkcCpEJDwk3Cl8JLQthCbEK/AvVBN8KgAtyB+MIwAo7ChYHNwkWCbUFpwfWCRkHOwchBN8GXwPjBpkHNgLMBi8EuQI0AjgF2QE6A+AAbwOO/7cAkgRU/KQD4fsLACH97vz//6z6pP6I+kH7Ivqa+Kv5NviD9+j4FPRR98f3wfMx9/30O/V99Dj09PPz80H0CfSW8jD0GfMt8+D0P/Tp9Pnz4Pa49Kz2D/gs91D4Wfu4+cH6D/2l+xf+b/28/rL/3/+XAcYCkwGyA6wDywQFBP8DQgmqA28HHAlABl8GagxCBogJUQjtCloHTgZ3D/gGKQp6CHULcgrBCdYJ+wnVCCULKQj8CG4JQwv7BeUIAAtjBqkIRga1CCoEaAbFAhUH8wPUAqkFkwIiA+f/IARD/3H/qgO2/FEAoP8j/AIB+Ppv/mj7aPut/kT4CfzP+zb6t/l7+Er8uvRm+hj5UPc/92H4FPi68m375vQC9vD13/lZ9ED38/dd9kX4CvaV+SL1Z/kC+Fv46ff1+vn3H/qG+4L5pf0l+wz+PPzq/p3+wP8uAd8AfwEyAf0DiQPUAm8DdQceBd0F0geXB8gIxQbBCgEK5glTCIwLxAmuCvQJOQzrC9AIHwwNCn4Jzwo4CscJrwqqB2gLGgenCIwI2wWHCPkFGwK+ByQGXAKPBikEDAFMAq8C0P6nAM0CJP5O/SX+OgDR+rr81v5G+gz6Bv0X+Sz5Jvkz+Gz6S/Qn/a71pfZ/+kD1GvUG+hj4S/PU+Tz2QPf/9Er5QPao9c73PPiv9TX3Dfyy9YL4Fvqw+Sn4rPqh+VT5Zfqt/BL81fh6/gb8Rf0f/iH/oP5D//oCRP+eAZQEDQPvA8ME/QRfBtYEuAlXBgEHvgr+BpIJXwlkCoYKZgmRCrwK4Am8C3UJQAppC1sJzAmnCQYJEAjTCekGywcmCIEGOwfVBAoH/QOgBZ4F7wJkBGwB/wMkBPr/8gAiA9P/C/86AgT95v2GASD97frT/zb7Svz4/Q/6D/uW+yf7Ovrz+UT6h/2v9/L5q/qn+q33Xvzw+YP3lf0p93D9OPds+076LPqY/K35c/v4+Qz+RfnG/Gv8of2U+kz9CwA9+q7+t/4g/or+eACH/z3+SgKT/7kAUQK8AbwCUwHsA7UD/gHcBXAEjwOXBlQEOAinAv4IYgSZBiwHqQbiB10EpwpVA60JggWbBtEGTgfoBCQFNwv7Ay0DTAggBpkCHAeqAqIFjgBrBagBgwDRBYr9BgNeASQAJAD9ABP/3QCr/NMAFQHa//38Of7lAMr5nwE7/DT8Ev6m/Lv+jf2T+4n9NP0f/uD9Ef1s/jD+4Pm//Qv/nf6d/Yj8vf5Z/c7/Fv32/2z7+v+8/Yf/mABq/tD/ZP+o/nIArQBF/noB8v+AATv/vALGAN//WgRpAp3/zgIQAsgBEAaO/58BaAYTAPkDrQONBCcCJgZPAToDtgdRAcID+AUzB3X/nAUUBsMCygOaBmsEzwE5B8MDwQLNA/8CdwMhBOMCVwJ6Ah8EoQMCAA8EBQPx/ncCNQSlAOr/xwL4Aez+/wHyAS3+5gLK/2kBEv/EAuj+d/1jA1j+WADw+5YBe//S/GgBqv/o+yb9lQGC/X3+AQBcAM/7wgDS/7H7GQCSAB/+Af1jAJ8AM/5DAFv/Bf+Q/5EAUwCM/G4DQ/77/tEE2/xmAp//VgD0A9X9/v/SBnj9tv+/A1IC8ABGATIGdP68CJQAwgFSAyr/jQZ//4QCVQIuBaIDTABxAMcJOALm/TsHyAH3BbcB0QFJBowAbAMQAP0F9QFPAuECzv3gBsIC1AGxA1X+/gclAOX9mgJiAmYE+vzwA8oA8P4cAx389gDwAyD+Gv63ABwCHv8ZAvz/ef8A/eT/RACq/gz/1wCnApL5gQCtBRf8PfypA9r+eP01BIr3jQAXBRb8bv+0/w8Cov0pANL/yv/R/lsApP+V/vACb/7TAfj/yP0tBG79Iv9+BSj8bP80A2sB4AAr/X4D0/ybA9sBAQAMADABPwDWA1sAxwBVAtUAHgIC/fkG8/+DAhv/PQFWBqT8iP+RCKj8twPl/0cA2wQJ/ygD+P5rA1D7agcD/VIB7wHT//cAEgIu/1UA8wDl/vsGYPxpAzP/+gJw/VMAVABmAY78eAAe/oUAOgXp96UFOPs9Bgr58gPV/Ub/U/9K/SADk/uz/Q4DKwAq9e4Iz/0l/+n7vwEO/MAEffsSAUABbPq7AsD8OQHj/ZH/e/qbBEP7Cf72AcT6EgPj+j8CLgSl+KgEuAKj+3n83QWt/tf6XwX/AAn6twJzA1v5WQNpAMX83AJXAK/+bvu3Cfr//Ph2BRwGbfsAAXICkfusB374xwQB+cAIg/8p+zgEbv3UB170hwvj+OP/TwAUA/cF5PTJCM8DAvxKAtICi/4S/fAEfgEe+0YAiQg3+tz33gdJBfX5d/0PAoD+O/lqBEcB3fU4CXD6PwLZ+zn+kAKk9jkImwAd9QgDYv85+Qf9lgNy+9L1TwhW/Qf5pv52BDr3sfwZBrD3bf5OA3798fqcBKv7bvWoBST/Rfet/QwGUP9I+yb+DwRsBOnz8wBVAfz9DPiiAScEYvdMAXH9vQPb/gACp/LqDJ8Cc/IHBowADwGx8wQJbfyiByL8Rv3wAzf9EwbB9J4JOABr9en+VwTkAmr/H/c/B8EEdPb++J4Qwfq/8UYRCOurEbL8Pv/d/gX8Lwml+d0AMvomBw7/iwGW994Ge/cf/nQCwP9F/BoAG/s29P8If/jC/2f3rvq5Az734f3rBEv2pfuAAVUAOfU8AocCkPETAdgGPfH6+PgLSvPI9OkCEwUQ67AJGQM17c4BEgaKAGzw4Qe2/5z25/o1CUcDjuv2Bg0IM/W2+pQJLvdWALAKJ/Ly/QkNHwCV99n5qgXXAef4HwRw+M4I+QCr/ToBm/+AADcBhP/sAUwGUfq1BmsApgRnAWwGe/4t+0UMn/yRAh8E8gDsAacA2P2kBdkCBfpt/EAHgAXU9bYC4gO2Amn9Pv8LBMv6vwRF+QAAJgCBAGgDu/XT/fsEYgeA+14IyAqbDIcE8v74BioG1v9z+XIGrwDcApj8DAGB+9/3Pf3Z+NHxl/W7/T71Iv3K99b5tvn19m/+9vXN+7oDNPka+RX+mwBTAXv9pv74A70A8AGQ/9QCqwaX+LwEtgSu+1/9N/zFAOz6Qfek/Kf3PfdX/uQBXf88+fP91/kl/C4C5P/8/pP9MP8O/WkBjAcTBdL8BPyUBGUBkgqnB2IBSQFnAroF0gCKAHv/6/x2/hECdwWsA8D90v8yA5T9sf3e/rj5vPm1/2kAD/3tAGYA5Pwp+q/8YgEC9xr4b/yy+yz+mgAFABD6v/k6+nD8dQDI/Rn5NPgJ/Nf8Avtk/of70vqG/df6kfyy+SP8z/3k+6/6KvuF/e77w/s6+rL5jvdP9tf5f/iq+Yz8Uf3s+Yj1zvr/+aT5ifrZ+TL3svkf/0r/bv9m/4QAnfqw/fj9D/+oAwID/wD8/zsGxQboAxEGigYZAgwDlQRBA5AEaAn4CG0DmQUFBcoEzQNCAYIFlQQmA48DPgUWCGQIsQV0AlQDegWlBd4C5QPaAP4CxgNd/on9sf8y/1P39Pd89un3Xvpt9PHwFO9e76LsKun051vj8uJ04HbcFd3J3prfutr21kbWo9ik1zPW/NYS1OXWLNkp2CzagOFW41blGujR61zxLPV/+6H8AANhCF8NKhHAFaQYdxxqHSkeqB98HoAhsSBuHgYdbCBqHsQf6R81Hd8fJx5nHk0deR7vHwMgwx5eIOwg6yHpIjogyCC3IW4haiBCHeIanRp0FrITcg/gC+4GIQSS/jr4D/aE8l3rSeUw4QLdtdfhzyPMh8R8wn29obQ/tC2ujqs+qAmk0aDRo0ezrr8e09TifOn38tP8aQNHCe8QtxBZFnsbWh22Ho0k1iMwHrMYmwv7AJD6Rvbj70nuUOqi6tbsO+3q7YntXu1Z7grwhvGK+BkCJQ0qGOMgWydALIE10DbSN3E4nDUdN1Q1ezN2LVkvPDGILp8kORx3EKgOyRRVDmoOyxX/IMceABuME3US+hwDH3sZKBjjHpYqEjDHLssv8zK5NHMo7BliEawRYBY6EBMDFP+u+4L10PBa5K7crNbzyGO6sq7hrJSqCqhvoEGU6ZQcmUugu6RdqsK3T8275RD56wm2HOcqsTX+O0M0hTemNyY00y/9KEMiIB0wFugHyfjB6w7ilNXBzQ/Gw8RlyGLMTNE22ALha+hS7obyHPrtBDAQExw4KTI1I0FQSAFK/0g7RhJC/TjfL7sj7xkPEisKgwMv+mf0wewT6ZTn8ubW6U/rvPF++gsDPQuvFmcfzShkMFQ0QDZFO/Q/3UDcQMg9uzpdOCw0jC2oJeUg/xyHF20S9gy5CrgIdQaHAa/93vuB+mv4dvV08lryTPGa7InpZOKZ36Xa39KA0AXKPcffwBO49K4Xp6aoKKqQrGq61dB94sj6BwkiElocwycZLrgpbShEHOQaExgEEZgN7AaDAIT2o+ns24fRJsrxxYHDHMWQyT3Tdt8z6YTyM/zwAt8JBhAmFgYeUiY3LSAzETnsPNs8qTkYMnMoIx80FS8K6gBJ+hf02/La7hLsOOmB54noH+xa7yv2mv6EBmIXEB3BKno2gD1qRuZHJ0UKPq4+9DztOlo6Hi8GKL4jYB6wF3APhwnzA7kClQCO/g0A3AOUBx4KCQlMB10JUguVCVEJvgSjBP4FrwO5/zf7wvgh7ZvmfNdxyYDDEbgdsAehl5cSkYqQAZUYlGyX7qhZxNLftQG4EoMf5i5EOrs+AT2AOW0unytkJEwWqQzOBJH5A/Go4YjPcMSQveq6trkMvvTDTc9K3cToYPMBAHELKRIzF9sbaSAtKyY27jk1Pp4/lzkMNE4qlBs7EhwHrvsu8qzqmuZP5r7njeYM5oTl1egQ7+byO/q+AMEK+BgjIV4p4ixtMo83Lz4TRQNCE0IVRR8+ATjdNaIwZCrYHfYMAQNNAQ77E/nY9zL0WfhY/cr71vw8BWoNcBFEEXoOGBJXGiod8RkUFkgUtBANC1r/QPKv6DbdotFtwo22xqrUoNOUloYwhBaGmoydkgGTUKQyylzxvx8/OrtEW05qVMhVV0zDQGcvSSLRFkwGR/Xj7ETkpN3M0g7BTrRErnqvhbJfugPFrNRi6CH6jgZDE30gvia6Kd4pqiZTKjgztDWkNt020C66KGUhVxHOAwb3OOoI4mjZ8tRq1Q/ceOMt6Q/tAPPY+Iz/xwpEDq8VVR6lJ2IyPTciPppB8EhiSgZHRTzeLQclIxx4F2oRCgzlCWEG7wXbAZb9dvxd+zv8Nv2O/pkBRQkiEDoXsBySHyshFSJYIFIctBhvFNISMw5FCZsDi/ye9p3tq+Ow1sfH0rvHsd+pWaWFnzKYipLIkHOUDpztovCtFM618wUdCUb4VdVYX1z2WgpSa0TcK2gTVAKt86DjRNhO0XDNdsmbxOC53K9iss632sGO0Gfegu/7A5YU8iCDLDcyhDEYMf8s6if0JaomVSMcIqEg8hjXEK0Grfq8727oxN9/1w3WtdXA2+rkf+4g9cD+lghlEDIaxxq5HBEfQSf+Mp42zTv0Os08VkJaQVw4AS2yI30bKhXQCkoAhgGQA94DdQEO/Rf96gDsAs3/ZQHyBzISkxmVHHMgSCmVMGowWilcIdsboBa/EKcFZv3U+KvzYO1R4zrWr8xqxva9TLZurZukhp4Lm2uZM5pvnFOg9KHjsMLUHf08LPVOh1ntWypfBlv0Tlw9JSDXBzf67+hK1pLLE8MwxRnJrcVAuu+xa7MGu0XJ5tax5Kb2Ig02Hj0rrzRZOK03azWyLpQjoh44G9gWihenFXoP7wp7Azn4+O4Z6Bnet9cP1RvU2tyb5gzxUfnyAt0LRxXDHEUfCyD1IVUmlSm0K9cpcChaKistsS13KvAhXRn+FegSShFRDd0JNge6CK4IpgeIA4gBdwLABFwIwgiaDAQS4BnEISMmmCacJ+InoSU+I0gezxjuE6MODAe5AK/56PBU6YHeztK3yMO/SLkptDCyGLFGrqOsuacSo9agV6Cenz+tLNNh/sYtUk9tV7BXn1yqW61NLDm1F4v8lvDx4V7Ow8QfwN7FjtDxzo3BTLcZuLS/Gs542VrjT/RcC8ceZy33Nfw4EDejNaAusyCjGBYSqQ0ZEIwRJgyDCB0Ck/hc8oHsh+NM2yfXgdUE3lHoZfKW+gwE8gxqFyse6h1GHXscQiDwJEcmISVnKAcrbDF3MjIs/yMAH58boBilF+IPdBACDJ0JjAYIA/H+u/rN+YT5Mf5kBZMOkxbZIDsqti9JMWUu7CiMJGAfgRh5EqMM6gebBBIDGf5o93vvOuLm04jGxLtTtCOy+7Dbr8uvHK4GrPOp56oFqmOoWbvi3ngIkzgnUllXA1qSXKBVAkQnK5IIbfMb5rvUnsE6uXW4fcQf0ojRAsgIwt3EEMxh2RviH+sJ/N4PaR/kLHE1kThIOV03LC0eHyIWFQ52CBAJTAh9A64Dbv5m+av1C/JH6wHmUeIy3XHjaurG83T7DQU8DlMZ1yHlI/IilSFvIS8ivCEYHqEaoxsrIKEkVifAI8kgRRw0GHUUEQ9BC30EBgQGA80C6wEkAcMBAQfeDCsR4xQgFz4chyIcKcYr+CsSK/IohSa5IsgcIRYgD4kHTwHr+/P2EvNa7VzlEd+a2F3QRMnZwda7kLkCube2lbEerFOox62TsKGzr8R14V8EwSqYRLlLwk63UNxMzj9IKgsKx/I645fUXsf9wL+/jcavzp3RQ80GyOjLGtIf3tjo+vE5/1UNyhlyJtgvVjQBNCwx/yfoHJYTpgt/B9IHnAhkBR8DO/3E+Kr1X/Rm72Tq6uYF4y7nBu1T9tb9BwitEP0Z6R/VIPkeThyhHQQgXiCIHsQbahxCIYkmXSiXJdsh2RyHGUIU2QvKBvIC7QPZAxIDJgElAokEcQn1DFUP6hK4Fe8a6B6OJJMp9Sz3LQQsCyh4IkIciBOqCh0Ccvte9uLxwetr5A3h7twA1iDOasTau8q37LO6rzKrKKrfq0+w97VKtJ673dbQ+g4hNEIJSGhFcUl2R909aSyMEg35XPBX48TRGcSvveHBk8/H1dPPCcm1yATQPdp25+Lu5fjaCFIV6R4YKT4tyS+XL7wpXB7UFfsQTAsyC+kLlgdFBBgDZPuW+cP3uPJN7YLnM+JT4jTqmPGB+3kDpg0cFpIegSLpH7Ie+R3EHhQfERuXF30WMRvuIK4k5SJBHhUbPhhSFtEQrQz8CBsG3wYiBU4ErQIFBH4GMgvSDgURjRSqF3IdsyOMKHIqUSphKOYlRiMBIOMbRRUIDkUG6v5i+TTzF+4c5wjhmtoU1bHPacfFwqK8E7oUucO1KLPLrSurj64Fsd+6jNF77jANvirkOM88HEJbQxRAkTOYHb0BPfH949jYctE3zJLMRNHJ01TR3su8yS/OptWE4PXodvIsAEUNDhpMJvcsoTBWL0oreiJzGa4SvQwkDFgOcA4RC9gGt/9K+5P4WvXm7eTmneFD4DTnie7L93//SwlYEoMbaSA9Ie4faB5NIB0hcB8GHvQcyCCwJaco+CYXImccARh4FZEQpwrsBF0C+QKUA9cErwR7Bi4IhAsODsUPmRLVFXobACDvJHgoSSrAKlQqcSheJacgFhi1D9oGaADf+iP14+3S5R7hE9yw1bzN9cU3v8S6ULdfsoqtha2frt+yDrhEtgK5ws0P7Z4Nsy61Nyo3NjwnPkU6Ay8FHNgE6PuE8TXif9WqzljOD9cu2sXSPMrtxhDL1NKB3RrkDeyV+osITxR9IdIofCw/LZYpYCFOG5QYeRQbFJsUdxFlDsAM6gXWAaD+uPfj7lTm+95t3c/kVuxj9Xf8EQXpDasWiBwCHdId7B49ILEgtx5zHW0dVSKhJn0odCawIYseYB3VGXgVug89CZoEngF1AbsB/gGPA6YEqAchCscMjhA/FKYZ1h4/JOAmnyd4KM8oqShRJxQj4RzVFaAO0Ag8Asz6afOB6lri/Npg1KfNeMdZwjq9zLrXtze11rLdroGujrA+suK2uMUi3+D6WBmFK9wwSjR/Nnc2Uy+FIfIK2vtG8S/pGOL43HXbTtx+3fnZddGCyvzJkc3x1a/ee+YJ8k//PgzQGewiSif+JgElUh8LGmUWjxTRFekYyBqeFykTqAw7BuMAxfpy8PnmfuAI3sfiNupH8276awINCnwRMBduGeIZGBqmG+0eoh/UIVUjkCfHLMguKCxDJTcfcxqaF4sUog7vCMkEQgIGAlACRAIbA/UCDgTaBL0GqwqCD0QVRBpuHwcjRiWMJionLyehJSAiohtoFNkNCwjVAx/+fvYC7ZXlTN2v0yPL7cEpu363ALVtsqOtIazxqgGt47AjsMi4L80j6RoHMiHPJsooOC5RMHcvfiWLFSUDQf3L9pbui+da48/gWON04XXV1cwqySbKuM/41/fcduWB888AwQwoGNYcYR7WHkUdmhnxGcEbKxsiHnkg6R0wG2UYTw/RCdMCvfed7X3kFeCo4unpZ+9l9Ib4sv+wBwMOLBLIELoSjxa3Gmsg6iEgJlcqazB3MxIybi17JjYiWx15GdYT4w5iDO4H8wZWBaMDiwI/AMr+O/+qALEDkQgLDSsT6Rl0HzEjwyUDJu8lDCXdIV0evRjFEnYOuwgVBov+vveN8Obknd3+0knJmcB1ueS1LrJRsXmuVK1+qzCqpKe5rBjBctgv97oKcRPXFzgenSWHJ6Mk0heCDRcGOwJ3+pr1IPEv7l/s4uXA2lzR5M6nzU3RFtUd2Y/h1O0N+ecCVwuAEBMTCxVsFQEV2BfWG/8dziHyI2QjQSIWHnUWQA70B4b/kvdv8U7tsO5Z8r/15faS+BL7FP/nA1YGfQg1DMQQbxcqHQciKyWuKZosxC/gLyYuiSrNJtgkXCCzHe0XORJGDssKfwikBNsBnv9E/p/+df9tAUsGpwmEDl8SkRV8GrMdFCBtINcgXSALHwEcyBeLEo0O0geJAor4AvDW5dfbLtTExyvCU7vjt7i2k7FMr+OqeqqEq1iuobpRylzhq/RPAzwJ+Q7pFy4dwiAnGT0QtAfIBdQDDwFu/S35R/Zr8pbrDuOF3XbZ39e21i3Y09v54wntufOu+UP9p/8pAm0EfQYRCg4PwBPTGJgcliDTIu0hrR7SF3YSgw1dCJIDzv7b++z7w/w4/SH+Sv2Z/usAwAEfBEYFdAfDDPsR2xgmHZwh1iTUJnUqUiu+K+Ur/CmuKWQoNiUPIUAdXBlbFaAQWAt+B2EFxQMNAyYDcgTWBkEJXgtBDSoQpxKIFGoU6BS3FPgU5hLwD2sMxAc5BL/+//go8TLpoeK22TfSdcrJw6nA0717ujS4ZLSasouyA7PFujbGtdhW6OnyhPaM+B7/AQXXCw4K/gXbAN7+0P/qABwCaQGx/5v7Dvdg883w/+607LHpeegt6Qvuw/LM9mf4XPiT+XD6yvzn/eL/YgKrBbYJfAzlD6MR6RKFEv4PwQ2LCokIWQZTBB4DpgLTA5oFVQebBxEI+ge7CAYKMQveC2gN2Q4mEWkTCRWSFkkXvBfaFqsVwRO3EQYQoA2ZDLsL3wuADIUMGwx9C0ELCAs1C6ALGAxbDU8OkQ8IERkTAhXXFr0XFxhtGFIY8xc8FyQW/RTHE0QS1BBtD5cNiAssCVIG1APnABv+cPtf+BT2bPO88GjtJerU5mrjc+Dc3YbbLtoX2GvVUdNb0GvOiMyMy5DKhMm8yCzIqcj8yLrKRMzwzYzQctLg1JrXsNpd3tThquUZ6WbsYPCw8+D28/kc/Eb+YADKAoQFdAhbC84N5g+wEeMTvRWeF5QZ5RpKHJcdPx6fHych+yF2I3skgyWUJjUnBiiEKHwoVSi7JxMngCYxJk4lTyVGJGojqSInISgg2x7NHa0cXhs6GncZcBitF+kW4hUBFcsTIxJZEJQOtAz6CjkJsAf3BcIE0gL+AFX/PP3g+nb44/WY8/HwLu6d65fo6uXy4srfNN1t2UvWIdMmz//LGcljx4nGVMYFxvHEHMRXw5rCDcTrxeHHscn1y/rN18861M7X99v94JrkUuj16ybwv/Tu+Nz9IwJ1BZsJEw2vD/cSPhUrFowXrhgWGRwaYhsUHPIcMx0wHfIcsBzjHEccahuXGrYZKRkSGYwY8BcwGE0YqBfNF6IXuBcRGHgXGRcnF7UXNhjYGCkZ5BlXGtEZpxllGcgZNhrPGU0ZOBkcGfkYVhjWF6sXXRc+FroUdxONEpcRMRBvDjENRwzZCgoJAgduBTQE+AHF/4H99Psp+uf3DfUV85vwJu6764DneOWm4preE9wf15bTWNJtzt7MssuBySbJUMfMxC3Ee8T+xL/Fzsbrx43J2suRzcHQzdNb18DbmN3f4ZDm2ulD77fzavfH/EkBgQRICLwLKQ7gEL4S0BPoFfoXlhkeGzAcJB0HHgceUB6RHh4e3x3eHJEbuxuMGzAbqhu4GyMbIBuiGiQahhrPGs0awRq2GnYajhqrGuAaNBt7Gx0bSxq/GdgYhxgCGCsX/hZoFtYVgxWuFF4UFBT4EtgR6RChD+YOCA6dDH4L1wokCdYHdwbEBKADtAFE/6v9QPtK+d/39PTf8trw3e3h6+roW+bU46/gBd2l2drWz9Ma0tXPsc6+zbnLKsriyBXIUciHyL7I+cnNytvL2s35z2jSpdUy2LDa5d2r4ALkq+dh6/zvW/RI+PP8WQDPA3EHxAk4DIsOLBDVEf8TERWdFkwYSxkzGhQb6Ro+GzYbUho2Gp8ZUBlaGSwZEhkxGe8Yzxi6GEgYWxjMGH4Y0xhOGVUZ4hlUGqMaUxsBHNEbzhuhGzgbBRurGh0aHRrRGUsZ7hgtGJoXABcTFiUVPhT5Et0R0hCWD9MOwg2LDC4LGQpeCOYGOwVoA38BjP9a/Yb7Tvk79xz1q/Ii8JTt1uoZ6AflNeIY36Lb69hz1bXTmNFxzzrObcwqy6rJY8jBxyLIJcnAyQvLz8zdzQ/QctKg1OrXydqH3W3g5uMw6CDsSPDG9Hr4TvwtAGADbgbNCV8MPQ7OEK0SUhSxFkMYYBnAGowb/Rs9HFAcBBzOG4Ab6Rq+Gkwa9xnDGT4ZGhlfGLMXkRcmF+EW7xbtFuoWRRdUF1YXwhfjF/kXPRgLGOYX6xe0F5UXsBd0FyYXGBdpFv4VfhWyFEEUwBPfEpgS3xFFERQRRxBrD+cO5A2mDNYLhgoXCRMIewYRBaIDGQJUAN3+mPyc+sz4OvYD9JXx8e4e7THqqufz5Mfh295p23jYrdWj07vRKdDdzk3N2suaygfKxckwyhXL/csHzU/OVNAi0mnUSdfR2dnc999G49nmwOon7zTzIvcP+63+zAH7BMUHPgrpDNwOsBC5Ep0UPhb3FzoZURoSGyMbSBsuGwEb8BqxGmkaSRr7GcgZmhnLGDQYsxfIFnUWbRZZFgoXHBcVFyAX/RbAFusWshaoFgQXXxauFp8WcBYPF+QWmhaXFtQVKBXiFDQU1BPME2oTyxLZEhMS2hGMEYoQCBAqD50N/QyqC3QK4wkyCO4GswW3Ax0CUwD7/ej7IPo29zX1W/MA8cvuhez36aTnqeTz4WjfNtwW2afWD9QA0nPR0c/Mzl7OjMzOy8jLScuUzEvNic5b0PrQDdNI1fbWwNnv3HHfVOJZ5tbpe+1r8lv2IPpE/n4BWwT4BvoJAwwvDrYQVxL3E/8V0hfbGCwa/xoFG9saGhsJG50aGhvJGlsashpKGmAZSRm2GGEXzRbuFXsVuxXaFeMVNBbaFZkVMxV8FHcUchQLFPsTlhMUE1MTBBPpEi4TDhO3Eo8S0BFsEXcR/xDgENMQuxCjEKgQDBAMEFkPxQ7dDbcM5ws+C3YKZwmtCJ0HVgYnBY0DAAKhAJP+svzx+s34GPdL9VbzL/FA78zseOqV5wXlD+LS3n3bY9gc1tjU5dOd0pTRrs9HzgvNDMzTzKnNjc6z0GvR39JY1QDXo9k93FLeOeHx4/rmZOuC787zjfjB+8b+ZAKyBCsHBQqQC4sNlA+4EFQSeBQIFjwXThg0GI0YkhgYGGgYHhjtFzMY2Be3FyMYoBdNFxwXuxXqFJMUhxN7E+gTkBMWFNMTLBMYE4oS+xH3EZMROxE5Ec8QuBBFET4RiBGmEUIRYRHeEIMQWBAZEAEQCRD2D9MPPhAaEA4Q0Q9HD8AOLg4CDYcMuwvYCkcKNAlbCGsHIwbVBDkDiQG9/wf+0fv7+Yf4n/aw9MfyWfA27nTrpOi75aDiQ9823ErZMtf41b7U2NOo0jHRh8+fziXOgc5yz8PQOdJR0wbVAdfR2Hvb9t1y4BPj5+U26aTsuPAY9fj4jvzh/4MCwARPBzcJ+AoGDXUO2g/DEVYTzRQzFhAX3xcrGEwYYBgBGLUXihczFzoXQhf+Ft0WhRaZFcUUFxQnE8sSphJeEo0SUxLMEXgREhGZEHkQBhB9D9UO4w1LDe4M7wyYDesNHQ5iDjoOCg74DQ0OzA0pDkUO/Q1hDscOYA+8DzIQCxDrD4YPAg9pDvYNTw3DDO4LMAu4CsIJ7AilBxoGswTIAgoBUf/N/UD8dPrM+Oj2uPSX8rXvPe256YvmXeNi37rcYNpV2JPXRta91G/TmNEe0PjO885fz2bQV9Gt0kHUP9Wa197Zydvp3l3ha+OS5rXpWe1w8YD1dfnm/Pn/jgKnBLIGswhYCrsLWQ3uDmUQbxIpFAEVGhZwFmsWixZ0Fi4WEha0FU8VRxVAFVQVHxWMFPYT+xIQEpYRGRHuECgRChHcEIwQ8g9SD7wO2Q05DbEMzgskC4IKIQpXCosK8Ar4ChIL8gq1Cn4KUgq2CrIKYgunC+YLiQzxDKYN0Q1tDlsOYA51DkoOgQ6VDk8OKw64DU4NHA1tDMIL+QrLCewIzQeZBpUFeQTyAk8Bjf+c/Xz7bfm69kL0PPE57jLrvedE5AjhJN7W2yvaI9g11krUPtKU0PjPoM/ez0HQ6tDV0YHS7NPN1bTXX9r43Evft+GV5LPn+OoG797yJvac+S38gf7FAOwCegU7BzMJugrLC6ENcg/iENASEBQ/FbIVAhZ4FjkWmRaAFhIWVRZoFmYWoxbBFksWIxbKFU0V9xS6FK4UhBRTFOQTNBPJEm0SBBKEEe4QFhAdD8oOQQ70DRoOvQ1mDfUMpQxHDFQMbAx3DJ0MvAxwDM0Llww9DKoOHxLQFLcSEhLmE3YThxMpElISUhVJFjgUeRGzD+ENjgpdCVcIlwYxBdkBcfuN9r/20vQm8kbsT+Uc4WLagtaE1M/QMM/O0EfSVNVb1HbSBtCB0KzS/dNZ1jHX3do23LrhU+b26DDs8+uI7PntT/FT8wf4Of3WALsDwAQNBWsEhQTjBcAGZQigCEkHGQdHCDIKfQxeDngNgAoHCKkG2wYICeIKOAvDCx4LtwqQCT8JaglnCYQJvwjFCFoJjgw7D9sRPRIYEEwOqAxpDbAPIhIbFKkVDRUrFYYUjBNIFaMVrRanFoYVvRdCF94YrhfsFCcTlg8ID8wNlw7rDsUODg6gC98KMQkzCdYJBws0DSMOgQ++DysQARCeD1IOCA16DFEL8Ar6B4IEQQAr+1L2W/Aw7FHkG99n2czUBtH8ylnFsL4Et8awqK+MtWbHUNKP2J/S+8jizBzX7+rT8dD2fvWG+RMDlwjgDR0KUwn3BJgDaQOBBhINJBD5Dv4H2f8L+tL4Sflq+z/8K/3A/cn9xvvz+Ur6FPwD/2L//AAFA2EI9A7LEWARxw3VCY4HsgelCUkOWRFCEj4QjwzhCNgGTAZLBpcHnwgyCkIL8QtvDGsNlw3JDbELzguWDaoQkxcMG6gcExuIFlkUYxPOFU4X5BcUGOEVBhfSFWYUBhN1EOMQVg8MESUSKhSdFlAX3Rd9FhMVZBLzEjoUbxlNG8Ia9BXjD04MNwe8BA0BhP43+4v14O8I58zfeNpS0oHMLMYHwje/i7VerwWh2J/htAvLr+Pe3snI1bTntyzbofSABaP6uvFT9nQE8xErC8UGrf3r/yEGQgtaEAESbhElCB78N/JZ8HvyXvulAFIAcP2c9cDu0uzS8Xv4RP9BAKMBzQMkCXERFxPrEq4OYwruCLIM4BJsGzof+BreErsJhgUNBywLng8dE5oS4BCuDU8KSQj/CJ8JBwwzDQ0PwRGuEsUWjhTZEt4RRBEZFjMZJhwMGhMYFxV6EW0R0g/ID1YPtA+NEIoQ/RC5DY0KzAm6DGcSuxZ3FzsVQhRRFJUXhBbWF9gV2RRaFEYQpw2cCY4G2wEL+nDziOyC6F/mX9401/3NxMVowUS8f7oEtXOw/ad5po25Tc6x4bXeTsrnvZLCVeQ5/OsIPQLd+M37BAV8DeQI4gmKB28KVg7QDKEMFQtNCugCSPtA9zT42vkp/in+lPsr+hz1JO9S7AvyJ/qtBEcIWAVhAggD+QjqC4YPXw7NDAQO3RBrE/8U6BSOEZkNFwp5CccLmBDjE+MTvxDLCzgHoQZRCOYN3BEEFXoUWQ+2D40P2BPhGEUYoRgmFtcXtRmsGWcXNRKxD68OYg/VDk8PBRDEDo4NJAslCHQIJQsEDQsSlxNlFN0TJBG3EqoTsRdkFwIWQBRnE2gTyhARDFEHjwBV/+z7sfcB9vbsFehQ3srW09CBzZXMg8lJxsS5urFfpjamlrmn0zTr6+nz0zG4CLWL0m/2iBEUDqUATfVy9tIB1wJRCDsLZg/4EzsSZg22Bp0EUwHC/UX7ZPwi/SwB8QIXAPL6O/K166zoDPI7/m0JvgxhByL/Zvrc/qgEcg7vFOMXgRW9DzwKZwizC/4O2hAnD3oNQA3TDasOCg7bDJ4LCwnjCBkITw68FK0YzxlUEmANLAt9EDUahx/LIt4f3xg8FCIQaxCUFbUXUBfoEkcOrgsoC64MNQzhCiYKUQnSCTUMhhDxEzYUWBO+D1QQEhR1GBgc5huUGAIWahKgEH8Odw0YCxkJfAO++g/3wOyi613mU+Ct3cvTCtEfycbDDcEIvau2lbQ9sAK19cmM1+ve9tNzxAm+C8vN6Tr+5QiEAlT4LvJj8mb6FQHzDXcXnRWNDoAEIf7G/qwD5QjPCakIaQXN/aP6E/u1/AAAOf2D+AX2kPpNAjYHoQaxAsX/Vv8OBPIGJQwvEcUS2A/pCRkEZAMuCNQNchJgEs4O9QphB2oHeQwOD9YSJBHGDpsOqw8SE/YT9hNYFYwV2RW1F7IWZhgMGiga3BjcE48SeRF6FKsV6BPBEFQLKAm3BxUKmQ0bDwMPtgzOCNkIiAvOD88TjBQyFOoSXhKdEwcVvRVzFugW3hTdEvwOiwu9CjwHcgVX/1D6tvY88ufvC+kb4/Dcntkv14XTYdFiy7vHrsF8u7y4zrgLxBTWAOF23SzOtb07wEvSTu0D/xICvvlM603lUebb8mwCixDVFEINNgQ7+nP4Lv46BxsQdxP8DogFX/13+9f+FgSuB7AFKQK1ACgC/QNCB90HNgcGCNkHdAmJC7oP0hAIECAMUAh4BroHUgwXD40RIBDPDFAIdAZXCG4NiRPQFR0UaRDJDAUMTA6hEs4WiBj1F54V4RHDEK4RJhS9FncXRBbgEqgPPA2iDOYMHA1GDDwKIQk4CAUIlwhtCEkIngh/CVIKtwunDfkOuA/KD4gPOQ98EAATMhRGFDASAg66C0oJ5AgwCMoGxgPI/t75KfaC8nLwN+/36trndOPb3yLditqJ2a3WbdPF0MPO8M+s0wjWNtZa1UDUVNTi1l7bwd4C4zTmRuYs5x3nLuno7TPy6fdK+s/6wfoC+2f8+P/BBEsHKQjbB2cG3QX1B2kJ5QpNDGANHg0IDYINog30DkgRaBJcEg8S9w9WD3QP9A9SEPwOFw00CzQKQArNClgKRgqxCc8JQgq9CvcKzgpwCysMSg2ODioOWg7pDaYO6w9zEGwRZxE5EdcRzRF0EZ8QVA8ND6MOuA7gDkUNhwy9C1QL6wsTDfQNiQ6BD1sPbw80EM4QpRFjEtYS8RKKEkkSJRJ1EXwRQxFxEE8Pcg2kC1YKYAg/BzMFkwJl/wv8svhp9ZbyUu9h7K3o1uTg4Jjc4NhZ1cPRr87bzJnLFMpSylTKu8kcy03Mbs2tz4PS+9SN16TamNwD39Ph/uTc58fru+5U8mr1oPdR+un8Jv8uAp0EugWuB5sIHgkgCokLnQzcDd0PuRBCEUQSABOhE7oUZRVmFWwVZRVkFBwUmBO9EhUSTBFkEIsPRA+qDlcOXw5bDn4Ozw6MDuQOyw6SDvMOLA9JD+cPURAWENsPEhDMDvkOxBBPECEQ5RAiDzMNoAxgC6EK9gpRC7sKogqAChsKrwrTCxYNhg5SD6sPmw83D0AP/w+VEBIR0RG5EU4QlBCZEKcPFxDbD1YOKg22C+sIRwdNBUoDbwDY/Sn6PfYe8+Tu6etT6B/l6uHB3eTZRdYe0kHPNczvybHJdclzyZDKIMuHy/nN+s+X0qrWOdrG3AbgGeLU4xXnmOnL7TDxBfVU99X5yPsu/eL/UwJWBIsGtweGB1gIjgg0CcYKmQz0DfUOLhDEEF4RwBJBFG4VwhYqF9wWeBb2FUwVDxV8FNwTQBM+EoIRzRCBEDIQfxByEE0QcRAOEMsP6w/1DzMQIhAaEJkPOw9ND0APnQ8YEN8PYQ8LD1QOmg05DeQMRwylC+kK+QkZCeMI/AheCZsKJwt5CxcMUAzYDKwNjg5IDxAQWRBAEN4Pug8PEEoQwRDbEJIQ7Q/nDp4NdwxbC/cJfwiwBs0DWwEg/nH7UfhH9W/yMe8m7GjolOWw4VTeStuN1yfUqdC1zYTL2Mp8yyzLMcxszdPM+85/0YDTlteR28fdn+B94sTjv+aA6dHtNPHx9Oz2HPkE+7/8zv+dAvMEpAahB2MHyAekCOAJdQu2DdQOEw83EJUQUhEvE+gU8xUMF8wWGxabFSkV7xTdFLoUtBP0EtoRzhBsEHcQXhD7EFoR9xAWEd8QmRDwEGsRzBHHEfcRihE7ESoRyBAWEcQQlRC3D/YODw69DA0MvQovCgwJhAgKCF0HpwdlB8QHkggiCYIJgQrMCnALKQybDA0NqA1eDrcO6g4mD2gPeA+5D48P8w6ZDqENUAwjC18JmAeWBcEDmQAe/k373fdt9cTx3O4r7IHoS+XD4lTeuNvt2CjVr9JwzwrNdcs3y+fLDsw1zSnORs4s0DXSpNQj2JTbHt6S4CHj++Rl6JXrru9f8/P2WfkM/C3+QgBUA3EFiQcSCQMKLgqkCjoL4Qs8DfgOng8EEOcQEBGbEacSiBMkFN0UqhQZFJQTARNfEv0RrBHfEJIQJhCZD4APdw98D98PSRBMELsQ7BC9EPgQ1xDREJMQnxBsEGQQWhATEHMP/A4iDigN2gwGDGkLqQqqCV4IUwdzBu0F/wUrBnUGogbSBgMHMgfxB7MIjQlNCtcK/AolC5QLxAtIDNQMCg0UDbMMHgxeC94KGwpZCVQI0QYEBecCDgFn/pH8JvqN9xz12vFe73XsvOmm5ifkT+Ha3VXbIdiN1EzSec+UzTvNdcxbzPHM98w0zaTOGtB10pXVn9hk2xDe5eD84qfmA+rv7RPys/VO+ED7rf32/9cCbwVoBw0JSgqHCmILGQzKDPUNFA+QD88PgBDUEGYRKBLAEvoSGBO6Ek4S8hGcESIR2RB0EN8PtQ9nD4gPrA+tD+wPHxB9ENoQVRHBEQMSRRIsEkoSFRJAEkYSDxLSESwRYhCgD4gOow3NDKgLoQpWCSgIMQdLBtEFjgVbBXMFfgWLBQsGaQbWBpoH1AdYCLoI5AhDCXMJ8wkuCnsKwAraChUL7wrvCpkKWAq2CfkIIAjXBrcFSATfAugATv8x/Wn7Z/kk90n11fKR8PHtX+vP6ADmhuPe4LPdV9tb2BPW1NR80xLTJ9M400zT8dOW1NHVotez2cnbM96M4IHieuXi5x7rYe7x8eX0Dvj3+kz96/93AnMETwYMCCQJVwpACwIMjwx9DeoNSw4XD1QP1A9UEJsQ1BAEEc4QyRC4EGIQIhDMD4QPCQ/NDsEOrA7fDjgPUQ/DDw0QThCcEB0RZRGzEewR1BGgEZ4RYxE2EfkQexD7D04PWw6gDbsMDAw1CykKHwkRCPkGOAZfBcQEcAQTBLMDhAM3AykDOgNTA3cDtgP4Ax0EPQRRBIQEqQT8BDEFWAWXBZcFpQV1BYQFRQUeBboEEwSWA9UCDQJeAYgA2f/R/rH9ePwf++P5kPgv98D1IvSB8sfw9u4R7XXr0OkV6GLmceS84kbhF+Ab37jeVt7q3YbdTN003bbdjN6B3wLhTeKO4yrlYuZN6HXqs+wn73fxevOR9Xj3jPmQ+7P9vP9uASYDTwRzBasGzQf0COUJzwqbCxIMfwzrDEkNvA39DRMOFw78DbcNlg2BDUgNYg00DSIN9gzyDAsNaA2zDf0NTw5fDm0OhA5+DpwOog6RDjsOrQ07DbYMawwdDMMLWwucCt0J1AgvCHYH6AZIBogFlASOA3UCrQELAZEARwDa/2X/4/5h/g7+yv3H/eL94P3d/b/9qv23/c/9E/6H/sD+A/8U/wL/M/8v/2j/oP+a/8H/ov9u/3f/H/8K//3+qv5Y/uT9NP3T/C78ofsK+yv6efln+Fz3bfZm9aj0v/PT8gjyxfDo7+juLO7L7Ubt5+xk7PTrjetE60fra+vD62fsAe1z7Qbuuu6M75Lwu/HT8iz0PvWG9qf34vhB+nD7kvzC/bX+6v//AAACFQP8A/cElQV4BikHuAdnCMoIHglkCZIJnwm4Ca0JzAnECcEJ4QnXCcgJ1AmpCZwJhwlfCXEJYAlKCTQJ5giGCFwIBgjYB4AHVAchB4UGJwatBVAF/wSLBDkEygM4A5IC+QFiAQIBiwAIAKv/Mf/D/mP+B/7G/bj9iv1U/U39LP0Z/f/8/Pwf/TX9iv2l/bj99/1E/nD+nv4E/1P/lf/F/wIAQQBCAJAAsACtAP0AOQFPAWcBeAGLAWsBkAGbAX4BkwG0AWkBDwETAdgAmABxAGEAJQDD/5L/O//e/qv+gP4S/qX9Zv0B/XT8M/wB/Nj7ovt8+zr78frF+pr6ePpy+m76avp7+mn6g/qj+rn6+/oa+2f7nPvL+w/8Sfyo/AD9Qv2x/Q/+fP7k/kT/wv9QAMMAPgGuARoCegL8Aj4DjQMMBFoEnASwBB4FVQWMBeUFCAY9BlAGbAZpBmEGZgZcBiQG/gXoBacFewVSBRwF+ATEBH4EMATzA6YDMAPSAokCGAKtAWIB5gCLAFIA6v+i/2j/Tf8x/wP/8P6//pz+f/5T/i/+Jf4r/hr+E/4Z/hb+IP5A/lP+hf6r/sn+6P79/hv/OP9Y/57/yf///y8ATQB6ALUA5QANAVEBlAHEAdgB+AETAvwB+AH0AfEBAgIfAhwCMgJAAj4COAJGAjkCPQIVAu0BxgGaAa4BggGCAXsBXQEnAdEAjwA6AO//uv+b/07/Dv/n/p3+Xv4j/vj9BP7a/er99v3o/RD+F/4q/l3+dP6i/vH+C/81/2H/l//z/wIANwCAAKoA4ADwAPkAKgE3AVwBhAGZAdAB+gENAjICUgJvAp8CpALKAv4CFAMmAzcDTwNkA2cDYANVA0kDRgMkAx0DCAP4At0CrAJyAkQCIgLxAcoBewFmASUB8QCnAHQAVAAEANf/jv9c/yP/4/6k/lj+NP76/cL9lv1W/S39//zx/OX84fzz/Oj8/fwB/QP9E/0m/UP9YP1s/W39aP1d/Ur9P/1H/WD9av2P/cv99P03/lv+jv7E/uj+M/9r/67/BgBuALcAGQGFAewBWQKpAhcDawPLAxEETAR6BHcEgQRxBFgEMAQRBNIDoANrAykD/ALBAnkCSgINAsYBpAFfATsBEgHVAKQAVQBDACYACwDy/+X/1f+y/6n/nv+X/5T/hv+C/3z/Qf8l/7r+Uv7V/Xj9kP3k/eP9Gf1z/E78OPwF/Gj8LP3T/Tn+Yv6U/rj+8/4v/27/4v8cAEMAeQDXAEsBggHDAToChALNAgkDOgNjA3kDlwOtA+AD/QPeA9YDqwOUA2MDLQPxAp8CewI5AgsC4gGcAVEB7wB+AAoAnv85/9/+g/4v/u/9u/1z/Sv99fzC/Jv8b/xY/EP8RvxH/FP8bfyJ/KL8w/ze/Ab9M/1h/ZX90P0Q/lL+k/7G/gz/Q/+I/8P/+/8/AIUA0gAgAWcBuQECAlUCpgIIA1cDnwP9AzYEjgTKBBMFQgVlBZEFkwWdBZIFfwVtBVkFMAUIBdgEmgRRBAkEtgNlAxoDuAJeAgkCrgFsAQ4BzAB7ADgA8/+w/3b/Pf8P/+X+4P68/qz+j/55/nT+bP5u/nT+e/5n/oj+kv6z/sL+3/4G/zf/Xf+Y/8D/+P8yAGoArADhACoBXwGuAdsBGgI1AmwCkAKqAtQC8wLwAvoCAgMHAxYD/QIHA/wC/wLdAggDsALXAqgCkAJ1AmMCcwJUAncCbgK5AqsCPwPeAvwDZwOUBO8CIQTFB6QOdA7hCpcINgJG+gvwSvDz74vxefRt+UACjAPmAbL/+f2K+j/1cPQC+UD/4gB3/7sCcgB6/Z/3vvAJ8Szz5/ea+v7/vAiCDh4S6w6OC6wHgf9N9+P3/v6vBpMGuQRaCR0LVwWE/Wr6Bvpn+Bv1m/qoBtsMHw7HDZ8QwRADDN4FjALl/wH+Y/4q+0X5Qfcr+fr5EvmO+b/7af2j/nr+CQDjAID/UvwQ+lX6ZPqB+777qPtP/OT8wPyA/GL6XPqM+tf6E/sE+hz7Nv0C+r773f3lBdkEIAZEDVoUARYWChkIKgsVDFAGmf/q+0z6ovkj+Mr6IfhM+cb10vPy8nLzxfl6/R79Hf1DBFYFKwQu/8f9bv88/Tn+//6EAOEEGAMiBKkD/gL5APD9Ev/f/f//+v0/Afv9IAEXAaICef9s+5EBmgoGDdcH4waRA+D+vvjN97X4QvvY+H/6rftUAVsC9ACB/nj7r/ku9Obz5fL+9Oz27/mV/Af+SfyB/lz8ePz7/B0DcwoMDioQ6xUdG14bLRV+CnEGwv/a/cH6BgC0A4IHGAq/BjgEE/9j+lb0q/Or9E/1CffL+ST98v3h/I36+Pl9+Gb3s/fO+YH8t/49Ae8CLgPAAvABPAEUAAj/bf9TATUDJgOAAwoD5wLJAdQAz/+P/i/+h/3C/dz9Bv45/3gATv8g/rL8RPzO+wr77fqU+8H8M/09/fD9WP7u/Uv9X/wY/Df85/zV/Dn+Ff+UAG0AZQBN/4D+t/49/T/+9/yg/n3+L/88/iL+RP4S/R/9c/va/Ez8qvwt/Rj9HP66/TT9rfye+zH8Wfzt/AX+/vyD/jD/eP94/yD/Bf+e//7/IACvAMYAZwHaAcwCsgLXAnkCoQFSAfIAJQHEACYBrAEvAjIBSQDw/wwAbAAF/6r+nf6g/pn+uP7p/n/+8P0l/pj+Tv6H/gz/Hv/g/ov+gP43/kr9U/3F/XX92PxM/KX8Rv3C+xH60/m/+R36jvpW+yP85PsK/OP6X/rd+Vr5bfq2+9r7ZfwJ/UX8G/yc+636x/ti/Wz9tvzg/KL9d/5u/Zn8K/6C/kz/ev4z/gAAVABLAvMCMQMFA44DuAOLAxkDIQQxBl8HVgecBv0G8wZSCC0IDAh0CPAHKggNCN0HzQe8BysIcQiOCNUGdAY2BtcFegUFBdQEKAQ8BLMC+wKDAUcBRAFzAKMAgv8U/+b9Yv1n/F37Tvkt+Cn3VvZE9Sj0afOG8ubw/O8u7oTsyOzs6+nrJuq56Krn5+eg5qjmUue86EPq7+nN6vLrPO4n77zwkPKg9Qv45vku/PX9JgBmApkEmAbNCHAK9QvFDUkPXRCPEkgTRBU+FW0VgBY4FuQWaBdCGIsZ9Bk7GSUZhRgjGN0XthcgGOYYehkKGRoY7BZyFEkTcRHXD/wOHA6jDSANPAsJCXgHbgTcASf+z/r6+X/4P/dt9eDyRvAA7OrmrOIM3uHbKNlU2WHXuNUg01TREM4TzIDKrsjyyYXJQsya0MfW+d0B5KTmm+r26ufrZO0Q7wnzVPpjAfcGKwwDDQAOMgxhCwgJPgoXDHwOAhEtEQgSJREVERcP5Q2qDIYLrQpLCbgITAlRCZsLKA1RD0wR/BHKEf8Q8xCHEqAVsRhqHXsfhSIKJVskCyN9IH8euR3NHggfLSDyISMiECI3HwYc0hm6F4gVzxNKElISvRIWEj4Qig2pC4AIqQW+AKD8lPgX9uXykO8s7bvpnedN43XhatyL2RfVJtE7zEbImcXcxF3FycaFyJPK3s6z04HZwdzF4bHk9ekr7ozyp/Xf+W7/WAQ8CtQMCQ8+Dz4Rtw9ADuoNrA1XD9oO3w1+DJkK5AhhBo8DygFMAQcAZ/9+/ln8VPwu/Dz9Q/5MAWkEBwbeB4sHaAiHCaEKHg0YD6YT/hdQGrUczhxvHmMdoR7QHakezSITJYMmDiT1JI8hjiEoHloaFxqVGtYbDRrDGKsWPxcRFlQUNRKwEOIQ1A/lDCEKOQkZCUgJOQjCBgoF8QLoAGT8gfhZ9G3xxu507JHozuTg4Xnbzdiz1L/MgsqRxRTEGcNdwWfBIMV+yezN0tIJ1a7b6uDN5YLqne6g8s74ef4sA1gI4wzYD/sQPhK2EPgQ1xBvEE0QpA+AD7AOSwz9CsgHQwYNBOMBcgD7/YT9d/ul++/62fwb/tQAUAOIA/8EeQV0Bl4IrAlVC5kNpRHlFEAXfRrCGSEdrBwRHrId2R5ZIZohHiafImEkNSKmIE4fvxvHGooZlBojGioajRm8GEsYUxdZFAkTRBHZEEwQ8A+mDzYPvg9RDhYNigr8CCcH2wUyAy8ARv3U+UD24/Go7+Lq2ujg4+HfHNpI03nMi8V/wFu+oLwBvVK7+7qdvpHCK8Zjy5DQ89dO463p0e5a8nD2wvv6AroGKQtvETMWkhqMGUgYUBanFXUVzhIMEXUPSA56DW8JtgUiAl0A3v3y+zf5ofbh9kv1u/XS9eT3IPrM/QMARAGfAiQEFwVKBwgJ2QkKDlMReBV9GDUazxkPHOkaoRuxGlcaQx1oHsYifyF0IqQhLSDZHZkZ9hYhFuwW/BfRFw8Y6Re2F8QW4xLjEMoO8Q6yDtUO2w64D8wQzA9+DnELyQldCL8GtQT4Acz/QfzI+O/y3u6L6bbk0OBH3G3XgNK6y7jFR8DSu0W3CbZBswO0eLjavp7Fqc1d1AfcKeUn6jrvNvKV96n8AAVZCnoQyxbAGmMeIB1GGwwYPRazFIkSdxDxDrQNHwyYCBoFeAFd/kn7xPib9ZrzivKR8RDyCfLl9M722vrs/JD+UgC2AQkD7wSmBm4IAwzXDukTOxb/Gc8aJxwiHesb2hv/GisbihzrHvAeyCBZIP4f5x7WG0gZwBaIFQAVIxU8FeoU6BV2Fe0UBhPgEMoPKA7tDacMUAxkDKEMWwxhC1gK8wgkCLoFmAKP///7SvjX88LvL+ut5+ni+d5J2afTWc2DxxfCcr3FuUC4DrQ7s2u1s7rfwdbKQ9K22jTk4epJ8fjyPfd1+pQBVAj4DoIVMRtpIM8gdCCuHAAZbBfSFOkSGxFZD3oOHQx8CTkFaAF0/cL6OPfQ9Nby7/CY8Q7xLfN79Gv3o/m6+879Mv98ALgBdAJyBPoF3QkQDgQS1BYsGHQaRBkHGRgXIxe1F6saFB9uIjgm3CaXJ7EjhiAjG9AXHha1FSQXuRgKGgEbrxr7F5sUsxBLDt8LxQsAC2EM5g1HDywQAA8pDtcLiwp1B8wETAIkAO/9ofvn+GD1evJF7Vzpv+Lm3CvV3M8gyhTE8L+rvE25Lrg/tXi1Jrkdv8TGUs//1vfdhefq7F/ybPQn+Db8mQNnCjkQuBbvGqkfOCAdH34cexngF4MVthOcEooRDhG6DwsNQgm7BZsByf3n+Zb2sfN38i7y9vHw86314/e9+pr85P0Y/2n+I/8H/nQAQQLDBfQKng4cFDwXIhpUGjwbvBmQGiUbtBxvH/QhmyXhJcImRSQfIkAe7xqTFw4WEhU9FfEVsRVxFQMUpRJxD1cNUgraCBoIQAivCJ0JBQrICeYJFgg0BzAFbQOsAdn/fP6g/Fz6NPfe9DDxPO2m6EnkSt/l2kDWINEJzT7JUMQzwYa8tLkvu1W/TsRWzEPTDttv4+znEOwp7MnugfCO9db6lAFWCZwQ8RePGgUdzxvkGRwYkBUrFC0ThxMGFCEU+xJwEVMOSgqCBswBOf7++qn4ePbK9WH1m/Ud99L3Afkd+nf7u/ur/OL78fsA/Jv9BABIA10HRQusD/YSfBUpFggXGRYOF+EXmxgdG+wcACBoIUIiTSHSH/QdXxtdGZMX1BUbFusVMRYuFbITGBKBD3cNkApxCOUGOwanBXcF5QTJBLYE9gMTA8gB5wBy/xv+Kvw8+lD45PX28zbxJe696rTnO+TP3wDcP9gg1HvQ1Mvqx5nFaMXaxn/K5s4M1ADaIt8W40fkGOX25FjmLOjX64nw1/ZD/ooE9wk6DTkPJBDyD5kPeQ/UD/4QdhLVE7UUzBRVFOwSAhGwDr8LJAksBpoDAAFp/9T9I/3v/Mj8af1a/RT+8vy6/Pn6EfpT+SH5DPqH+1j+WwFtBf8HaAs5DXIPthCzERMTzBMIFpUXrxmxGrob6xs4HD0b8Bn7F1AWlhQYE/kRZhBlDzEOaA0RDPwKVQn/B98GUAVbBCsDPgIHAo0BQQHRAFMANQDu/4z/E/9I/ln96fuO+uX4b/aE9DPyavDh7V3soupM6VDn1eQ/4qPfm97C3VDe895Z4KXiHeX55q7nT+cK593m2eYp53znIel965TupvGD9Hj3gvro/BT/xQBYAs4DQwWHBpIHuggRCkALkwxwDW0OIA9DD1IPUg6lDRQMwwqBCUYIugcZBwUH0Qb+BokGTAZNBYwEmwPmAj0C/gFEAtACSASaBWkHwghxCsgLAQ3ADXYO5g79DoYPAw+ADwAPJw8KD8IOpA44DiQOtw0+DVcMmguDCooJowjKB24HEgcSBx0HRAeQB58HWQfjBisGsQUeBYwEIQSrA6gDfwNpAxsDrQICAlEBTQAg/9n9dPxI+2v5Xvi59pD1/fPH8izxl+9I7vrsL+w769fqeuqr6oDqE+pc6bHo4OdT543mGuY/5rDmx+fx6I3qPexe7kHwL/IK9Of1ovd3+ST7t/xk/vv/aQHSAhgETgWUBrsHowh1CUcKrgoZCyELDQvKCoMKTAolClsKXgq/Cu0KNgsuCzcL+wreCsMKowrbChsLwQs5DAENow0eDn8OrA65DowOVQ7jDbcNSA0cDbYMaQwcDOcLwAuRC10LHwvnCooKBgpgCZ4I0AcgB4MG2AV8BRMF4QSaBEsEKgTDAzYDdwKuAQ8BWQC9/y3/jP4L/p39//yM/Nf7DPsr+hr54PeP9k31GvSe8oTxMfAQ7/7t8ewG7GfrGuvO6vLq1urI6qLqa+r26YnpCOnV6A3plOlx6pfrH+2g7nHwGvLI84b1R/fk+IH6AvxW/av+9v/3AAcC9wLeA+ME+wUEB+wH4whuCdAJ8wkCCtkJswmcCZ8J2wn/CVgKlQrtCgoLPgtHC0gLQgslCxkLCAsUCw0LQwtlC5sLwgsDDC0MGQwKDLALYQvSClYKtAlHCckIfghbCE0IOggtCBUI2AeOBxoHkwYBBm8F7wRdBP0DpQN1AxgDygJzAh4CwQE+AdEAUgDY/z//o/4Z/oj9+Pxf/L/7Kvue+ub5Uvl3+L/37fbw9e/03/Ph8vzxNfGx8Dfw0O+b7zXv7u6d7gfumu1H7e3st+y17LjsFu147fbtne5174XwlPHN8vvzP/We9un3EflA+kn7WPyD/ZH+uf/pAAoCBQMSBNkEeAUoBqAGAgdwB90HKAh/CLUI1AgbCToJYQmbCewJGQpfCocKhwqSCm4KTgorCh0K8wkPCjQKRQpwCpwKsAqoCp8KZgorCsQJewkPCdYIlwhrCFEIOQgsCCMIJwgICAII3AelB2wHIQfUBnAGJga+BW0FBAWxBGgEAwShAy0D2QJyAgICowE9AcsANwDS/1D/vv4O/oL92fww/Iv75PpC+p/5BflB+Jz36PYb9kP1mvQC9G/zH/PM8ovyRPIM8sLxdfEz8fLwwfDL8MHw2PDw8Ejxq/EY8uLynfOq9MD11fbW99r41Pmu+on7cfxe/T3+S/8bACABFgL2AsADhAQ/Be4FvAZXB+4HdQjGCB0JXgmICZgJ1wkMCkIKkArCCgQLNAtnC3ILjgtqC04LYAtLCygLEAstCz8LUgtcC20LUwtJCyML6gqICjAK3wl+CR0JwgiWCEYIKAj6B9MHvgdwByMHnAY5BpIF8gSLBBUEjQMwA9sCbAIkAhMCzAFLAR8B6QCpAGIA3/+C/wX/l/4s/rj9W/3d/Fb85Ptp+wj7efr5+Yj56fgn+KT3Fvdb9vz1m/VX9SP1AvXo9K/0mvRN9A/0zfO686PzqvPj8yf0a/S69FL19vXN9pb3f/hN+Rz62/pr+yD80Pxo/Qv+6v64/6wAaQFLAggDsQN7BCUFpwUrBsoGIgd+B7sH2wf5BzoIdwiQCN0IQAltCbgJ1QnrCcsJyAm0CXgJQAkECf0IwQi3CNgI5QjPCAIJ/wjmCNgItQiBCEMIDQjRB5YHbAdIBxgH/QaxBooGUwb1BbYFMQXdBHIEBwSxA2oDXwP8AqcCZAIfAvEBkwH9AKsAZQDe/4D/Lf/3/o3+O/7N/Wj9Gf2U/AH8q/sw+7H6Dvqj+U75xPhq+Ab4p/df9y33zvaL9kj27vWH9Tf18fSP9Fr0MPQj9DP0S/SA9NH0D/Vq9dP1G/aI9vX2b/fW92T4DPmu+Y36QfsI/Pj8vv2A/kv/6/+gAFYB0QFVAvYCeQMDBIsE7ARwBfQFbwa3Bj8HeQe4B/kHFggjCFgIWwg/CJwIugi5CPIIAAkICdcIzAimCJsIcQgGCBwI5QfRB48HeQdgByMH8QaWBmQG/wWgBU4F+ARjBEUE6wN+Az8DGwPkAnUCOAK8AZ0BRAHzAKUAcAAZAOT/nf9i/xv/4f6I/iL+7v2G/V39Cv22/G/8Nvzl+4v7WPvw+pf6Lfrs+Zf5Ivnp+Ij4dfg5+AL45/ex9373UPcp9x/3Bvcd9zb3P/dX91n3a/eO9/P3Q/jC+Cn5uflO+sf6VPva+3L8Ff3D/Wn+Af+r/1AA5ACmATgCwgI9A9IDqwP6AzgETgVcBtgIwwm8ClsMlwwxDQMNQQ1bDQsMWQsLC38KHgrwCTEK7AozC+sL4guuC/4LcgrJCY0IiQcXB1gGGAZhBoMGFgecBlwGSQaxBc8EvAPqA5gD/gLyAWMCGQJKAkUBjQD4/6D/Sf+K/h//N/7w/bD9Ev18+xj79vog+tL5F/m9+Cf4t/e091b3bffP9nz2v/aA9uP1h/US9T71bfVF9Yv1S/Wl9UD13vUx9q/2NPf09jf3QPfd9wL4C/iI+GT4NflV+f/5JvtS+zX80P2o/nL/eACJAPsApwFMAtcCZQNhBAAFGwVgBjMHXgiWCSIKEguHC+0LYAwTDR8NeQ3qDHgNqA1RDYgMXQ2FDSkOpg4ZDkoNTwxwCuEI0gdiBjgFkgRuBO4CuwPZAT8CdQE1AJ8AMv68/8b+0v/J/4r9zv04/Iv7cPuO+yr9q/3B/aD8If06/Qv7vvq0+jf7h/p9+bn5PfqU+4f7Wfqr+nP5x/ho+e35LftY/Gz9kfws/DD7iPrI+nP6wvv1/Ej+df+N/p/+av6y/Tr9bv1w/goAdwGEAuwCVwKlAnwBsf9V/x3/DgALAU4B6QDr/0//cv8m/u/9Kf2f/Zr9gf2Y/YL8hPyw+rz54vjc94f44PiB+cT6mPsT/Cr7f/kZ+Mv2ZvV19GT1m/WZ9jb3zvfW9wr3U/bL9NjzUfPF8p3zxPSp9P71RfYn9+z34vhX+h373/y0/Qf+mf4z//7/vgDTAewCJQSTBUgGWQe2B7oHjweTB3IHbQdtB00H4gdjCNgIGQp9ChgLLAseCrsIjwa+BdAGWwb/Br0HawgOCo8JcgnoCGsHCAfhBa0GbgWCBBYEkgSJBHAEPgQOBJkEcgMFA4sA3v9UAMX/kv5Y/bn7PvpY+ir5Evg890b3GvZt9ub0/vHN7m3uSO0m7Z7tdu2C7UTszeu/6bLpZegZ6KroMucK5nblEuaa58rpxevZ7sXxbvML9Kr07/NQ8/zyV/Mu9DT2Mvho+lH9t/9OAY8CXwK3AVcBVgBRALQAPgFDAhwDRAQ5BU8GhQYjBpoFvgTUA18DPAMFA0sEbwWABuQHfQcjCPQHTAh8CHsJfAqECqsL5Av6CysMKwywDCkNgg0WDfgMDA3HDeANnA2VDtwNKA4PD44PxQ8iD2MPzQ61Dl8OKQ62DawNzgwpDH8LUgqOByMGqwP3AesA1/7I/2P9NPsz+TL2MPWX8jnx1+9H7x/ufOyW6tLovOV149HhHeET4QLgpN8w38TeFN5n3YXdi9/u4vzmVOvi7rXxvPMh9kf3Cvei91j4EPoQ/Zv/9QG0BA8HDghwCAoHYAbcBYAFfwUDBR0FJwXVBegFjgVQBekEVAStA7YC1wH9Ac0CbQOtBH0FywZTCI0JHgpdCq0KSAtkDPkMOA7EDgQRuhI5FLEVshUvFiYVUhRIE7wSLBL9EuwTnxRvFfMVWBYzFqwV2BSUFDwUWBTrE+UTtxM4E8oSDxE6DzgN1wvyCSkHHgTfANz9lvvO+hv6uPm9+Bb4efRx7pXotOLK3vTcF9vt2x/a2NjO1WLSq8/mzNfM6cxyzrPSCdbW2kDfVeQh6a3ukvRt9737NPxh/mQASQMzBnUJXg0BEEkS1BBfD1oMewpBCRsI4QYZBscFoAVFBCQCBAB5/r/9Nv24+4b68fow+xn9JP4cAGsBRwRiBsMHnQj/CJUKOgxuDxoRZRQ4FyQbdB0HIFYglCAeIVUgNSBvHtUd5RxPHQQcVRvLGQYYZRZfFD0SeRArD+kNtg0QDZgN+Q3oDuwOZA7VDVQMkApfCCUGDQRZA0gC2QFbAA7/gf34+pT34vHs7ITn4+Jv3jTbAdcP1WXRPs/xygPHdsLLwMm+Fb7kv6bD2MvG1v7g6emn8TP1FfuA/BD///4xAqQGSg1eFA0YRBsjGnIathW1EQwNwQgIB9oEVANoAfn/MP9g/vX8y/q0+NP2CfUZ843xFPF78ij2+vl2/4UDxgi1C4gN1A12DAcN4w0pEKQTQxceG+4euCDHIT0fFh7qGuIZZBm5GEka6xnaG5obExtqGcEWOxWwExsSWhFzD1gQWBAJEQQSpxAcEUUQHxDjDrkNWgz2ChUKLwlTCDQHdwfbBmkGYgSbAJH8+PVD8FvoSOMd3s/Z+9YH0/LOEsp2w86+yLjVtTazmbQztl+6usI7zavZWeYH8IT2A/4RAUwGVQjyC9EP9xY6H8kk7ieHJuoj+x0kGKIP8Qh/BJUBcv/t/IL5ovZ48+vwYO3A6fXnquYW5/bmj+jk6rTvjvWq+2ABDAfsCxkQtxGFEu4SYhS+F8wb7x8VIxsmxCWvJbchmx6aGasYyBfhGLEaHBp7GwoXOBafEVoObgsbCQ4KUwtDDsEPOBHJEjcTTRTgFNMTThRqFEAWVRcvGP8YmRisGJoXmxWyEUYORgp7B94Dfv7d+KHxguvc4xPeo9Zf0e3KMMaQv+e3v7ABq2iosaj+qaCuS7AatzfCGNCm34Puv/qfAwkPlRKoF7AXEBpKH2AnWzB/MyQ0LC+nKPUdMxNDB2T/b/sS+XH2i/IR7qDpXOWq4JLbY9hf2L/Zq9z+3ozjCeqF8+r8CAUHDSoTlxiIHK0cEB52Hb4gMyVOKZ4toC1cLiEpQiRRGnkTBg0BDMoMTRCTE5sTThXoDxoOwAcdBVYDQANfBwULbBAUFZwXBBsVHGgezR6sHZEdExxmHhofoCFoIvwi7SJvIcYdCRckEIkJiwUmA20Ai/1V+Qnzcezc4wrauNG4yM7EL8D4vK+35LNDr9er2KnhqderdrHjtRq+Bspm16fnFvjcBswQQRwxIVEnGydSKHgoKisYMbIvhC+wJyIhwxYDDuACZfjN8KrpBua+4THeu9p+2snYB9iW1jzWx9e622bg+eXH7XP4XgQFDy0YGB5JIqkm3SecJ5YnySfUKoctoi/gLLQp8iPIHSEWGg6wB5gF2AQeB5EGpgOgBNAA/wIwABz/gwEiA0gKjg17Ep8X8huxIh0lTSgiKF8pIyslKucqQylsKecolifvI0weGRhREW4KaQNX/bD4vvUZ89btxejJ4jbcLdYE0T/KU8WawYi+K720ux25+Ll3uEi5v7d2t5W2a7gswHbQYuTz+MkNRRflH3QhaiMZIMEgMSBWI9EuBjITM48qwx8LEicINf2F8rTqIuVs4lDgdd0l2D7V/9U418vWhNdF1sDYCN5t5Zvt3vnyB84UliG3KG8onCiZJzImVifUJ/MpHi3nL4Is/CenHKAS0gcfAKb5cfbl9YP5AP0U/Ff9z/d0+2L6D/71AM8BaAkWDtwXwR2JIx8pHC0kMxAy3DBaLbwqpCw7LVcuhyxBKfckpx9HGBkQTgjeArAAcP6D+4n3kfMA8Hjs6ugB4nDe6Ndu1FzTmsyYzNPHT8ZPxS6+6LtitY60fLMXtle60r5fxaHPA96H7dj8PAoDFWEZGCGiIRclZiU+JZUnAyjPKtAjtx7DEuoHt/xu8ofpyuBa3NjXJNYR06PQjM960dPSA9XB18/aOeDt5yPxvfrNBmEUCSBvK4EvUTCKMPMuyS4YLbYreSkpKLwk7R9TFjAMrwHA+pHzUe726Qro3eol7ALwA/LO9NL5AADSBVkNrBHvGbchDyljL/EyAjbMNlg3zTQYMsounyt4KbomhSIQHt0XyxI8Dp4Kmgd+BYMDWwGC/wb9/voH+KH1kPM68pnwTu8+7L/pI+e74mHhUt4I2rnVptB0zIbHT8IjwVC/6MAOwCm/RrsntsOybcPg4bD8fhrKJB0iDR5nHYwW/hUMGGsbnS12OCU08CNyEhEBZPme86bql+CS2MjVDtX21arSudLv2AveROCf3WjZeNg/4CHvm/5xEGYhpCyMMz81OS1xKG0nhSfBKiAsWSrIJ+wjUBmTD7ICFPnE8cTtFufj4qDgteJy6lfvKfay+igCYQh2DiIQpBIIGNMiMTGlPBxB5UP6Pus9XjhhLygqziKYJEMktSSUHdAUmgtsBVsCcf5W/Fr6k/sG/8QBhALOBK0ExAZxB8kEGAEh/qv7vPoY+Sr2g/Qr8uTs6+MM2rvOmcgPvyC3cLJCrb2wsbR1tRi4GLjkujXGWtY56KH9LhBYGuMjDidFKywu6jGPM+QydDRnLQAkeBeVCtX9ufWf7NXgCtgRz5LJHsh6yBvJ0s0F1MXXj9wP3wfhQOk+9RMD4BHkIfItezgrP0o/eTwsOmo3izRnMBYrqiIBHBsVKwmb/wbzKekh4ODaStUw09nVydov5J3rV/JY+V0CGQuPF7wfCSr9NAc9ZUZmSxVLIktCRnlCLTxwNFgt3STUHgoXZxFECdwDov3b+DX2c/RB9d33mPtl/6kEggnpDo4SjBRDFJITyBI0EwAStA+VDJ8FtP6Q9tLs1uNi3KDR8MxIxZ684LWTrRmsj6zFsNiyMLPJrkuqTKp7t2rdqwQbJ1A96TbzKu0mjyW2JogsrTDiNXhAHziHI14M2PVs6mrotONP19DMSsX8xVbI5Mu5z3PU7dzG38vgj98e4nTt8wBhFOAhpjASOKY64DuWNjMwBDG/Ll8s+iexH4IXUxJCC9X+4fRa5x3h5tmP1KzRbdHo2JDmYPF+97H7b/yCBMsMche4IZUtmDgyQgVJ9koHSv9H5UFHPcMxTSliItQZ2xj1EUQPMAlmAnb5S/KC7ZTsi/KC+EQAhQVkCmIQcRW4F6QavBj2GJEa9hn7GTAXoxImDc4HOv2V9UvqIN4e1hTN8sVLv1yzBKaGoBqbG6C9pGelMKhNrBiyU71ozTziDP9MFyErajSZMiQ01zn1PXFCMUB0OVYz2iv/HcUMb/n05zHeZ9bjzInCCbscuEi6Gr/BwmjH080x1jzdq+Ri7vf5sQt5Hq8tJjliQUREXkU3RaBAtzv1OL8w5SoTIWITBgmC/fHyX+bo29XQasocxr3FiMltz4nacuX67jD3Y/7dBZgRxhxzKn8570PvTUlTg1QaVTVNIEfDPFM0Wiw2I/0cjRIYDfoEW//E+AHwNutp6a7s8vIt+gcBWAi3Dm8VfxvWHg0hmiI4I84l7CS/IQQfkhdtEiMMnQRE+fjtYN8316DSP8kLxWu6J7SxrkOpBqUMowujAqkDs4y4JLtTvFG8O8vL7zQWUTnvS75FwTZPL14vTjBUMuAxBi4lL98owxOf/IXnM9gd1SvTb8Zdu8K1k7dawcTOXNbl2l7gKN7I4OfnrPPgCd8gKDIEO31An0LNQPs9kDnvMs8xmy6xJ2kcihFLBysAD/uS64/f2NR7zCjKhcvpzQnVXuEq7/f6DAKBAw4HJg3aFhQlozPmQsFNz1VrVa5VekgIQJ02TCsBJlUdGxseFGwQjgilAvX7jfNm7aPoGOp68Br7CAkSFUoaOR9pH0wfCCDuIAkiHyMdJDUlvCRlHscVTQuPBEf96PiJ7Cng7tEnxo/FX75rvgW3eLD3ql+i55wFmsOfLK0EvNDGrMp4ztLgb/z5GrAyRDxGO5U3Nzd9Ngo3NTeRM6cxfSsrG5oDnO9p3tzVUNKUytXBkbqMt0S6aMIbyXfPaNeh3XHiu+m+9O4COxXgJ+ozDTu3P1xAZEGhQ7Y9tDh+NBUs7yLzF4wKHf889/PsXuW628XRMMtYyjvNDtLz2ErhAuzy9E3+SQamDdsWVSJkL9Q9+kXjTaZPNFA3S8RCmjpkLr8nvx9+GxIWjw5pBwAAifmF8qbtRuvC7KTy/vuxBvQPFBf/G/AfZSRHJ4oofimhKfYoyCidJp4gIRrnDw4HnPya8mroUt9g2nrQ88niwQu9mrc8sRSvi6zPq7Wt6LButeO5mL3fv7zAb719wqPmxxbUO3VSmke8LA0e6CArKKUvIzEzKlIsEymrFd75XOSn1bbS89Qnyjm8cLWPuJbFlNZ23fzeTeGp3AvdU+X49K0KQiLiMjM4sTs8O5A3HjXBNDgx/DBAMH0n0h5GFPQJlgJE/BDuFOCS0g3KD8p5zjjWgNyv5KHquu+Q8/T1YPpKBGMTSCSsL9I1XDfKPRVKaE+2UmBHdzjzMBErbyhiHq0WcBB1DC8H7/kV73/neekp8SX4RAAqBTwJDQ4VEKQRAhZlHM8gySWMKSMq1ynRJzskRyDXGe0QZQfp/x77MfqW84HsWeCO01PL875HtvetPKtUrPGvCLJtsJuv1K39rs60prs5x/rTkuam/RUSfyJvLacwHjB2MAAvNTAmMFMwli3lKYccVAuY+n/pzt5A11DR/cr9xn/DicSQxhnK3c4K0/DXC9ta4QDrY/eBCLkaEygeMls4EDqrOgY6fTcmNUw1iTHaLSkm9RpdD3MDYPh/63LgmNYr0j7Rs9I91dTX4dxC4jjpUPBK9zgAPQpcFW4gaCu8NIU710CIQZxDukHDPGU2oS6wKKElaiF1HREXGg+5CNwA1PuR9mj1XvZ7+10BZwZuCt0MHBAXFCoYCxxMHxMgoyHmIiAkWyN7ILkbRBaiECAKQwQw/LPzMetX5JXejdbLzQ3IE8FNv1q+ZruQuImy9rLAsh+2k7mWvGG/FsCxxPzYsv99Ilo6Rj5bLCQbpxiyH+0nDC96MoAyQDEvIjkLdfWa5Nfc69op2dXP68srzDPPcNRm10PYmdbG1eXS6tgs51n6oRBuIQYpzCj4KVcoGSgYK6YudDP+OAg3IDBGJkYalxD8B+z+wfLe6Z3gIN7a3rLf2eHb4efiQOJK43XllOjm8Cf9ggz0GYUjFSQUInEgSiQ4L584Z0CVPuhAIz5pNfcq0x7LGtYXyRT8C/sCQP1o/fT++v+0AI39K/sx+zH7UP9qB10Q0xgQH6ginSIbIJ8dkB60H4AfUR1UGPUUtxE/DdQCnPf26RXhWdo/0W/NZ8gNxkDD4LmqsMinYaKVpZ2q2rImt2a++MNXyFvVEek6A8UZ2icuKBoj4iFyJQMtNTW5NyA26i8lJ7UWLAUr9lXsiem/5OPfVdd90sHODM5wzzbOjM6TzfjPatO/3ADqtPqQCkAUbhnFG0wesx+3JOsqLzIQOOw6UzfHMHQmMh0yF1gQRQlg/5L4bPEC7nHp2eWA41nhwOGc4PXfN9/F4jbqQ/Wm/9wIrw7ZEtUUihioHlMmXi+HNl87VzwDOKQxbSr+JD8i4h+vHVwX6xHeC1EIbQbNBFoDhQHm/9v9+/3TAP8FnAv+D4kSYBNSEhoSjxI4FCAWVRfGFtQTMg/wCKQDVv28957xQ+xs5orghtpr1oHRxMtaxUzB777kvPm9vLsKvBu5KrgEuSK+X8+L6AkDjxL8FI8MCgZqB0UOeBpaJg0u0zLyL3MmQBdlCGj/kPts+r73qPRX78nsa+mx5c7iGt452q7UbdN31ZDdY+qy9mv/XwIeA6ECVQMZBs0MjxW+IZspUCxTKvwkkSCNHCQZ4RO/EEcN4guDCLEEBwC9+6v4VvXc8RntbOvh6hPuVvJc9zX8vP68/tP9svwn/r4D0gzvGVwnhzJTNQw0siqeH/QaqhvPIWYl4CfOKAEouCSvHPQRKgkPBu0GigmrCnYL3g0qD9UNQAsQCcYHrQdZBp4GrgflCbQMuAtcCFMDUv4K+hT1+fDH7fPtffCW7jTrB+Qm3WDWMdJFzrfJJMxxzcPOv896zAjLAs7D1Pbb2OL16VnuJ/P49qT8ogFqCGsPAxJAFV4TnRGcDlANJguECYkKGAiUBlACG/6o+jD37/Us8wDxEO/K7QTt4+0v76vwk/NZ9Ub3L/kK+6L9YwEDBbwJIA2DD8gQaRG+EZsSWhOXEuMSaRKXEv8Qvw7FC48JJgjqBtIEjwJdAcoAjQHJASYCNgLIAqoCMgLfAqsGnQukEbkUPBVHFCETpxLgEvMTxhWcGBUbJRzAG80avRmIGcgY5BjxF6oX3RfBF6cXKxdmFpcU8hHADRQKyQaRBZMF1AV/BZUEUwJe/9/7D/ix9avz4fJE8JPu8epj6OrkDeNH4PPditwL2vHXs9SS1NDTatRO1A/Uh9SL1+vdSuQs6vLtau/l8M/y5/U++oL/JgVqCfkLagulCfcGnAYkB7AH6giQCPcHtAVkA7AAZf6//Tz8FvuZ+DT3lvaT9n73efcy+MD4J/l/+bT5fvrA/Jv/CAMXBTwGCQc2B1YHhwfgByYIYQmpCYUK/AlyCVII8wcNCLUH/Qe5B8wHHwgHCYIJkwlOCWkKQQtHC8sKXQkyCuIMHg1nDUIMIA3tDmMPQg4aDMoN+w8LEpIRfhFGEucT7xTrEgcSgxLCE5IUlBOsEiwSGBIXEbIO8gyOC6sKJQl5B0gFPANfAfv+Jv3E+tf4cfVu8sHvbe2+6//pQOiG50XljuIV3wDcPtth2g3bQty53Mjb69uO3mXiqeW251Xp5eqB7I7v0fG+9Fr4Lfsd/k//QP/h/U7/HwCWAXACoAIFBHgDKwNFAcMA7wB6ANH/Yf7T/cH86PzK/CP81fzr/O78evx3+2D7P/zO/Tn/uP93ANgA6gCwAHAA/ADsATsDogMzBE8EXgREBDAEsQRaBS8GYQYBBlYGSgaLBxEImghACYUIXgkCCL0IQglrCiMMCwwADCcL6QngCIwIrghrCaQJ5wn0CewJsAkyCX8I4wh3CSIKiQrHCgsLdAt6C90KwwqJCp4KLAqmCd0IfwjiB1IHUQZoBYwEBwO/ARAAQP9e/lr9Tvx3+xX6xPjK9jT2M/UG9I7zUPK98W/wsO/w7tzt0Oxg7Crsuuus6/XsaO7u79jv/e978JDwzfBD8Kjx8vMc9tb2C/fB97X4J/n++If5DPvK/IP9AP4s/sX+xv7F/ib/iP+NAKwABAAEABAAqwAdAfEAAAHTAHoAUgD2/6YAcwH2AYwChAJfAsoBjwE+AWoB6AHxAWUCiwJvAmICLgKCAtICGAP/AtgCzQLZAvICTQMIBAkEXARJBN0DwgOTA4YDzANOBG4EiQRLBPUDogNLAzgDGgNzA5wDqwPMA80D1gOpAy8EVgR9BJ4EbQSCBMoEDwUjBTsFLwUHBbMEXAQjBAEE3AOeA3EDCQOmAhYCzgF9ARQB8gCcACIAAAB3/zL/tv5X/jL+qP09/bf82vvj++T66vu4/Pn9Sv7X/TX+Ovwk+gD5jvr8+1r8p/oh+e34x/mB+Sb5FPm9+bX6vflc+cf6QPya/JT7qfpY+1H7W/ol+qP7MPz7+4P7vvuS/DL8jfua+3T8vvxj/In8bP3x/br9TP3G/cP+D/+J/jb/LQDcAFwAXgA8AXIBgQFBAeUBvALZAqYClQIiA44DhgN1A88DRQRABPYDHwRVBGcELgTjA+wD5QO2A3gDWANHAwEDvQJyAgMC6QGNAW8BUgFGATgB7QCEACwADQD//+7/6f8BAOD/qP87/w7/2/6t/o3+aP5L/kb+Df7h/dH9nv14/Uv9N/0p/Qz9FP0b/Tn9Nv0j/Q795vy8/Ir8gfyJ/Jr8xPzT/Mr8ufyj/I78f/xy/Hv8n/yj/LH8v/y5/NP80vzL/M78ufzL/OD8Cf1G/ZX90P3u/fX9+/3+/RX+Pv6S/uH+J/9Y/2T/bf9g/1D/UP9e/4L/pf/X/wIAGAAuADoAPQBHAFUAZwB7AJgAowDGAOAA+QATASIBJAEhASIBMwFSAXIBkQG5AcoBzQHIAcYBwQGvAbEBvAG9AbgBsgG3AbgBrAGZAX8BUwExARAB+wABAQkBGgEZAQgB5gC4AIIAZgBUAFwAaQByAHoAfwB6AGsAWAA9ACgAEQAJABAAIABHAF0AdAB3AGsAXwBBADYAOQBQAHcAoADBAMUAtQCiAJUAiACDAIwAkQCRAKUApAC7ANEAzwDPAL8AuwC5AMMA2wD7ACQBQgFUAWMBWAFTAVABTgFYAWQBcgFwAXcBdAF1AWIBTgFHAT0BOwEzAUMBQwFFAU0BPgE6AR4BDgH9APgA9wD+AAkBCgEJAf8A+ADjANkAzADOAM4AzQDVAMsAygDIAMYAxgDMAMsA0QDTANcA3wDiAO0A7QDtAPMA6ADoANwA1wDkANYA1gDZANEA1gDOAMcAygC4ALsAsACuAMIAwADLALsAwADEALgAqwCmAKkAowCdAKAAqgCnAKEAmwChAKEAkgCSAI8AiwCIAI4AlACUAIsAhQCDAHkAdABrAGkAYwBcAGEAXABaAFMAUABMAEYARwBBAD4APQBDAEMASABTAE4AVgBYAFYAWQBkAFsAYQBhAGkAeAB4AIIAhACIAI8AkwCVAJsAowChAJkAogCkAKIAqAClAKQApACpAKoAqwCpALAAqgCwAKsApwCuAKwAqwCnAK8AqACrAKEAngCeAJ8AoACZAJEAiACKAIkAgAB7AIAAfQCBAHwAewBsAGoAbwBtAGoAZgBqAGoAYgBkAF0AVQBeAFYAWQBXAFcAWQBYAFsAWQBdAF8AZABYAFkAYwBbAFcAWgBWAFoAWQBYAFsAVABPAFAASABOAE0AQgA+ADoAOAA9ADIALwAqACEAIgAiABcAGgAmABYABwAHAAUAAwD5//P/6//u/+b/4P/e/9j/0f/Q/8P/wv/I/8b/vf++/8H/uP/A/7n/tf+7/6X/rP+x/6r/tP+s/6z/pv+Y/6L/nf+l/6n/pP+b/6D/n/+e/5X/jv+M/4n/hf+E/47/iv+L/4r/h/+G/4b/hP+K/4r/h/+G/4D/gv+C/4T/i/+H/3//gf9//4X/gP9//33/g/+C/33/e/97/3r/d/98/3r/eP9z/3P/df9t/23/bv9u/2b/Z/9r/2X/Xf9f/1//Wf9P/1H/U/9O/0j/Qv9G/0X/Nf8z/zT/L/8p/yf/IP8i/yH/Gv8e/xT/Ev8X/wz/Dv8G/wn/CP8H/wj/Bv8D/wL/Cv8I/xL/Df8J/wv/Af8J/w3/D/8O/xL/Ff8b/xX/GP8Z/xv/F/8X/xX/Dv8Y/xf/HP8g/xb/Gf8g/yD/Iv8j/x//Hf8c/xv/Iv8d/xj/IP8e/yf/JP8k/yb/Iv8a/x7/Iv8a/yX/If8i/yX/G/8g/yr/I/8n/yT/M/8y/yn/Of82/0T/Q/9N/0b/Tv9S/1z/Y/9g/2f/Y/9o/2r/eP9y/3T/c/91/3X/e/98/3z/hf+B/4r/jf+S/4v/if+V/5X/k/+Z/4//l/+W/5L/kv+N/5j/lf+R/5D/j/+P/4f/iv+L/4v/hv+L/5D/kv+E/4n/i/+N/5z/jP+H/4f/gf+H/4X/gv+D/4P/f/99/3X/cv9t/2z/ZP9l/2j/Zv9k/2b/Yv9g/2X/W/9X/1b/T/9Q/1H/SP9I/0L/SP9I/z3/QP88/0H/QP89/zj/NP8s/y3/Lv8n/xz/Hf8X/xn/F/8T/xL/Df8M/wz/Df8J/wr/Bf8C//3++/79/v7+A/8C/wb/CP8I/wL/Af8J/wf/A/8L/xD/Ef8M/wz/Cf8K/wz/Cv8V/w3/BP8R/w//D/8M/w3/Ev8T/w3/Dv8W/xf/F/8Y/w7/E/8T/xP/Gv8X/xn/F/8c/xj/GP8e/xf/F/8X/xn/Gf8X/xz/Ff8V/xj/F/8e/xj/Hf8R/w3/Dv8R/xD/Dv8W/xH/Cf8U/w//DP8U/w//Dv8P/wj/Bv8N/xL/E/8P/w//EP8L/wr/CP8D//3+CP/9/v3+//4H/wH//v72/vH+9P7y/v/+8v7v/u/+7f7r/un+6v7z/vj+7P7q/uD+6/7n/uj+7f7u/uv+6/7p/uP+7P7o/uX+5P7p/uf+5f7u/ub+6/7o/uj+5v7f/t7+4v7n/ur+6P7i/uP+3P7r/ub+5f7n/un+9f7u/vL+6/7w/u3+7/7u/uz+8v7p/uz+8v7x/vf++P71/vX+9P76/vv+//7+/gD/Bv8K/w//Df8V/w7/Dv8Y/xn/G/8c/xb/HP8e/x7/Hv8j/yX/Jv8s/zD/MP84/zz/PP86/z3/PP9C/0f/Rf9T/1D/Tv9W/1X/WP9U/1f/Uv9S/1b/Tv9d/1v/W/9h/1X/V/9X/1n/Xf9b/1j/VP9L/1b/W/9Y/2H/XP9Z/2L/XP9b/2T/ZP9m/2j/b/9v/27/bv9x/23/bv90/3L/cf92/3T/cf9w/3D/eP93/4D/gf+K/4T/hv+K/3//hf+A/4r/iv+L/43/j/+O/4//kv+R/4//lP+c/5j/mf+a/53/qv+p/6T/q/+k/6j/s/+x/77/vv/D/8H/u//G/8f/y//M/9X/3P/h/9b/2v/x//D/+f/7/wIAAAD//wMAAAAGAAYAEQAfAB8AKQAzADIANwAuADcANwBBAD4ARwBSAE8AWABaAGgAZQBsAHAAbgB2AIUAjgCMAJwAlwCbAK0ApwC6ALQArgCzAKcAuQC4AMUAzADPANcA1wDgAOMA4QDjANoA4ADjAO0A9wABAQABAwEHAf4ABAH5AP4ABwEJARMBGwEpASgBMgEjASQBIQEmASkBKQErATEBQwFAATsBSAFBAUcBSwFLAU0BUQFeAVgBXgFhAWMBZwFfAWMBXwFZAVoBWQFkAWkBagFrAWYBagFlAWQBYgFoAW0BdAFuAXIBdwF+AYABdgF2AXsBeAF3AXwBfgGFAYIBgwGHAYsBhQGJAYABeQF6AYABhQF/AYEBgQF9AXwBegFyAXIBagFsAWsBbAFrAWkBYQFjAWEBYAFcAVYBVgFWAUoBTgFRAVIBXQFQAVQBSAFLAUcBRAFCAUcBSwFPAUkBSAFHAUIBTQFDAUoBSwFQAUUBSQFKAUsBSwFEAUUBRgE6ATwBOwE6ATcBLwEzAS8BJwEjASEBIwEoASQBIgEfASMBIwEeASEBJwEmARcBIgEdAR0BDAEPARYBFAEGAQ0BBAELAQkBBAEBAfsA/wADAQgBCAH/AAYBAgEHAQgBAwEXARIBFAEKAR8BEQEaAQsBEwEjARUBIwEYASIBDwEpASYBLQEtASgBHwEfATEBTAFJAV0BdAGNAbcBzwHzAScChgK/AlYDDAOsA2gDEAXPBM4GJQmhEbEWDBIQCxYDFAKm/y79bPsi/u8BHQSkAZ7+d/1S+075pvUJ95H80QANA18DAQPHA/IBdv5d+zz60/wb+1X7xv2+/z0EigEtAen+DP64/Mz6ff6UAL0BrABt/l39fP0O/Xz8UfwC/nr7TPqH/ysDXghBCEwKSQxGDcgI3gC6/WcCrQtWDuAKGwKkANQBFf+K9xn3Ifun+x30dOyA823/zQUp/8H7Av8HAe75G/ND95b/pAPb//L9WACOAzQDqP7O/Ej/egFLBJYBaQFHAcEBcQEs/WH6C/oS/R/9Jf6D/vUCLQZ0Aun/6Po+/Bj90f3hAWAEugW8BDkCggBl/l/9D/7k/poAeQCFAXUBJgDj/iP9J/vm+oX7mfum/ZP9V//c/eb9sfy5/HD8JvpO+uD7T/+A/wIAxv+8ANcAxf2C+1/9EgHDAHP+5v04AOf/Gf4s/JX9Qv9g/v38Rf2p/2v/Nf5q/Mf8pf11/Nr7/Ptp/M365PoL/Ib8ovsd+m36M/so+vf49fma+5r7ivpN+iX7ivvE+sr5bvrG+4n7uvom++n7nfy1+xH7p/sB/JH7BvuR+/X7GPzG+4v8aP0E/br73Ppb+9D7p/sX/KH8Pf1O/dX8R/1P/ZD9L/3t/Mv9yP0C/i7+7/2E/lr+E/70/Wj9b/1L/XT9+v3n/dD9Gv7D/Yv9Df0N/VH9R/1o/W79hP2F/VT9Lf1//ar9Zf0E/fX8yfzi/Cf9mf3S/ZP9ov0t/dD8Xfy7/Pv8EP0o/Tr91v3E/ab9bv2m/bX9aP0j/Sz9sf3w/RH+/P0P/j3+Qf75/cv9iv3P/en9W/6o/rP+tf5e/m7+Bf4G/vz9CP53/rb+4/53/qn+yP6p/kf+RP50/pX+Wf4O/ov+9f4t/8n+u/7T/un+mf6B/rb+6f4K/0H/ef9o/0f/yv7b/tD+2/6m/vX+O/8d/wT/9f4+/+P+p/7h/ln/X/82/0H/iP+B/2T/Sv8T/87+8P70/uH+wv6F/tP+4P7O/rf+5P7+/tn+lP6a/h7/A//o/uH+LP8B/77+q/7r/of/TP8h/y7/sP/Y/2P/zP65/vj+Bv8g/zX/sf8FAAsAdf8Q/wP/Kf96/4f/vv/9/1YANQDu//H/2f/V/4b/Lf90/zUAwQC2AEsAUwBRAOf/Hf/X/j//8P8DAPX/hgAeAeEA+P9M/zv/l/+9/8j/DQCAAKkAawAXANv/of/N/57/bf9+/xwAzwB+AFAASwD2AMoADgBb/1j/BQAfALD/7P+rANoAagDM/6b/Sf8n/z3/R/+Y/8b/LAA8ABgA5v+9/5T/Zv8t/6r/OQBuAGsAYgCpAJcAEwD+/xUAGADb/63/6P94AI8AjQCGAMMAaQDK/2X/Uf+X/87/SwDqABsB4gCpAIAAcwBOAOj/qv/A/0oA9wCiAakBbgHcAEwAs/88//f+Jf/A/3sABwFQAUwBAAFFACn/s/4E/0T/kP8DAPwAbwFNAQMB3gCcAPv/ev/O/7QAMQFJAWQBqgHeAdkBXQF6ACEA3//s//r/mAB8ASoC/gEkAXwANQDP/3z/qP9XAJ0AoQDwAFYB3QBKAN3/HAAzAG3/Ev9D/0cAzAAqAcEB3gGGAccAdAAeACUATAAVAVgBmAG9AaMBMQHt/wT/zP5g/x4AbwChAGEALQD2/+H/j/9G//D+2/5y/wMAagDwADEB8wBUANX/tf+n/+D/LQC7AK4B2QG5AR0BlgBDAAAAJQBKAHIAigCZAOgA7wCyAD8ADAAEANj/yP/T/0EAsQAUAR8BhAD8/7b/sP/d/yEAZQB0AEcA2P/N/xUAHQAAANb/t/+0/7n/yP/N/9//3f/l/+r/2v/w//r/TwCgAJ0ARwAHAEIAlQDUAM0A6QAGAfIAcwA2AHkAzADbAIsAlACEAIIAbABBAEMATQBSADYAAAAHAB8ATgBbADcATQCEAHQAIAAYAFgAtADWAJcAdwBwAIgAwgDvAN0AtgCzAJIAjgBuAHQAowBrAAQApP/3/2EAfgAwAPn/EQAKAMb/k/90/3D/i/+n/8H/df9//5//z/+n/27/sf/q/w8A6v8IADYAWQAHANT/AABnAI8AfwCpAMoA0QCNAEEAGwBCAFYAcAB1AJ8ApACkAIkADACo/3b/k/+U/53/9P85AGAA8v+o/27/lv+j/4r/2f8dAGQAPQA5AGoAfwA+APP/+v/x//7/FgBEAFQAFQDG/8L/iP8Z/8b+7v5S/3H/Mv8c/0j/bv8s/wz/Y/++/8r/nv+n/+3/8P/u/wEAFgAbANv/qv++/+j/1v/D/9v/8//W/6v/pf/G/wMA+f/j/93//P8LAN7/5P8WABkA9f/E/6b/rP+z/7v/vv+7/8//wf/k/w8ACgDs/+j/DAAYAO//yP/f/yQABACH/03/bf+X/4X/Tv82/1D/kf+H/1j/Vv9p/1j/Of83/0j/Yf9b/w7/qv6x/vX+Mf8i/9f+2f4D/yf/FP/5/gj/IP8a/zj/Qv9a/1n/g/97/1L/+P6d/rT+1P7t/tb+/v5I/1b/OP/8/gL/+P4D/9n+uf68/sL+7f4L/wr/4/6g/mT+IP4S/kT+e/6j/rT+rP61/rH+gP5C/jL+Kf5G/kz+bv6V/tD+2P69/p/+ef5n/kX+Lv4q/lz+kv6b/nf+Qv5D/i3+8v3B/Z39mv3Q/dX91/3c/dn9t/15/V39bP2R/Z79tv3h/Qf+PP5b/lb+T/46/hr+E/4N/if+NP44/kP+QP42/hX+8f3g/dD92/3u/Rj+Rv5f/nf+aP5F/i/+Cf7f/df91f3n/fv9Bv75/f398f3M/a39kP14/XL9jv2r/c397f0N/v796v27/aD9nf2l/bX9wv3g/eT9+f0M/g3+A/7X/cX9yv3a/ef96P3v/f799f3X/bn9i/1q/V39W/16/YT9jv2o/cH92/3b/b79y/3V/ej98f3l/en9DP4i/iP+J/4N/vn9+/3j/ef90v3W/eP9CP4u/jb+Lv4d/v392f2w/Z39k/2P/Yf9g/2N/Y/9jv19/Xv9fv15/XT9g/2L/Z79uP2t/cH9r/2T/Xv9X/1n/YH9kP2j/cD93P3Z/dP9t/3C/cH9sf29/d/9//0N/iX+G/4B/t790v3T/d396v30/QP+BP7+/fv9/P3v/fH98f30/f399v39/f/9Bv4R/hH+Gf4g/jf+Tf5k/nb+h/6L/oD+e/52/oD+jP6K/on+h/6J/ob+iv6M/pz+l/6J/pH+jv6O/oT+d/52/nj+ef56/nn+jv6R/p3+o/6g/pL+kP6P/qL+wP7I/sr+y/7X/s3+yP69/rL+vf7J/sr+yf7S/t/+8f7h/tb+2f7Y/uD+7f7//g//HP8k/yT/Gf8i/yv/Nv8//yz/O/9b/1z/Qf87/1D/TP87/yf/JP8+/y//J/8d/yn/PP9M/07/Rv9S/2f/df9d/1j/a/+D/4b/e/+C/4j/dP9r/2H/af9g/2D/Vv9M/2H/UP9Y/1f/Q/8v/y//LP8r/zb/L/89/0r/TP9S/1f/Vv9m/2H/Wv9z/37/f/93/4P/of+V/4r/mP+W/5z/iv+F/5z/k/+c/6f/r/+5/7n/t//A/8r/0P/L/8r/z//Z/+f/5v/g/+H/6v/n/+v/9P/3//7//f8AAAQADQAKAAgABQABAAUAAQADAAUABQAGAAoADAATABgAGgAYABUAGgAfAB8AIwAiACoAMAAsACcAKQAvACkAIQAfACEAIwAgACQAKwAvAC4ANABBAEoAQwBJAEoAQgA+AEIAUABUAFUAWQBfAFsATgBIAEQAQgA9AEMARwBJAE4ATQBRAFQAUABRAEwASwBPAFEARwA/AEkASQBJAEkARgBAADcAMwA0ADAAKQAsADQAMwAlACUALgAsABwAGwAdACAAHwAfAC8AJgAkACUAIgAqACMAJgAkACUAIgAlACsAKAAqACsAMQA4ADQANAA2ADcANgAxACYAIwApACQAIwAiABoAFwAWABQAFgAZABcAIAAnACMAHAAfACkAJgAiACIAIgAnACAAKAApACgAKgAwADYANAAzACoAKwAqACoAJQAiACQAIAAoACsAJwAqACsAJAAnACcAJAAfAB4AFwAaAB8AHAAZABoAHQAaABMACwAHAAMABAAKAAMABQAAAAYABwAGAAUABwAOABUACgASACEAGAAYABkAFwAWABEAEwAaAB4AEQAWACEAHAAhABwAHwAcAAsADQALABEAEAAUABkAIgAaACEAKgApAC0AKgAuADMAOQA/AEEAPABJAEUASgBIAD8ATwBLAFIAWQBXAGEAZABgAGAAZQBmAHIAcQByAHkAcgB8AHwAfgB0AHQAegB6AH0AggB9AHAAdgB2AHoAfwCAAIMAiACCAIMAgQCIAIUAfAB8AHAAegB6AHUAfQB3AHkAeQCCAIcAggCNAIcAigCOAJIAlgCTAKgAnQCfAKQArgC3ALcAvADGAMQAwwDMAN0A2ADfAOAA3QDxAOsA6wDvAPkA9wD7AAYBBQEPARwBEwEYARsBIAEeARYBGQEcASkBGwElASEBGgEfASYBKQEsASQBIwEvASoBKgEuAS8BKQErASkBKwElAScBJgEqAS8BJAEuASkBIAErASwBJgEwAS4BJgE1AS4BLwEjASEBNQEtAS4BKQErAS0BMQEtATIBMAEoATYBKgEwAUIBRQE8AUIBRgFAAUwBSAFAAUYBRQE4AT0BPAFCATsBOAEzAT0BOwEwAToBOQE/ATcBPwFGAUABPAE5AT8BQgE5AT0BRQE6AUIBQwFDAUMBRAFGAUUBSAFKAUQBRAFBAUIBRAFJAUcBSQFFAUABRgE9AUABSQFBATwBPAE4ATkBOgE4AT8BMQE1ATcBMgE3ATEBKwEtASYBIQEkASEBIgEeASIBIQEcARYBFAEOAQwBDQEJARQBCgEJAQkBBgEHAQYBAgH3AP4AAwH+AP4A9gDvAO8A9wD1AOkA6wDqAOwA4wDdAOcA4QDfANwA3QDaANYAzQDSANUAygDHAMgAywDNANEAywDIAMsAxADCAL4AuAC2ALAArgCpAKsApACgAKEAmwCeAJUAkwCRAI8AiQCCAIAAfAB/AH0AfAB5AHwAfwB+AHkAcwB3AHEAZQBgAF8AWgBlAGAAVwBXAFMAVwBVAFoAWgBaAFUAVABWAFgAYQBeAFkAWwBaAFoAXgBVAF0AWABUAFkAWABXAFYAWQBZAFcATQBQAFIAUwBYAFEATgBHAEYATwBEAEwARwBGAFIATQBLAEgAUQBKAEsAUQBJAEkARQBGAEkAQwBMAEkAQABIADcAQQA7ADoAOwAtADkANgAyAC8ANQAtADAALQAkACoAIQAlACkAJQAaAB8AGwAbABwAHAAZABQAFQAVABcAFQAjACAAHgAeABUAFwAgABwAHQAeAB4AIwAgACIAIQAfABAAFQAXABkAHwAiABsAGwAdAB0AGgAeACIAHQAjAB4AFgAbABcAEAATABMAFgAWAA0ADwAPABQADgAHAAgA/P8DAP//+v8BAAEAAAD8////+/8AAAAA+//5//b/+v/7/wAA+//1//n/+v8DAAgAAQAIAAoAEgAQABMAEQARABMAEQAWABoAFwAeACQAKAAnAB8AJAAtADMANwA0ADUANgA3AEAAQQBIAEUARQBNAEcAUQBRAFgAWgBUAFkAXwBcAF8AYQBiAGQAYwBoAGoAcABtAHEAZwBtAG8AbQB0AG4AcQB6AHcAegCAAHcAegB+AIcAiwCMAJAAjQCZAJQAnQCiAJkAngCZAJQAnACeAJ4AlwCbAJ4AnwCmAKQAqACtAKQApwCsAKoAsACsAK0ArQCjAKsAsQCtAK4ApgCmAKYArQCsALAArQCwALUApQCrALcAugC3ALwAtAC4ALwAuAC/AMAAwgC9AL4AxADHAMMAyAC/AMMAyQDGAMkAwADPANwA1gDVANEAzQDVANUA1ADWANUA1gDZAOEA4ADbANoA6gDoAOQA4QDmAO0A5ADkAOUA6ADuAO4A3wDlAOcA2QDeANkA2QDYANsA4wDcANwA2QDSANcA2QDcANoA2gDbANcA4gDZANgA4ADaANYA1wDbANAA1QDUAM8A0gDRANEAywDSANUA0wDQANAA0QDOANEAzgDIAMcAxgDFAMMAwAC7ALYAswCwALIArwCvALAAsACtAKkAqQCnAKgApQCqAKwAsAC2ALYArACxALYArgCyALQAqwCpAKAAnwChAJ4AlgCZAJoAlgCWAI0AmACXAI8AlQCUAJcAlQCZAJwApQCWAJ0ApgCaAJ4AmQCbAJoAmgCcAJYAjwCVAI4AjQCLAIEAiACMAIsAjgCBAIwAjgCIAIkAhwCMAJAAkACOAJMAiwCOAIwAjwCSAIkAiQCLAI0AlgCPAIgAlACCAIYAjACOAIwAigCFAH4AhQCFAIgAgwB+AHgAhgCIAIMAiAB2AIYAfQCBAIIAhQCOAHsAhwCFAIgAhACCAIkAggCBAH0AigCMAIgAhgCIAIYAhACHAJcAhwCHAIQAgACLAIAAfgCFAIUAdwCAAIoAigCSAJYAigCJAJMAlwCPAJAAiwCIAJEAigCIAIcAjgCSAJMAjgCXAIwAiACVAJgAlACRAJQAkACPAIwAiQCEAIcAiACJAIMAgACKAIUAgQCIAIoAiACKAIkAhgCSAJYAjwCGAIQAmQCUAJAAjwCSAJIAlQCaAJQAmACWAJgAkACUAKAApwCmAKkApACmALgAqQCkAKgArACiAKMAqgCqAKIApQChAKMAqgCkAKQAowCuAKwAswC7ALMArwCxALAAswCtAK4AtQCtAKwAswC3ALUAswCyALsAuwC2AKwAuACzALIAuQC8ALYAtgC3ALcAtwCwAMIAygDGAL0AvAC+ALwAtwC3AMQAwQDHAMQAxADFAMQAxADLAMUAwgDFALsAxADDAMYAzwDIAMMAxgDEAMYAxAC4AL0AuAC1ALMAvAC/AMAAvAC0ALcAuAC6ALoArgCsAK0AswC3AK0AqgCiAKoAnACcAKUAngCaAJUAmwCSAI4AhACPAJEAhwCKAIwAjgCMAJYAjQCLAI0AhwCEAIUAfgB7AH8AfwB8AIIAgAB+AIMAeQB/AHYAeAB6AHoAeQB1AHUAagB5AHYAeABtAHEAfwB3AHAAdAB3AHIAbQBqAG0AbABuAGYAZQBoAG4AagBrAG0AaQB1AG8AcwB6AHMAfACCAHsAfACCAHwAfQB5AHoAegB2AHYAeQBzAHEAdQB4AHQAaQBrAGoAaQBlAGQAYABUAFkAXQBRAFgAUgBSAFUAUABRAE0AUwBOAE0AUABKAEkASwBKAEkARABCAEMAOwA+ADEAMgAxACsAJwAfACUAIgAaABMAFAAQAA0ACAABAAUA+f8AAAcA///u//T/9v/x//D/6//p/+b/6f/k/+L/3f/g/+P/3f/h/9b/3P/c/9r/2P/R/9T/0//P/9T/0f/M/8T/wv/B/8L/vf+7/7b/tP+t/7H/qf+k/6P/mP+f/57/kv+W/4v/gv97/3r/fv9+/2z/bP9r/2v/c/9j/2T/Wv9a/1n/Vv9W/1T/UP9F/0b/RP9M/0D/PP8y/y7/LP8m/yb/If8g/yP/F/8h/yz/G/8m/x3/Gv8a/xT/Ff8Q/xH/Bv8N/wn/CP8I/w7/E/8P/wj/Cv8K/wn/C/8D/wH/+f78/vr++P4E/wL/Af8C//j+9f74/u/+9v7z/vf+9f7z/vD+6v7q/uj+8f7p/uX+6P7o/uX+1P7a/t3+0v7X/s/+yf7R/sr+y/7L/sL+tv60/r3+uf67/r/+xP7P/sr+zv7S/sH+xf69/rX+u/67/rv+u/7F/sD+wv68/rr+vP7B/r/+vv7G/sD+wP66/sL+v/66/rv+vv68/rz+uP69/r/+w/6//rj+xP63/sP+v/68/sj+vv7G/sL+wf7G/r7+wf7J/sX+x/6+/r3+yf7M/sH+xP7C/sn+0f7H/s/+0P7S/tT+0v7L/sH+yv7V/s7+y/7V/sz+zf7d/tv+1/7d/tr+2f7f/t3+3v7e/ub+5v7j/vD+7f7t/vP+8P7q/vL+7v71/uz+9f73/vb+BP/9/vj++P70/vz+Af/8/gr/Dv8L/wP/Cf8N/w7/H/8W/xv/GP8d/yL/Hv8m/yb/K/8s/zL/I/8o/zL/MP9A/0P/P/81/0L/Rv84/z7/Sf9C/0T/Rv9C/0L/P/9E/0n/RP89/0D/RP9J/z//Qf9S/0v/TP9P/0//UP9g/2L/YP9n/2X/bP9s/3D/cf9n/2z/aP9q/2r/b/90/3n/eP93/3f/fP92/3v/ev91/4H/dv94/3z/ff+D/4D/gv+L/4b/g/+K/4P/iv+J/4T/g/+E/4P/jP+E/4z/if+N/47/kv+P/5H/k/+T/5j/mP+Y/57/m/+h/5r/ov+i/53/m/+c/6P/pv+l/53/of+t/6H/pf+m/67/tf+p/7H/tP+0/7v/sf+4/73/qf+j/6H/\" type=\"audio/x-wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ğŸ§ **7. Test Model from Checkpoint** ğŸ§\n",
        "\n",
        "Test the trained model by loading from checkpoint and generating speech."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2MJFvxB-g_B",
        "outputId": "fdc53814-002c-4444-dd6f-8d3dbd9acc91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Starting Final Library Patching Process...\n",
            "  âœ… [transforms.py] Assertion removed.\n",
            "  âœ… [modules.py] Inserted shape guard (size > 1).\n",
            "  âœ… [lightning.py] Added sample_bytes param.\n",
            "  âœ… [export_onnx.py] Added security globals.\n",
            "ğŸ”§ Patching Complete.\n",
            "\n",
            "ğŸ“‚ Checkpoint: epoch=2868-step=1575188.ckpt\n",
            "ğŸ“„ Config copied.\n",
            "ğŸš€ Exporting to /content/checkpoints/en/en_US/hfc_female/medium/epoch=2868-step=1575188.onnx...\n",
            "Applied 194 of general pattern rewrite rules.\n",
            "\n",
            "\n",
            "âœ…âœ…âœ… EXPORT SUCCESSFUL! âœ…âœ…âœ…\n",
            "Saved to: /content/checkpoints/en/en_US/hfc_female/medium/epoch=2868-step=1575188.onnx\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TEST MODEL FROM CHECKPOINT\n",
        "# =============================================================================\n",
        "# Load model from checkpoint and generate test audio.\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import pathlib\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import write\n",
        "from IPython.display import Audio, display\n",
        "import sys\n",
        "\n",
        "# Add piper src to path\n",
        "if f\"{config.piper_dir}/src\" not in sys.path:\n",
        "    sys.path.append(f\"{config.piper_dir}/src\")\n",
        "\n",
        "from piper.train.vits.lightning import VitsModel\n",
        "from piper.phonemize_espeak import EspeakPhonemizer\n",
        "\n",
        "\n",
        "class ModelTester:\n",
        "    \"\"\"Test trained Piper TTS model from checkpoint.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: PiperConfig):\n",
        "        self.config = config\n",
        "        self.model = None\n",
        "        self.model_config = None\n",
        "        self.phonemizer = None\n",
        "    \n",
        "    def load_model(self, checkpoint_path: str, config_path: str) -> None:\n",
        "        \"\"\"\n",
        "        Load model from checkpoint.\n",
        "        \n",
        "        Args:\n",
        "            checkpoint_path: Path to .ckpt file\n",
        "            config_path: Path to model config JSON\n",
        "        \"\"\"\n",
        "        if not os.path.exists(checkpoint_path):\n",
        "            raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
        "        if not os.path.exists(config_path):\n",
        "            raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
        "        \n",
        "        # Load config\n",
        "        with open(config_path, 'r') as f:\n",
        "            self.model_config = json.load(f)\n",
        "        \n",
        "        # Load model\n",
        "        logger.info(f\"Loading model from {checkpoint_path}...\")\n",
        "        with torch.serialization.safe_globals([pathlib.PosixPath]):\n",
        "            self.model = VitsModel.load_from_checkpoint(checkpoint_path, map_location='cpu')\n",
        "        \n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            self.model.model_g.dec.remove_weight_norm()\n",
        "        \n",
        "        # Initialize phonemizer\n",
        "        self.phonemizer = EspeakPhonemizer()\n",
        "        \n",
        "        logger.info(\"Model loaded successfully.\")\n",
        "    \n",
        "    def synthesize(\n",
        "        self,\n",
        "        text: str,\n",
        "        noise_scale: float = 0.667,\n",
        "        length_scale: float = 1.0,\n",
        "        noise_scale_w: float = 0.8,\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Synthesize speech from text.\n",
        "        \n",
        "        Args:\n",
        "            text: Text to synthesize\n",
        "            noise_scale: Noise scale for synthesis\n",
        "            length_scale: Length scale (1.0 = normal speed)\n",
        "            noise_scale_w: Noise scale for duration predictor\n",
        "            \n",
        "        Returns:\n",
        "            np.ndarray: Audio data\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise RuntimeError(\"Model not loaded. Call load_model() first.\")\n",
        "        \n",
        "        # Phonemize text\n",
        "        phoneme_lists = self.phonemizer.phonemize(self.config.espeak_voice, text)\n",
        "        phonemes = []\n",
        "        for sentence in phoneme_lists:\n",
        "            phonemes.extend(sentence)\n",
        "        \n",
        "        # Map phonemes to IDs with interspersing\n",
        "        id_map = self.model_config[\"phoneme_id_map\"]\n",
        "        pad_id = id_map.get(\"^\", [0])[0]\n",
        "        \n",
        "        phoneme_ids = [pad_id]\n",
        "        missing_phonemes = []\n",
        "        \n",
        "        for p in phonemes:\n",
        "            if p in id_map:\n",
        "                phoneme_ids.extend(id_map[p])\n",
        "                phoneme_ids.append(pad_id)\n",
        "            else:\n",
        "                missing_phonemes.append(p)\n",
        "        \n",
        "        if missing_phonemes:\n",
        "            logger.warning(f\"Missing phonemes in config: {missing_phonemes}\")\n",
        "        \n",
        "        # Convert to tensors\n",
        "        sequence = torch.tensor(phoneme_ids, dtype=torch.long).unsqueeze(0)\n",
        "        sequence_lengths = torch.tensor([len(phoneme_ids)], dtype=torch.long)\n",
        "        \n",
        "        # Handle speaker ID\n",
        "        sid = None\n",
        "        if self.model_config.get(\"num_speakers\", 1) > 1:\n",
        "            sid = torch.tensor([0], dtype=torch.long)\n",
        "        \n",
        "        # Generate audio\n",
        "        with torch.no_grad():\n",
        "            audio = self.model.model_g.infer(\n",
        "                x=sequence,\n",
        "                x_lengths=sequence_lengths,\n",
        "                sid=sid,\n",
        "                noise_scale=noise_scale,\n",
        "                length_scale=length_scale,\n",
        "                noise_scale_w=noise_scale_w\n",
        "            )[0]\n",
        "        \n",
        "        # Process audio\n",
        "        audio_data = audio.squeeze().cpu().numpy()\n",
        "        audio_data = audio_data / np.max(np.abs(audio_data))\n",
        "        \n",
        "        return audio_data\n",
        "    \n",
        "    def synthesize_and_save(\n",
        "        self,\n",
        "        text: str,\n",
        "        output_path: str,\n",
        "        **kwargs\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Synthesize speech and save to WAV file.\n",
        "        \n",
        "        Args:\n",
        "            text: Text to synthesize\n",
        "            output_path: Output WAV file path\n",
        "            **kwargs: Additional arguments for synthesize()\n",
        "            \n",
        "        Returns:\n",
        "            str: Path to saved WAV file\n",
        "        \"\"\"\n",
        "        audio_data = self.synthesize(text, **kwargs)\n",
        "        sample_rate = self.model_config[\"audio\"][\"sample_rate\"]\n",
        "        \n",
        "        write(output_path, sample_rate, (audio_data * 32767).astype(np.int16))\n",
        "        logger.info(f\"Audio saved to {output_path}\")\n",
        "        \n",
        "        return output_path\n",
        "\n",
        "\n",
        "# Initialize tester\n",
        "model_tester = ModelTester(config)\n",
        "print(\"âœ… Model tester initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JS1Zci-b429r"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TEST SYNTHESIS FROM CHECKPOINT\n",
        "# =============================================================================\n",
        "# Generate test audio from trained model.\n",
        "\n",
        "# Configuration for testing\n",
        "TEST_TEXT = \"Hello, this is a test of the fine-tuned Piper text to speech model. Is this sounding correct?\"\n",
        "\n",
        "# Determine checkpoint and config paths\n",
        "if config.colab_mode:\n",
        "    gdrive_model_dir = os.path.join(config.gdrive_output_path, config.model_name)\n",
        "    test_checkpoint_path = f\"{gdrive_model_dir}/last.ckpt\"\n",
        "    test_config_path = f\"{gdrive_model_dir}/{config.model_name}.json\"\n",
        "else:\n",
        "    test_checkpoint_path = f\"{config.local_output_dir}/checkpoints/last.ckpt\"\n",
        "    test_config_path = f\"{config.local_output_dir}/{config.model_name}.json\"\n",
        "\n",
        "# Load model and synthesize\n",
        "try:\n",
        "    model_tester.load_model(test_checkpoint_path, test_config_path)\n",
        "    \n",
        "    print(f\"\\nSynthesizing: \\\"{TEST_TEXT}\\\"\")\n",
        "    audio_data = model_tester.synthesize(TEST_TEXT)\n",
        "    \n",
        "    sample_rate = model_tester.model_config[\"audio\"][\"sample_rate\"]\n",
        "    print(f\"Audio duration: {len(audio_data) / sample_rate:.2f} seconds\")\n",
        "    \n",
        "    # Display audio player\n",
        "    display(Audio(audio_data, rate=sample_rate))\n",
        "    \n",
        "    # Save to file\n",
        "    output_wav = f\"{config.local_output_dir}/test_output.wav\"\n",
        "    model_tester.synthesize_and_save(TEST_TEXT, output_wav)\n",
        "    print(f\"âœ… Test audio saved to: {output_wav}\")\n",
        "    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"âš ï¸ Cannot test model: {e}\")\n",
        "    print(\"This is expected if training has not completed yet.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "export-onnx",
        "outputId": "de1fbf99-17d2-4aed-8382-93fa2625bc9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using specified pretrained checkpoint for ONNX export: /content/checkpoints/en/en_US/hfc_female/medium/epoch=2868-step=1575188.ckpt\n",
            "/content/piper1-gpl\n",
            "Exporting ONNX from /content/checkpoints/en/en_US/hfc_female/medium/epoch=2868-step=1575188.ckpt...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Lightning automatically upgraded your loaded checkpoint from v1.9.0 to v2.6.0. To apply the upgrade to your files permanently, run `python -m lightning.pytorch.utilities.upgrade_checkpoint ../checkpoints/en/en_US/hfc_female/medium/epoch=2868-step=1575188.ckpt`\n",
            "INFO:lightning.pytorch.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.9.0 to v2.6.0. To apply the upgrade to your files permanently, run `python -m lightning.pytorch.utilities.upgrade_checkpoint ../checkpoints/en/en_US/hfc_female/medium/epoch=2868-step=1575188.ckpt`\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "/content/piper1-gpl/src/piper/train/export_onnx.py:92: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
            "  torch.onnx.export(\n",
            "W1205 11:34:43.409000 165 torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 15 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
            "W1205 11:34:53.659000 165 torch/fx/experimental/symbolic_shapes.py:7942] Unable to find user code corresponding to {u2}\n",
            "\n",
            "\n",
            "\n",
            "def forward(self, arg0_1: \"f32[256, 192]\", arg1_1: \"f32[1, 9, 96]\", arg2_1: \"f32[1, 9, 96]\", arg3_1: \"f32[192, 192, 1]\", arg4_1: \"f32[192]\", arg5_1: \"f32[192, 192, 1]\", arg6_1: \"f32[192]\", arg7_1: \"f32[192, 192, 1]\", arg8_1: \"f32[192]\", arg9_1: \"f32[192, 192, 1]\", arg10_1: \"f32[192]\", arg11_1: \"f32[1, 9, 96]\", arg12_1: \"f32[1, 9, 96]\", arg13_1: \"f32[192, 192, 1]\", arg14_1: \"f32[192]\", arg15_1: \"f32[192, 192, 1]\", arg16_1: \"f32[192]\", arg17_1: \"f32[192, 192, 1]\", arg18_1: \"f32[192]\", arg19_1: \"f32[192, 192, 1]\", arg20_1: \"f32[192]\", arg21_1: \"f32[1, 9, 96]\", arg22_1: \"f32[1, 9, 96]\", arg23_1: \"f32[192, 192, 1]\", arg24_1: \"f32[192]\", arg25_1: \"f32[192, 192, 1]\", arg26_1: \"f32[192]\", arg27_1: \"f32[192, 192, 1]\", arg28_1: \"f32[192]\", arg29_1: \"f32[192, 192, 1]\", arg30_1: \"f32[192]\", arg31_1: \"f32[1, 9, 96]\", arg32_1: \"f32[1, 9, 96]\", arg33_1: \"f32[192, 192, 1]\", arg34_1: \"f32[192]\", arg35_1: \"f32[192, 192, 1]\", arg36_1: \"f32[192]\", arg37_1: \"f32[192, 192, 1]\", arg38_1: \"f32[192]\", arg39_1: \"f32[192, 192, 1]\", arg40_1: \"f32[192]\", arg41_1: \"f32[1, 9, 96]\", arg42_1: \"f32[1, 9, 96]\", arg43_1: \"f32[192, 192, 1]\", arg44_1: \"f32[192]\", arg45_1: \"f32[192, 192, 1]\", arg46_1: \"f32[192]\", arg47_1: \"f32[192, 192, 1]\", arg48_1: \"f32[192]\", arg49_1: \"f32[192, 192, 1]\", arg50_1: \"f32[192]\", arg51_1: \"f32[1, 9, 96]\", arg52_1: \"f32[1, 9, 96]\", arg53_1: \"f32[192, 192, 1]\", arg54_1: \"f32[192]\", arg55_1: \"f32[192, 192, 1]\", arg56_1: \"f32[192]\", arg57_1: \"f32[192, 192, 1]\", arg58_1: \"f32[192]\", arg59_1: \"f32[192, 192, 1]\", arg60_1: \"f32[192]\", arg61_1: \"f32[192]\", arg62_1: \"f32[192]\", arg63_1: \"f32[192]\", arg64_1: \"f32[192]\", arg65_1: \"f32[192]\", arg66_1: \"f32[192]\", arg67_1: \"f32[192]\", arg68_1: \"f32[192]\", arg69_1: \"f32[192]\", arg70_1: \"f32[192]\", arg71_1: \"f32[192]\", arg72_1: \"f32[192]\", arg73_1: \"f32[768, 192, 3]\", arg74_1: \"f32[768]\", arg75_1: \"f32[192, 768, 3]\", arg76_1: \"f32[192]\", arg77_1: \"f32[768, 192, 3]\", arg78_1: \"f32[768]\", arg79_1: \"f32[192, 768, 3]\", arg80_1: \"f32[192]\", arg81_1: \"f32[768, 192, 3]\", arg82_1: \"f32[768]\", arg83_1: \"f32[192, 768, 3]\", arg84_1: \"f32[192]\", arg85_1: \"f32[768, 192, 3]\", arg86_1: \"f32[768]\", arg87_1: \"f32[192, 768, 3]\", arg88_1: \"f32[192]\", arg89_1: \"f32[768, 192, 3]\", arg90_1: \"f32[768]\", arg91_1: \"f32[192, 768, 3]\", arg92_1: \"f32[192]\", arg93_1: \"f32[768, 192, 3]\", arg94_1: \"f32[768]\", arg95_1: \"f32[192, 768, 3]\", arg96_1: \"f32[192]\", arg97_1: \"f32[192]\", arg98_1: \"f32[192]\", arg99_1: \"f32[192]\", arg100_1: \"f32[192]\", arg101_1: \"f32[192]\", arg102_1: \"f32[192]\", arg103_1: \"f32[192]\", arg104_1: \"f32[192]\", arg105_1: \"f32[192]\", arg106_1: \"f32[192]\", arg107_1: \"f32[192]\", arg108_1: \"f32[192]\", arg109_1: \"f32[384, 192, 1]\", arg110_1: \"f32[384]\", arg111_1: \"f32[256, 192, 7]\", arg112_1: \"f32[256]\", arg113_1: \"f32[128]\", arg114_1: \"f32[256, 128, 16]\", arg115_1: \"f32[64]\", arg116_1: \"f32[128, 64, 16]\", arg117_1: \"f32[32]\", arg118_1: \"f32[64, 32, 8]\", arg119_1: \"f32[128]\", arg120_1: \"f32[128, 128, 3]\", arg121_1: \"f32[128]\", arg122_1: \"f32[128, 128, 3]\", arg123_1: \"f32[128]\", arg124_1: \"f32[128, 128, 5]\", arg125_1: \"f32[128]\", arg126_1: \"f32[128, 128, 5]\", arg127_1: \"f32[128]\", arg128_1: \"f32[128, 128, 7]\", arg129_1: \"f32[128]\", arg130_1: \"f32[128, 128, 7]\", arg131_1: \"f32[64]\", arg132_1: \"f32[64, 64, 3]\", arg133_1: \"f32[64]\", arg134_1: \"f32[64, 64, 3]\", arg135_1: \"f32[64]\", arg136_1: \"f32[64, 64, 5]\", arg137_1: \"f32[64]\", arg138_1: \"f32[64, 64, 5]\", arg139_1: \"f32[64]\", arg140_1: \"f32[64, 64, 7]\", arg141_1: \"f32[64]\", arg142_1: \"f32[64, 64, 7]\", arg143_1: \"f32[32]\", arg144_1: \"f32[32, 32, 3]\", arg145_1: \"f32[32]\", arg146_1: \"f32[32, 32, 3]\", arg147_1: \"f32[32]\", arg148_1: \"f32[32, 32, 5]\", arg149_1: \"f32[32]\", arg150_1: \"f32[32, 32, 5]\", arg151_1: \"f32[32]\", arg152_1: \"f32[32, 32, 7]\", arg153_1: \"f32[32]\", arg154_1: \"f32[32, 32, 7]\", arg155_1: \"f32[1, 32, 7]\", arg156_1: \"f32[192, 513, 1]\", arg157_1: \"f32[192]\", arg158_1: \"f32[384]\", arg159_1: \"f32[384, 1, 1]\", arg160_1: \"f32[384, 192, 5]\", arg161_1: \"f32[384]\", arg162_1: \"f32[384, 1, 1]\", arg163_1: \"f32[384, 192, 5]\", arg164_1: \"f32[384]\", arg165_1: \"f32[384, 1, 1]\", arg166_1: \"f32[384, 192, 5]\", arg167_1: \"f32[384]\", arg168_1: \"f32[384, 1, 1]\", arg169_1: \"f32[384, 192, 5]\", arg170_1: \"f32[384]\", arg171_1: \"f32[384, 1, 1]\", arg172_1: \"f32[384, 192, 5]\", arg173_1: \"f32[384]\", arg174_1: \"f32[384, 1, 1]\", arg175_1: \"f32[384, 192, 5]\", arg176_1: \"f32[384]\", arg177_1: \"f32[384, 1, 1]\", arg178_1: \"f32[384, 192, 5]\", arg179_1: \"f32[384]\", arg180_1: \"f32[384, 1, 1]\", arg181_1: \"f32[384, 192, 5]\", arg182_1: \"f32[384]\", arg183_1: \"f32[384, 1, 1]\", arg184_1: \"f32[384, 192, 5]\", arg185_1: \"f32[384]\", arg186_1: \"f32[384, 1, 1]\", arg187_1: \"f32[384, 192, 5]\", arg188_1: \"f32[384]\", arg189_1: \"f32[384, 1, 1]\", arg190_1: \"f32[384, 192, 5]\", arg191_1: \"f32[384]\", arg192_1: \"f32[384, 1, 1]\", arg193_1: \"f32[384, 192, 5]\", arg194_1: \"f32[384]\", arg195_1: \"f32[384, 1, 1]\", arg196_1: \"f32[384, 192, 5]\", arg197_1: \"f32[384]\", arg198_1: \"f32[384, 1, 1]\", arg199_1: \"f32[384, 192, 5]\", arg200_1: \"f32[384]\", arg201_1: \"f32[384, 1, 1]\", arg202_1: \"f32[384, 192, 5]\", arg203_1: \"f32[384]\", arg204_1: \"f32[384, 1, 1]\", arg205_1: \"f32[384, 192, 5]\", arg206_1: \"f32[384]\", arg207_1: \"f32[384, 1, 1]\", arg208_1: \"f32[384, 192, 1]\", arg209_1: \"f32[384]\", arg210_1: \"f32[384, 1, 1]\", arg211_1: \"f32[384, 192, 1]\", arg212_1: \"f32[384]\", arg213_1: \"f32[384, 1, 1]\", arg214_1: \"f32[384, 192, 1]\", arg215_1: \"f32[384]\", arg216_1: \"f32[384, 1, 1]\", arg217_1: \"f32[384, 192, 1]\", arg218_1: \"f32[384]\", arg219_1: \"f32[384, 1, 1]\", arg220_1: \"f32[384, 192, 1]\", arg221_1: \"f32[384]\", arg222_1: \"f32[384, 1, 1]\", arg223_1: \"f32[384, 192, 1]\", arg224_1: \"f32[384]\", arg225_1: \"f32[384, 1, 1]\", arg226_1: \"f32[384, 192, 1]\", arg227_1: \"f32[384]\", arg228_1: \"f32[384, 1, 1]\", arg229_1: \"f32[384, 192, 1]\", arg230_1: \"f32[384]\", arg231_1: \"f32[384, 1, 1]\", arg232_1: \"f32[384, 192, 1]\", arg233_1: \"f32[384]\", arg234_1: \"f32[384, 1, 1]\", arg235_1: \"f32[384, 192, 1]\", arg236_1: \"f32[384]\", arg237_1: \"f32[384, 1, 1]\", arg238_1: \"f32[384, 192, 1]\", arg239_1: \"f32[384]\", arg240_1: \"f32[384, 1, 1]\", arg241_1: \"f32[384, 192, 1]\", arg242_1: \"f32[384]\", arg243_1: \"f32[384, 1, 1]\", arg244_1: \"f32[384, 192, 1]\", arg245_1: \"f32[384]\", arg246_1: \"f32[384, 1, 1]\", arg247_1: \"f32[384, 192, 1]\", arg248_1: \"f32[384]\", arg249_1: \"f32[384, 1, 1]\", arg250_1: \"f32[384, 192, 1]\", arg251_1: \"f32[192]\", arg252_1: \"f32[192, 1, 1]\", arg253_1: \"f32[192, 192, 1]\", arg254_1: \"f32[384, 192, 1]\", arg255_1: \"f32[384]\", arg256_1: \"f32[192, 96, 1]\", arg257_1: \"f32[192]\", arg258_1: \"f32[384]\", arg259_1: \"f32[384, 1, 1]\", arg260_1: \"f32[384, 192, 5]\", arg261_1: \"f32[384]\", arg262_1: \"f32[384, 1, 1]\", arg263_1: \"f32[384, 192, 5]\", arg264_1: \"f32[384]\", arg265_1: \"f32[384, 1, 1]\", arg266_1: \"f32[384, 192, 5]\", arg267_1: \"f32[384]\", arg268_1: \"f32[384, 1, 1]\", arg269_1: \"f32[384, 192, 5]\", arg270_1: \"f32[384]\", arg271_1: \"f32[384, 1, 1]\", arg272_1: \"f32[384, 192, 1]\", arg273_1: \"f32[384]\", arg274_1: \"f32[384, 1, 1]\", arg275_1: \"f32[384, 192, 1]\", arg276_1: \"f32[384]\", arg277_1: \"f32[384, 1, 1]\", arg278_1: \"f32[384, 192, 1]\", arg279_1: \"f32[192]\", arg280_1: \"f32[192, 1, 1]\", arg281_1: \"f32[192, 192, 1]\", arg282_1: \"f32[96, 192, 1]\", arg283_1: \"f32[96]\", arg284_1: \"f32[192, 96, 1]\", arg285_1: \"f32[192]\", arg286_1: \"f32[384]\", arg287_1: \"f32[384, 1, 1]\", arg288_1: \"f32[384, 192, 5]\", arg289_1: \"f32[384]\", arg290_1: \"f32[384, 1, 1]\", arg291_1: \"f32[384, 192, 5]\", arg292_1: \"f32[384]\", arg293_1: \"f32[384, 1, 1]\", arg294_1: \"f32[384, 192, 5]\", arg295_1: \"f32[384]\", arg296_1: \"f32[384, 1, 1]\", arg297_1: \"f32[384, 192, 5]\", arg298_1: \"f32[384]\", arg299_1: \"f32[384, 1, 1]\", arg300_1: \"f32[384, 192, 1]\", arg301_1: \"f32[384]\", arg302_1: \"f32[384, 1, 1]\", arg303_1: \"f32[384, 192, 1]\", arg304_1: \"f32[384]\", arg305_1: \"f32[384, 1, 1]\", arg306_1: \"f32[384, 192, 1]\", arg307_1: \"f32[192]\", arg308_1: \"f32[192, 1, 1]\", arg309_1: \"f32[192, 192, 1]\", arg310_1: \"f32[96, 192, 1]\", arg311_1: \"f32[96]\", arg312_1: \"f32[192, 96, 1]\", arg313_1: \"f32[192]\", arg314_1: \"f32[384]\", arg315_1: \"f32[384, 1, 1]\", arg316_1: \"f32[384, 192, 5]\", arg317_1: \"f32[384]\", arg318_1: \"f32[384, 1, 1]\", arg319_1: \"f32[384, 192, 5]\", arg320_1: \"f32[384]\", arg321_1: \"f32[384, 1, 1]\", arg322_1: \"f32[384, 192, 5]\", arg323_1: \"f32[384]\", arg324_1: \"f32[384, 1, 1]\", arg325_1: \"f32[384, 192, 5]\", arg326_1: \"f32[384]\", arg327_1: \"f32[384, 1, 1]\", arg328_1: \"f32[384, 192, 1]\", arg329_1: \"f32[384]\", arg330_1: \"f32[384, 1, 1]\", arg331_1: \"f32[384, 192, 1]\", arg332_1: \"f32[384]\", arg333_1: \"f32[384, 1, 1]\", arg334_1: \"f32[384, 192, 1]\", arg335_1: \"f32[192]\", arg336_1: \"f32[192, 1, 1]\", arg337_1: \"f32[192, 192, 1]\", arg338_1: \"f32[96, 192, 1]\", arg339_1: \"f32[96]\", arg340_1: \"f32[192, 96, 1]\", arg341_1: \"f32[192]\", arg342_1: \"f32[384]\", arg343_1: \"f32[384, 1, 1]\", arg344_1: \"f32[384, 192, 5]\", arg345_1: \"f32[384]\", arg346_1: \"f32[384, 1, 1]\", arg347_1: \"f32[384, 192, 5]\", arg348_1: \"f32[384]\", arg349_1: \"f32[384, 1, 1]\", arg350_1: \"f32[384, 192, 5]\", arg351_1: \"f32[384]\", arg352_1: \"f32[384, 1, 1]\", arg353_1: \"f32[384, 192, 5]\", arg354_1: \"f32[384]\", arg355_1: \"f32[384, 1, 1]\", arg356_1: \"f32[384, 192, 1]\", arg357_1: \"f32[384]\", arg358_1: \"f32[384, 1, 1]\", arg359_1: \"f32[384, 192, 1]\", arg360_1: \"f32[384]\", arg361_1: \"f32[384, 1, 1]\", arg362_1: \"f32[384, 192, 1]\", arg363_1: \"f32[192]\", arg364_1: \"f32[192, 1, 1]\", arg365_1: \"f32[192, 192, 1]\", arg366_1: \"f32[96, 192, 1]\", arg367_1: \"f32[96]\", arg368_1: \"f32[2, 1]\", arg369_1: \"f32[2, 1]\", arg370_1: \"f32[192, 1, 1]\", arg371_1: \"f32[192]\", arg372_1: \"f32[192, 1, 3]\", arg373_1: \"f32[192]\", arg374_1: \"f32[192, 1, 3]\", arg375_1: \"f32[192]\", arg376_1: \"f32[192, 1, 3]\", arg377_1: \"f32[192]\", arg378_1: \"f32[192, 192, 1]\", arg379_1: \"f32[192]\", arg380_1: \"f32[192, 192, 1]\", arg381_1: \"f32[192]\", arg382_1: \"f32[192, 192, 1]\", arg383_1: \"f32[192]\", arg384_1: \"f32[192]\", arg385_1: \"f32[192]\", arg386_1: \"f32[192]\", arg387_1: \"f32[192]\", arg388_1: \"f32[192]\", arg389_1: \"f32[192]\", arg390_1: \"f32[192]\", arg391_1: \"f32[192]\", arg392_1: \"f32[192]\", arg393_1: \"f32[192]\", arg394_1: \"f32[192]\", arg395_1: \"f32[192]\", arg396_1: \"f32[29, 192, 1]\", arg397_1: \"f32[29]\", arg398_1: \"f32[192, 1, 1]\", arg399_1: \"f32[192]\", arg400_1: \"f32[192, 1, 3]\", arg401_1: \"f32[192]\", arg402_1: \"f32[192, 1, 3]\", arg403_1: \"f32[192]\", arg404_1: \"f32[192, 1, 3]\", arg405_1: \"f32[192]\", arg406_1: \"f32[192, 192, 1]\", arg407_1: \"f32[192]\", arg408_1: \"f32[192, 192, 1]\", arg409_1: \"f32[192]\", arg410_1: \"f32[192, 192, 1]\", arg411_1: \"f32[192]\", arg412_1: \"f32[192]\", arg413_1: \"f32[192]\", arg414_1: \"f32[192]\", arg415_1: \"f32[192]\", arg416_1: \"f32[192]\", arg417_1: \"f32[192]\", arg418_1: \"f32[192]\", arg419_1: \"f32[192]\", arg420_1: \"f32[192]\", arg421_1: \"f32[192]\", arg422_1: \"f32[192]\", arg423_1: \"f32[192]\", arg424_1: \"f32[29, 192, 1]\", arg425_1: \"f32[29]\", arg426_1: \"f32[192, 1, 1]\", arg427_1: \"f32[192]\", arg428_1: \"f32[192, 1, 3]\", arg429_1: \"f32[192]\", arg430_1: \"f32[192, 1, 3]\", arg431_1: \"f32[192]\", arg432_1: \"f32[192, 1, 3]\", arg433_1: \"f32[192]\", arg434_1: \"f32[192, 192, 1]\", arg435_1: \"f32[192]\", arg436_1: \"f32[192, 192, 1]\", arg437_1: \"f32[192]\", arg438_1: \"f32[192, 192, 1]\", arg439_1: \"f32[192]\", arg440_1: \"f32[192]\", arg441_1: \"f32[192]\", arg442_1: \"f32[192]\", arg443_1: \"f32[192]\", arg444_1: \"f32[192]\", arg445_1: \"f32[192]\", arg446_1: \"f32[192]\", arg447_1: \"f32[192]\", arg448_1: \"f32[192]\", arg449_1: \"f32[192]\", arg450_1: \"f32[192]\", arg451_1: \"f32[192]\", arg452_1: \"f32[29, 192, 1]\", arg453_1: \"f32[29]\", arg454_1: \"f32[192, 1, 1]\", arg455_1: \"f32[192]\", arg456_1: \"f32[192, 1, 3]\", arg457_1: \"f32[192]\", arg458_1: \"f32[192, 1, 3]\", arg459_1: \"f32[192]\", arg460_1: \"f32[192, 1, 3]\", arg461_1: \"f32[192]\", arg462_1: \"f32[192, 192, 1]\", arg463_1: \"f32[192]\", arg464_1: \"f32[192, 192, 1]\", arg465_1: \"f32[192]\", arg466_1: \"f32[192, 192, 1]\", arg467_1: \"f32[192]\", arg468_1: \"f32[192]\", arg469_1: \"f32[192]\", arg470_1: \"f32[192]\", arg471_1: \"f32[192]\", arg472_1: \"f32[192]\", arg473_1: \"f32[192]\", arg474_1: \"f32[192]\", arg475_1: \"f32[192]\", arg476_1: \"f32[192]\", arg477_1: \"f32[192]\", arg478_1: \"f32[192]\", arg479_1: \"f32[192]\", arg480_1: \"f32[29, 192, 1]\", arg481_1: \"f32[29]\", arg482_1: \"f32[192, 1, 1]\", arg483_1: \"f32[192]\", arg484_1: \"f32[192, 192, 1]\", arg485_1: \"f32[192]\", arg486_1: \"f32[192, 1, 3]\", arg487_1: \"f32[192]\", arg488_1: \"f32[192, 1, 3]\", arg489_1: \"f32[192]\", arg490_1: \"f32[192, 1, 3]\", arg491_1: \"f32[192]\", arg492_1: \"f32[192, 192, 1]\", arg493_1: \"f32[192]\", arg494_1: \"f32[192, 192, 1]\", arg495_1: \"f32[192]\", arg496_1: \"f32[192, 192, 1]\", arg497_1: \"f32[192]\", arg498_1: \"f32[192]\", arg499_1: \"f32[192]\", arg500_1: \"f32[192]\", arg501_1: \"f32[192]\", arg502_1: \"f32[192]\", arg503_1: \"f32[192]\", arg504_1: \"f32[192]\", arg505_1: \"f32[192]\", arg506_1: \"f32[192]\", arg507_1: \"f32[192]\", arg508_1: \"f32[192]\", arg509_1: \"f32[192]\", arg510_1: \"f32[2, 1]\", arg511_1: \"f32[2, 1]\", arg512_1: \"f32[192, 1, 1]\", arg513_1: \"f32[192]\", arg514_1: \"f32[192, 1, 3]\", arg515_1: \"f32[192]\", arg516_1: \"f32[192, 1, 3]\", arg517_1: \"f32[192]\", arg518_1: \"f32[192, 1, 3]\", arg519_1: \"f32[192]\", arg520_1: \"f32[192, 192, 1]\", arg521_1: \"f32[192]\", arg522_1: \"f32[192, 192, 1]\", arg523_1: \"f32[192]\", arg524_1: \"f32[192, 192, 1]\", arg525_1: \"f32[192]\", arg526_1: \"f32[192]\", arg527_1: \"f32[192]\", arg528_1: \"f32[192]\", arg529_1: \"f32[192]\", arg530_1: \"f32[192]\", arg531_1: \"f32[192]\", arg532_1: \"f32[192]\", arg533_1: \"f32[192]\", arg534_1: \"f32[192]\", arg535_1: \"f32[192]\", arg536_1: \"f32[192]\", arg537_1: \"f32[192]\", arg538_1: \"f32[29, 192, 1]\", arg539_1: \"f32[29]\", arg540_1: \"f32[192, 1, 1]\", arg541_1: \"f32[192]\", arg542_1: \"f32[192, 1, 3]\", arg543_1: \"f32[192]\", arg544_1: \"f32[192, 1, 3]\", arg545_1: \"f32[192]\", arg546_1: \"f32[192, 1, 3]\", arg547_1: \"f32[192]\", arg548_1: \"f32[192, 192, 1]\", arg549_1: \"f32[192]\", arg550_1: \"f32[192, 192, 1]\", arg551_1: \"f32[192]\", arg552_1: \"f32[192, 192, 1]\", arg553_1: \"f32[192]\", arg554_1: \"f32[192]\", arg555_1: \"f32[192]\", arg556_1: \"f32[192]\", arg557_1: \"f32[192]\", arg558_1: \"f32[192]\", arg559_1: \"f32[192]\", arg560_1: \"f32[192]\", arg561_1: \"f32[192]\", arg562_1: \"f32[192]\", arg563_1: \"f32[192]\", arg564_1: \"f32[192]\", arg565_1: \"f32[192]\", arg566_1: \"f32[29, 192, 1]\", arg567_1: \"f32[29]\", arg568_1: \"f32[192, 1, 1]\", arg569_1: \"f32[192]\", arg570_1: \"f32[192, 1, 3]\", arg571_1: \"f32[192]\", arg572_1: \"f32[192, 1, 3]\", arg573_1: \"f32[192]\", arg574_1: \"f32[192, 1, 3]\", arg575_1: \"f32[192]\", arg576_1: \"f32[192, 192, 1]\", arg577_1: \"f32[192]\", arg578_1: \"f32[192, 192, 1]\", arg579_1: \"f32[192]\", arg580_1: \"f32[192, 192, 1]\", arg581_1: \"f32[192]\", arg582_1: \"f32[192]\", arg583_1: \"f32[192]\", arg584_1: \"f32[192]\", arg585_1: \"f32[192]\", arg586_1: \"f32[192]\", arg587_1: \"f32[192]\", arg588_1: \"f32[192]\", arg589_1: \"f32[192]\", arg590_1: \"f32[192]\", arg591_1: \"f32[192]\", arg592_1: \"f32[192]\", arg593_1: \"f32[192]\", arg594_1: \"f32[29, 192, 1]\", arg595_1: \"f32[29]\", arg596_1: \"f32[192, 1, 1]\", arg597_1: \"f32[192]\", arg598_1: \"f32[192, 1, 3]\", arg599_1: \"f32[192]\", arg600_1: \"f32[192, 1, 3]\", arg601_1: \"f32[192]\", arg602_1: \"f32[192, 1, 3]\", arg603_1: \"f32[192]\", arg604_1: \"f32[192, 192, 1]\", arg605_1: \"f32[192]\", arg606_1: \"f32[192, 192, 1]\", arg607_1: \"f32[192]\", arg608_1: \"f32[192, 192, 1]\", arg609_1: \"f32[192]\", arg610_1: \"f32[192]\", arg611_1: \"f32[192]\", arg612_1: \"f32[192]\", arg613_1: \"f32[192]\", arg614_1: \"f32[192]\", arg615_1: \"f32[192]\", arg616_1: \"f32[192]\", arg617_1: \"f32[192]\", arg618_1: \"f32[192]\", arg619_1: \"f32[192]\", arg620_1: \"f32[192]\", arg621_1: \"f32[192]\", arg622_1: \"f32[29, 192, 1]\", arg623_1: \"f32[29]\", arg624_1: \"f32[192, 192, 1]\", arg625_1: \"f32[192]\", arg626_1: \"f32[192, 192, 1]\", arg627_1: \"f32[192]\", arg628_1: \"f32[192, 1, 3]\", arg629_1: \"f32[192]\", arg630_1: \"f32[192, 1, 3]\", arg631_1: \"f32[192]\", arg632_1: \"f32[192, 1, 3]\", arg633_1: \"f32[192]\", arg634_1: \"f32[192, 192, 1]\", arg635_1: \"f32[192]\", arg636_1: \"f32[192, 192, 1]\", arg637_1: \"f32[192]\", arg638_1: \"f32[192, 192, 1]\", arg639_1: \"f32[192]\", arg640_1: \"f32[192]\", arg641_1: \"f32[192]\", arg642_1: \"f32[192]\", arg643_1: \"f32[192]\", arg644_1: \"f32[192]\", arg645_1: \"f32[192]\", arg646_1: \"f32[192]\", arg647_1: \"f32[192]\", arg648_1: \"f32[192]\", arg649_1: \"f32[192]\", arg650_1: \"f32[192]\", arg651_1: \"f32[192]\", arg652_1: \"i64[s31, s12]\", arg653_1: \"i64[s31]\", arg654_1: \"f32[3]\", arg655_1):\n",
            "    # No stacktrace found for following nodes\n",
            "    select: \"f32[]\" = torch.ops.aten.select.int(arg654_1, 0, 0);  select = None\n",
            "    select_1: \"f32[]\" = torch.ops.aten.select.int(arg654_1, 0, 1);  select_1 = None\n",
            "    select_2: \"f32[]\" = torch.ops.aten.select.int(arg654_1, 0, 2);  arg654_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(\n",
            "    embedding: \"f32[s31, s12, 192]\" = torch.ops.aten.embedding.default(arg0_1, arg652_1);  arg0_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:199 in forward, code: x = self.emb(x) * math.sqrt(self.hidden_channels)  # [b, t, h]\n",
            "    mul: \"f32[s31, s12, 192]\" = torch.ops.aten.mul.Tensor(embedding, 13.856406460551018);  embedding = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:200 in forward, code: x = torch.transpose(x, 1, -1)  # [b, h, t]\n",
            "    transpose: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(mul, 1, -1);  mul = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:202 in forward, code: commons.sequence_mask(x_lengths, x.size(2)), 1\n",
            "    sym_size_int: \"Sym(s12)\" = torch.ops.aten.sym_size.int(arg652_1, 1)\n",
            "    arange: \"i64[s12]\" = torch.ops.aten.arange.default(sym_size_int, dtype = torch.int64, device = device(type='cpu'), pin_memory = False);  sym_size_int = None\n",
            "    unsqueeze: \"i64[1, s12]\" = torch.ops.aten.unsqueeze.default(arange, 0)\n",
            "    unsqueeze_1: \"i64[s31, 1]\" = torch.ops.aten.unsqueeze.default(arg653_1, 1)\n",
            "    lt: \"b8[s31, s12]\" = torch.ops.aten.lt.Tensor(unsqueeze, unsqueeze_1);  unsqueeze = unsqueeze_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:201 in forward, code: x_mask = torch.unsqueeze(\n",
            "    unsqueeze_2: \"b8[s31, 1, s12]\" = torch.ops.aten.unsqueeze.default(lt, 1);  lt = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:203 in forward, code: ).type_as(x)\n",
            "    type_as: \"f32[s31, 1, s12]\" = torch.ops.aten.type_as.default(unsqueeze_2, transpose);  unsqueeze_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:205 in forward, code: x = self.encoder(x * x_mask, x_mask)\n",
            "    mul_1: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose, type_as);  transpose = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:61 in forward, code: attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n",
            "    unsqueeze_3: \"f32[s31, 1, 1, s12]\" = torch.ops.aten.unsqueeze.default(type_as, 2)\n",
            "    unsqueeze_4: \"f32[s31, 1, s12, 1]\" = torch.ops.aten.unsqueeze.default(type_as, -1)\n",
            "    mul_2: \"f32[s31, 1, s12, s12]\" = torch.ops.aten.mul.Tensor(unsqueeze_3, unsqueeze_4);  unsqueeze_3 = unsqueeze_4 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:62 in forward, code: x = x * x_mask\n",
            "    mul_3: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(mul_1, type_as);  mul_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_3, arg3_1, arg4_1);  arg3_1 = arg4_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_1: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_3, arg5_1, arg6_1);  arg5_1 = arg6_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_2: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_3, arg7_1, arg8_1);  arg7_1 = arg8_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_1: \"Sym(s31)\" = torch.ops.aten.sym_size.int(arg652_1, 0);  arg652_1 = None\n",
            "    sym_size_int_2: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d, 2)\n",
            "    view: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d, [sym_size_int_1, 2, 96, sym_size_int_2]);  conv1d = None\n",
            "    transpose_1: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view, 2, 3);  view = None\n",
            "    sym_size_int_3: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_1, 2)\n",
            "    view_1: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_1, [sym_size_int_1, 2, 96, sym_size_int_3]);  conv1d_1 = None\n",
            "    transpose_2: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_1, 2, 3);  view_1 = None\n",
            "    view_2: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_2, [sym_size_int_1, 2, 96, sym_size_int_3]);  conv1d_2 = None\n",
            "    transpose_3: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_2, 2, 3);  view_2 = None\n",
            "    div: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_1, 9.797958971132712)\n",
            "    transpose_4: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_2, -2, -1);  transpose_2 = None\n",
            "    matmul: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div, transpose_4);  div = transpose_4 = None\n",
            "    eq: \"Sym(True)\" = sym_size_int_3 == sym_size_int_2;  eq = None\n",
            "    sub: \"Sym(s12 - 5)\" = sym_size_int_3 - 5\n",
            "    sym_max: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub, 0);  sub = None\n",
            "    sub_1: \"Sym(5 - s12)\" = 5 - sym_size_int_3\n",
            "    sym_max_1: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_1, 0);  sub_1 = None\n",
            "    mul_4: \"Sym(2*s12)\" = 2 * sym_size_int_3\n",
            "    add: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_1 + mul_4;  mul_4 = None\n",
            "    sub_2: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add - 1;  add = None\n",
            "    gt: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max > 0;  gt = None\n",
            "    pad: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg1_1, [0, 0, sym_max, sym_max, 0, 0]);  arg1_1 = sym_max = None\n",
            "    slice_1: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad, 1, sym_max_1, sub_2);  pad = sym_max_1 = sub_2 = None\n",
            "    div_1: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_1, 9.797958971132712);  transpose_1 = None\n",
            "    unsqueeze_5: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_1, 0);  slice_1 = None\n",
            "    transpose_5: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_5, -2, -1);  unsqueeze_5 = None\n",
            "    matmul_1: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_1, transpose_5);  div_1 = transpose_5 = None\n",
            "    pad_1: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_1, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_1 = None\n",
            "    mul_5: \"Sym(2*s12)\" = sym_size_int_2 * 2\n",
            "    mul_6: \"Sym(2*s12**2)\" = mul_5 * sym_size_int_2;  mul_5 = None\n",
            "    view_3: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_1, [sym_size_int_1, 2, mul_6]);  pad_1 = mul_6 = None\n",
            "    sub_3: \"Sym(s12 - 1)\" = sym_size_int_2 - 1\n",
            "    pad_2: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_3, [0, sub_3, 0, 0, 0, 0]);  view_3 = sub_3 = None\n",
            "    add_1: \"Sym(s12 + 1)\" = sym_size_int_2 + 1\n",
            "    mul_7: \"Sym(2*s12)\" = 2 * sym_size_int_2\n",
            "    sub_4: \"Sym(2*s12 - 1)\" = mul_7 - 1;  mul_7 = None\n",
            "    view_4: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_2, [sym_size_int_1, 2, add_1, sub_4]);  pad_2 = add_1 = sub_4 = None\n",
            "    sub_5: \"Sym(s12 - 1)\" = sym_size_int_2 - 1\n",
            "    slice_2: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_4, 2, None, sym_size_int_2);  view_4 = None\n",
            "    slice_3: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_2, 3, sub_5);  slice_2 = sub_5 = None\n",
            "    add_2: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul, slice_3);  matmul = slice_3 = None\n",
            "    eq_1: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_2, eq_1, -10000.0);  add_2 = eq_1 = None\n",
            "    softmax: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill, -1);  masked_fill = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax, 0.1, False);  softmax = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_2: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout, transpose_3);  transpose_3 = None\n",
            "    sym_size_int_4: \"Sym(s12)\" = torch.ops.aten.sym_size.int(arange, 0);  arange = None\n",
            "    sub_6: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_3: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout, [0, sub_6, 0, 0, 0, 0, 0, 0]);  dropout = sub_6 = None\n",
            "    mul_8: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_7: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_9: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_7;  sub_7 = None\n",
            "    add_3: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_8 + mul_9;  mul_8 = mul_9 = None\n",
            "    sym_size_int_5: \"Sym(s31)\" = torch.ops.aten.sym_size.int(arg653_1, 0);  arg653_1 = None\n",
            "    view_5: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_3, [sym_size_int_5, 2, add_3]);  pad_3 = add_3 = None\n",
            "    pad_4: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_5, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_5 = None\n",
            "    mul_10: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_6: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_4, [sym_size_int_5, 2, sym_size_int_4, mul_10]);  pad_4 = mul_10 = None\n",
            "    le: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le = None\n",
            "    slice_4: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_6, 0, 0, 9223372036854775807);  view_6 = None\n",
            "    le_1: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_1 = None\n",
            "    slice_5: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_4, 2, 0, 9223372036854775807);  slice_4 = None\n",
            "    slice_6: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_5, 3, 1, 9223372036854775807);  slice_5 = None\n",
            "    sub_8: \"Sym(s12 - 5)\" = sym_size_int_3 - 5\n",
            "    sym_max_2: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_8, 0);  sub_8 = None\n",
            "    sub_9: \"Sym(5 - s12)\" = 5 - sym_size_int_3\n",
            "    sym_max_3: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_9, 0);  sub_9 = None\n",
            "    mul_11: \"Sym(2*s12)\" = 2 * sym_size_int_3;  sym_size_int_3 = None\n",
            "    add_4: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_3 + mul_11;  mul_11 = None\n",
            "    sub_10: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_4 - 1;  add_4 = None\n",
            "    gt_1: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_2 > 0;  gt_1 = None\n",
            "    pad_5: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg2_1, [0, 0, sym_max_2, sym_max_2, 0, 0]);  arg2_1 = sym_max_2 = None\n",
            "    slice_7: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_5, 1, sym_max_3, sub_10);  pad_5 = sym_max_3 = sub_10 = None\n",
            "    unsqueeze_6: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_7, 0);  slice_7 = None\n",
            "    matmul_3: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_6, unsqueeze_6);  slice_6 = unsqueeze_6 = None\n",
            "    add_5: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_2, matmul_3);  matmul_2 = matmul_3 = None\n",
            "    transpose_6: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_5, 2, 3)\n",
            "    sym_numel_default: \"Sym(192*s12*s31)\" = torch.ops.aten.sym_numel.default(transpose_6)\n",
            "    eq_2: \"Sym(False)\" = sym_numel_default == 0;  eq_2 = None\n",
            "    eq_3: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1;  eq_3 = None\n",
            "    eq_4: \"Sym(False)\" = sym_numel_default == 0\n",
            "    eq_5: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1\n",
            "    or_: \"Sym(Eq(s12, 1))\" = eq_5 | False;  eq_5 = None\n",
            "    and_: \"Sym(Eq(s12, 1))\" = True & or_;  or_ = None\n",
            "    eq_6: \"Sym(Eq(s12, 1))\" = 1 == sym_size_int_4\n",
            "    or__1: \"Sym(Eq(s12, 1))\" = False | eq_6;  eq_6 = None\n",
            "    and__1: \"Sym(Eq(s12, 1))\" = and_ & or__1;  and_ = or__1 = None\n",
            "    mul_12: \"Sym(96*s12)\" = sym_size_int_4 * 96\n",
            "    sym_stride_int: \"Sym(96*s12)\" = torch.ops.aten.sym_stride.int(add_5, 1)\n",
            "    eq_7: \"Sym(True)\" = sym_stride_int == mul_12;  sym_stride_int = None\n",
            "    or__2: \"Sym(True)\" = False | eq_7;  eq_7 = None\n",
            "    and__2: \"Sym(Eq(s12, 1))\" = and__1 & or__2;  and__1 = or__2 = None\n",
            "    mul_13: \"Sym(192*s12)\" = mul_12 * 2;  mul_12 = None\n",
            "    eq_8: \"Sym(Eq(s31, 1))\" = sym_size_int_5 == 1\n",
            "    sym_stride_int_1: \"Sym(192*s12)\" = torch.ops.aten.sym_stride.int(add_5, 0);  add_5 = None\n",
            "    eq_9: \"Sym(True)\" = sym_stride_int_1 == mul_13;  sym_stride_int_1 = None\n",
            "    or__3: \"Sym(True)\" = eq_8 | eq_9;  eq_8 = eq_9 = None\n",
            "    and__3: \"Sym(Eq(s12, 1))\" = and__2 & or__3;  and__2 = or__3 = None\n",
            "    mul_14: \"Sym(192*s12*s31)\" = mul_13 * sym_size_int_5;  mul_13 = mul_14 = None\n",
            "    or__4: \"Sym(Eq(s12, 1))\" = and__3 | eq_4;  and__3 = eq_4 = or__4 = None\n",
            "    eq_10: \"Sym(False)\" = sym_numel_default == 0;  sym_numel_default = eq_10 = None\n",
            "    eq_11: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1;  eq_11 = None\n",
            "    contiguous: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_6);  transpose_6 = None\n",
            "    view_7: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous, [sym_size_int_1, 192, sym_size_int_2]);  contiguous = sym_size_int_2 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_3: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_7, arg9_1, arg10_1);  view_7 = arg9_1 = arg10_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_1: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_3, 0.1, False);  conv1d_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_6: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(mul_3, dropout_1);  mul_3 = dropout_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_7: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_6, 1, -1);  add_6 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_7, [192], arg61_1, arg62_1);  transpose_7 = arg61_1 = arg62_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_8: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm, 1, -1);  layer_norm = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_15: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_8, type_as)\n",
            "    pad_6: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_15, [1, 1, 0, 0, 0, 0]);  mul_15 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_4: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_6, arg73_1, arg74_1);  pad_6 = arg73_1 = arg74_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_4);  conv1d_4 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_2: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu, 0.1, False);  relu = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_16: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_2, type_as);  dropout_2 = None\n",
            "    pad_7: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_16, [1, 1, 0, 0, 0, 0]);  mul_16 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_5: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_7, arg75_1, arg76_1);  pad_7 = arg75_1 = arg76_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_17: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_5, type_as);  conv1d_5 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_3: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_17, 0.1, False);  mul_17 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_7: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_8, dropout_3);  transpose_8 = dropout_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_9: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_7, 1, -1);  add_7 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_1: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_9, [192], arg97_1, arg98_1);  transpose_9 = arg97_1 = arg98_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_10: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_1, 1, -1);  layer_norm_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_6: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_10, arg13_1, arg14_1);  arg13_1 = arg14_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_7: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_10, arg15_1, arg16_1);  arg15_1 = arg16_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_8: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_10, arg17_1, arg18_1);  arg17_1 = arg18_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_6: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_6, 2)\n",
            "    view_8: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_6, [sym_size_int_1, 2, 96, sym_size_int_6]);  conv1d_6 = None\n",
            "    transpose_11: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_8, 2, 3);  view_8 = None\n",
            "    sym_size_int_7: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_7, 2)\n",
            "    view_9: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_7, [sym_size_int_1, 2, 96, sym_size_int_7]);  conv1d_7 = None\n",
            "    transpose_12: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_9, 2, 3);  view_9 = None\n",
            "    view_10: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_8, [sym_size_int_1, 2, 96, sym_size_int_7]);  conv1d_8 = None\n",
            "    transpose_13: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_10, 2, 3);  view_10 = None\n",
            "    div_2: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_11, 9.797958971132712)\n",
            "    transpose_14: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_12, -2, -1);  transpose_12 = None\n",
            "    matmul_4: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_2, transpose_14);  div_2 = transpose_14 = None\n",
            "    eq_12: \"Sym(True)\" = sym_size_int_7 == sym_size_int_6;  eq_12 = None\n",
            "    sub_11: \"Sym(s12 - 5)\" = sym_size_int_7 - 5\n",
            "    sym_max_4: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_11, 0);  sub_11 = None\n",
            "    sub_12: \"Sym(5 - s12)\" = 5 - sym_size_int_7\n",
            "    sym_max_5: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_12, 0);  sub_12 = None\n",
            "    mul_18: \"Sym(2*s12)\" = 2 * sym_size_int_7\n",
            "    add_8: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_5 + mul_18;  mul_18 = None\n",
            "    sub_13: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_8 - 1;  add_8 = None\n",
            "    gt_2: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_4 > 0;  gt_2 = None\n",
            "    pad_8: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg11_1, [0, 0, sym_max_4, sym_max_4, 0, 0]);  arg11_1 = sym_max_4 = None\n",
            "    slice_8: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_8, 1, sym_max_5, sub_13);  pad_8 = sym_max_5 = sub_13 = None\n",
            "    div_3: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_11, 9.797958971132712);  transpose_11 = None\n",
            "    unsqueeze_7: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_8, 0);  slice_8 = None\n",
            "    transpose_15: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_7, -2, -1);  unsqueeze_7 = None\n",
            "    matmul_5: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_3, transpose_15);  div_3 = transpose_15 = None\n",
            "    pad_9: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_5, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_5 = None\n",
            "    mul_19: \"Sym(2*s12)\" = sym_size_int_6 * 2\n",
            "    mul_20: \"Sym(2*s12**2)\" = mul_19 * sym_size_int_6;  mul_19 = None\n",
            "    view_11: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_9, [sym_size_int_1, 2, mul_20]);  pad_9 = mul_20 = None\n",
            "    sub_14: \"Sym(s12 - 1)\" = sym_size_int_6 - 1\n",
            "    pad_10: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_11, [0, sub_14, 0, 0, 0, 0]);  view_11 = sub_14 = None\n",
            "    add_9: \"Sym(s12 + 1)\" = sym_size_int_6 + 1\n",
            "    mul_21: \"Sym(2*s12)\" = 2 * sym_size_int_6\n",
            "    sub_15: \"Sym(2*s12 - 1)\" = mul_21 - 1;  mul_21 = None\n",
            "    view_12: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_10, [sym_size_int_1, 2, add_9, sub_15]);  pad_10 = add_9 = sub_15 = None\n",
            "    sub_16: \"Sym(s12 - 1)\" = sym_size_int_6 - 1\n",
            "    slice_9: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_12, 2, None, sym_size_int_6);  view_12 = None\n",
            "    slice_10: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_9, 3, sub_16);  slice_9 = sub_16 = None\n",
            "    add_10: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_4, slice_10);  matmul_4 = slice_10 = None\n",
            "    eq_13: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill_1: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_10, eq_13, -10000.0);  add_10 = eq_13 = None\n",
            "    softmax_1: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_1, -1);  masked_fill_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_4: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_1, 0.1, False);  softmax_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_6: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_4, transpose_13);  transpose_13 = None\n",
            "    sub_17: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_11: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_4, [0, sub_17, 0, 0, 0, 0, 0, 0]);  dropout_4 = sub_17 = None\n",
            "    mul_22: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_18: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_23: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_18;  sub_18 = None\n",
            "    add_11: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_22 + mul_23;  mul_22 = mul_23 = None\n",
            "    view_13: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_11, [sym_size_int_5, 2, add_11]);  pad_11 = add_11 = None\n",
            "    pad_12: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_13, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_13 = None\n",
            "    mul_24: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_14: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_12, [sym_size_int_5, 2, sym_size_int_4, mul_24]);  pad_12 = mul_24 = None\n",
            "    le_2: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le_2 = None\n",
            "    slice_11: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_14, 0, 0, 9223372036854775807);  view_14 = None\n",
            "    le_3: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_3 = None\n",
            "    slice_12: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_11, 2, 0, 9223372036854775807);  slice_11 = None\n",
            "    slice_13: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_12, 3, 1, 9223372036854775807);  slice_12 = None\n",
            "    sub_19: \"Sym(s12 - 5)\" = sym_size_int_7 - 5\n",
            "    sym_max_6: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_19, 0);  sub_19 = None\n",
            "    sub_20: \"Sym(5 - s12)\" = 5 - sym_size_int_7\n",
            "    sym_max_7: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_20, 0);  sub_20 = None\n",
            "    mul_25: \"Sym(2*s12)\" = 2 * sym_size_int_7;  sym_size_int_7 = None\n",
            "    add_12: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_7 + mul_25;  mul_25 = None\n",
            "    sub_21: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_12 - 1;  add_12 = None\n",
            "    gt_3: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_6 > 0;  gt_3 = None\n",
            "    pad_13: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg12_1, [0, 0, sym_max_6, sym_max_6, 0, 0]);  arg12_1 = sym_max_6 = None\n",
            "    slice_14: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_13, 1, sym_max_7, sub_21);  pad_13 = sym_max_7 = sub_21 = None\n",
            "    unsqueeze_8: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_14, 0);  slice_14 = None\n",
            "    matmul_7: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_13, unsqueeze_8);  slice_13 = unsqueeze_8 = None\n",
            "    add_13: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_6, matmul_7);  matmul_6 = matmul_7 = None\n",
            "    transpose_16: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_13, 2, 3);  add_13 = None\n",
            "    contiguous_1: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_16);  transpose_16 = None\n",
            "    view_15: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_1, [sym_size_int_1, 192, sym_size_int_6]);  contiguous_1 = sym_size_int_6 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_9: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_15, arg19_1, arg20_1);  view_15 = arg19_1 = arg20_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_5: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_9, 0.1, False);  conv1d_9 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_14: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_10, dropout_5);  transpose_10 = dropout_5 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_17: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_14, 1, -1);  add_14 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_2: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_17, [192], arg63_1, arg64_1);  transpose_17 = arg63_1 = arg64_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_18: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_2, 1, -1);  layer_norm_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_26: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_18, type_as)\n",
            "    pad_14: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_26, [1, 1, 0, 0, 0, 0]);  mul_26 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_10: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_14, arg77_1, arg78_1);  pad_14 = arg77_1 = arg78_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_1: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_10);  conv1d_10 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_6: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_1, 0.1, False);  relu_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_27: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_6, type_as);  dropout_6 = None\n",
            "    pad_15: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_27, [1, 1, 0, 0, 0, 0]);  mul_27 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_11: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_15, arg79_1, arg80_1);  pad_15 = arg79_1 = arg80_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_28: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_11, type_as);  conv1d_11 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_7: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_28, 0.1, False);  mul_28 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_15: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_18, dropout_7);  transpose_18 = dropout_7 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_19: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_15, 1, -1);  add_15 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_3: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_19, [192], arg99_1, arg100_1);  transpose_19 = arg99_1 = arg100_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_20: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_3, 1, -1);  layer_norm_3 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_12: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_20, arg23_1, arg24_1);  arg23_1 = arg24_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_13: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_20, arg25_1, arg26_1);  arg25_1 = arg26_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_14: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_20, arg27_1, arg28_1);  arg27_1 = arg28_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_8: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_12, 2)\n",
            "    view_16: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_12, [sym_size_int_1, 2, 96, sym_size_int_8]);  conv1d_12 = None\n",
            "    transpose_21: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_16, 2, 3);  view_16 = None\n",
            "    sym_size_int_9: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_13, 2)\n",
            "    view_17: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_13, [sym_size_int_1, 2, 96, sym_size_int_9]);  conv1d_13 = None\n",
            "    transpose_22: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_17, 2, 3);  view_17 = None\n",
            "    view_18: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_14, [sym_size_int_1, 2, 96, sym_size_int_9]);  conv1d_14 = None\n",
            "    transpose_23: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_18, 2, 3);  view_18 = None\n",
            "    div_4: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_21, 9.797958971132712)\n",
            "    transpose_24: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_22, -2, -1);  transpose_22 = None\n",
            "    matmul_8: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_4, transpose_24);  div_4 = transpose_24 = None\n",
            "    eq_14: \"Sym(True)\" = sym_size_int_9 == sym_size_int_8;  eq_14 = None\n",
            "    sub_22: \"Sym(s12 - 5)\" = sym_size_int_9 - 5\n",
            "    sym_max_8: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_22, 0);  sub_22 = None\n",
            "    sub_23: \"Sym(5 - s12)\" = 5 - sym_size_int_9\n",
            "    sym_max_9: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_23, 0);  sub_23 = None\n",
            "    mul_29: \"Sym(2*s12)\" = 2 * sym_size_int_9\n",
            "    add_16: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_9 + mul_29;  mul_29 = None\n",
            "    sub_24: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_16 - 1;  add_16 = None\n",
            "    gt_4: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_8 > 0;  gt_4 = None\n",
            "    pad_16: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg21_1, [0, 0, sym_max_8, sym_max_8, 0, 0]);  arg21_1 = sym_max_8 = None\n",
            "    slice_15: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_16, 1, sym_max_9, sub_24);  pad_16 = sym_max_9 = sub_24 = None\n",
            "    div_5: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_21, 9.797958971132712);  transpose_21 = None\n",
            "    unsqueeze_9: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_15, 0);  slice_15 = None\n",
            "    transpose_25: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_9, -2, -1);  unsqueeze_9 = None\n",
            "    matmul_9: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_5, transpose_25);  div_5 = transpose_25 = None\n",
            "    pad_17: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_9, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_9 = None\n",
            "    mul_30: \"Sym(2*s12)\" = sym_size_int_8 * 2\n",
            "    mul_31: \"Sym(2*s12**2)\" = mul_30 * sym_size_int_8;  mul_30 = None\n",
            "    view_19: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_17, [sym_size_int_1, 2, mul_31]);  pad_17 = mul_31 = None\n",
            "    sub_25: \"Sym(s12 - 1)\" = sym_size_int_8 - 1\n",
            "    pad_18: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_19, [0, sub_25, 0, 0, 0, 0]);  view_19 = sub_25 = None\n",
            "    add_17: \"Sym(s12 + 1)\" = sym_size_int_8 + 1\n",
            "    mul_32: \"Sym(2*s12)\" = 2 * sym_size_int_8\n",
            "    sub_26: \"Sym(2*s12 - 1)\" = mul_32 - 1;  mul_32 = None\n",
            "    view_20: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_18, [sym_size_int_1, 2, add_17, sub_26]);  pad_18 = add_17 = sub_26 = None\n",
            "    sub_27: \"Sym(s12 - 1)\" = sym_size_int_8 - 1\n",
            "    slice_16: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_20, 2, None, sym_size_int_8);  view_20 = None\n",
            "    slice_17: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_16, 3, sub_27);  slice_16 = sub_27 = None\n",
            "    add_18: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_8, slice_17);  matmul_8 = slice_17 = None\n",
            "    eq_15: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill_2: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_18, eq_15, -10000.0);  add_18 = eq_15 = None\n",
            "    softmax_2: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_2, -1);  masked_fill_2 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_8: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_2, 0.1, False);  softmax_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_10: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_8, transpose_23);  transpose_23 = None\n",
            "    sub_28: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_19: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_8, [0, sub_28, 0, 0, 0, 0, 0, 0]);  dropout_8 = sub_28 = None\n",
            "    mul_33: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_29: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_34: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_29;  sub_29 = None\n",
            "    add_19: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_33 + mul_34;  mul_33 = mul_34 = None\n",
            "    view_21: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_19, [sym_size_int_5, 2, add_19]);  pad_19 = add_19 = None\n",
            "    pad_20: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_21, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_21 = None\n",
            "    mul_35: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_22: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_20, [sym_size_int_5, 2, sym_size_int_4, mul_35]);  pad_20 = mul_35 = None\n",
            "    le_4: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le_4 = None\n",
            "    slice_18: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_22, 0, 0, 9223372036854775807);  view_22 = None\n",
            "    le_5: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_5 = None\n",
            "    slice_19: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_18, 2, 0, 9223372036854775807);  slice_18 = None\n",
            "    slice_20: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_19, 3, 1, 9223372036854775807);  slice_19 = None\n",
            "    sub_30: \"Sym(s12 - 5)\" = sym_size_int_9 - 5\n",
            "    sym_max_10: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_30, 0);  sub_30 = None\n",
            "    sub_31: \"Sym(5 - s12)\" = 5 - sym_size_int_9\n",
            "    sym_max_11: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_31, 0);  sub_31 = None\n",
            "    mul_36: \"Sym(2*s12)\" = 2 * sym_size_int_9;  sym_size_int_9 = None\n",
            "    add_20: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_11 + mul_36;  mul_36 = None\n",
            "    sub_32: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_20 - 1;  add_20 = None\n",
            "    gt_5: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_10 > 0;  gt_5 = None\n",
            "    pad_21: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg22_1, [0, 0, sym_max_10, sym_max_10, 0, 0]);  arg22_1 = sym_max_10 = None\n",
            "    slice_21: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_21, 1, sym_max_11, sub_32);  pad_21 = sym_max_11 = sub_32 = None\n",
            "    unsqueeze_10: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_21, 0);  slice_21 = None\n",
            "    matmul_11: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_20, unsqueeze_10);  slice_20 = unsqueeze_10 = None\n",
            "    add_21: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_10, matmul_11);  matmul_10 = matmul_11 = None\n",
            "    transpose_26: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_21, 2, 3);  add_21 = None\n",
            "    contiguous_2: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_26);  transpose_26 = None\n",
            "    view_23: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_2, [sym_size_int_1, 192, sym_size_int_8]);  contiguous_2 = sym_size_int_8 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_15: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_23, arg29_1, arg30_1);  view_23 = arg29_1 = arg30_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_9: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_15, 0.1, False);  conv1d_15 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_22: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_20, dropout_9);  transpose_20 = dropout_9 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_27: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_22, 1, -1);  add_22 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_4: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_27, [192], arg65_1, arg66_1);  transpose_27 = arg65_1 = arg66_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_28: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_4, 1, -1);  layer_norm_4 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_37: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_28, type_as)\n",
            "    pad_22: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_37, [1, 1, 0, 0, 0, 0]);  mul_37 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_16: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_22, arg81_1, arg82_1);  pad_22 = arg81_1 = arg82_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_2: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_16);  conv1d_16 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_10: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_2, 0.1, False);  relu_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_38: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_10, type_as);  dropout_10 = None\n",
            "    pad_23: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_38, [1, 1, 0, 0, 0, 0]);  mul_38 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_17: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_23, arg83_1, arg84_1);  pad_23 = arg83_1 = arg84_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_39: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_17, type_as);  conv1d_17 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_11: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_39, 0.1, False);  mul_39 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_23: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_28, dropout_11);  transpose_28 = dropout_11 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_29: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_23, 1, -1);  add_23 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_5: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_29, [192], arg101_1, arg102_1);  transpose_29 = arg101_1 = arg102_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_30: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_5, 1, -1);  layer_norm_5 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_18: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_30, arg33_1, arg34_1);  arg33_1 = arg34_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_19: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_30, arg35_1, arg36_1);  arg35_1 = arg36_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_20: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_30, arg37_1, arg38_1);  arg37_1 = arg38_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_10: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_18, 2)\n",
            "    view_24: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_18, [sym_size_int_1, 2, 96, sym_size_int_10]);  conv1d_18 = None\n",
            "    transpose_31: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_24, 2, 3);  view_24 = None\n",
            "    sym_size_int_11: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_19, 2)\n",
            "    view_25: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_19, [sym_size_int_1, 2, 96, sym_size_int_11]);  conv1d_19 = None\n",
            "    transpose_32: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_25, 2, 3);  view_25 = None\n",
            "    view_26: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_20, [sym_size_int_1, 2, 96, sym_size_int_11]);  conv1d_20 = None\n",
            "    transpose_33: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_26, 2, 3);  view_26 = None\n",
            "    div_6: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_31, 9.797958971132712)\n",
            "    transpose_34: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_32, -2, -1);  transpose_32 = None\n",
            "    matmul_12: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_6, transpose_34);  div_6 = transpose_34 = None\n",
            "    eq_16: \"Sym(True)\" = sym_size_int_11 == sym_size_int_10;  eq_16 = None\n",
            "    sub_33: \"Sym(s12 - 5)\" = sym_size_int_11 - 5\n",
            "    sym_max_12: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_33, 0);  sub_33 = None\n",
            "    sub_34: \"Sym(5 - s12)\" = 5 - sym_size_int_11\n",
            "    sym_max_13: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_34, 0);  sub_34 = None\n",
            "    mul_40: \"Sym(2*s12)\" = 2 * sym_size_int_11\n",
            "    add_24: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_13 + mul_40;  mul_40 = None\n",
            "    sub_35: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_24 - 1;  add_24 = None\n",
            "    gt_6: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_12 > 0;  gt_6 = None\n",
            "    pad_24: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg31_1, [0, 0, sym_max_12, sym_max_12, 0, 0]);  arg31_1 = sym_max_12 = None\n",
            "    slice_22: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_24, 1, sym_max_13, sub_35);  pad_24 = sym_max_13 = sub_35 = None\n",
            "    div_7: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_31, 9.797958971132712);  transpose_31 = None\n",
            "    unsqueeze_11: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_22, 0);  slice_22 = None\n",
            "    transpose_35: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_11, -2, -1);  unsqueeze_11 = None\n",
            "    matmul_13: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_7, transpose_35);  div_7 = transpose_35 = None\n",
            "    pad_25: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_13, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_13 = None\n",
            "    mul_41: \"Sym(2*s12)\" = sym_size_int_10 * 2\n",
            "    mul_42: \"Sym(2*s12**2)\" = mul_41 * sym_size_int_10;  mul_41 = None\n",
            "    view_27: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_25, [sym_size_int_1, 2, mul_42]);  pad_25 = mul_42 = None\n",
            "    sub_36: \"Sym(s12 - 1)\" = sym_size_int_10 - 1\n",
            "    pad_26: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_27, [0, sub_36, 0, 0, 0, 0]);  view_27 = sub_36 = None\n",
            "    add_25: \"Sym(s12 + 1)\" = sym_size_int_10 + 1\n",
            "    mul_43: \"Sym(2*s12)\" = 2 * sym_size_int_10\n",
            "    sub_37: \"Sym(2*s12 - 1)\" = mul_43 - 1;  mul_43 = None\n",
            "    view_28: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_26, [sym_size_int_1, 2, add_25, sub_37]);  pad_26 = add_25 = sub_37 = None\n",
            "    sub_38: \"Sym(s12 - 1)\" = sym_size_int_10 - 1\n",
            "    slice_23: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_28, 2, None, sym_size_int_10);  view_28 = None\n",
            "    slice_24: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_23, 3, sub_38);  slice_23 = sub_38 = None\n",
            "    add_26: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_12, slice_24);  matmul_12 = slice_24 = None\n",
            "    eq_17: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill_3: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_26, eq_17, -10000.0);  add_26 = eq_17 = None\n",
            "    softmax_3: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_3, -1);  masked_fill_3 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_12: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_3, 0.1, False);  softmax_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_14: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_12, transpose_33);  transpose_33 = None\n",
            "    sub_39: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_27: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_12, [0, sub_39, 0, 0, 0, 0, 0, 0]);  dropout_12 = sub_39 = None\n",
            "    mul_44: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_40: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_45: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_40;  sub_40 = None\n",
            "    add_27: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_44 + mul_45;  mul_44 = mul_45 = None\n",
            "    view_29: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_27, [sym_size_int_5, 2, add_27]);  pad_27 = add_27 = None\n",
            "    pad_28: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_29, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_29 = None\n",
            "    mul_46: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_30: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_28, [sym_size_int_5, 2, sym_size_int_4, mul_46]);  pad_28 = mul_46 = None\n",
            "    le_6: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le_6 = None\n",
            "    slice_25: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_30, 0, 0, 9223372036854775807);  view_30 = None\n",
            "    le_7: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_7 = None\n",
            "    slice_26: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_25, 2, 0, 9223372036854775807);  slice_25 = None\n",
            "    slice_27: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_26, 3, 1, 9223372036854775807);  slice_26 = None\n",
            "    sub_41: \"Sym(s12 - 5)\" = sym_size_int_11 - 5\n",
            "    sym_max_14: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_41, 0);  sub_41 = None\n",
            "    sub_42: \"Sym(5 - s12)\" = 5 - sym_size_int_11\n",
            "    sym_max_15: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_42, 0);  sub_42 = None\n",
            "    mul_47: \"Sym(2*s12)\" = 2 * sym_size_int_11;  sym_size_int_11 = None\n",
            "    add_28: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_15 + mul_47;  mul_47 = None\n",
            "    sub_43: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_28 - 1;  add_28 = None\n",
            "    gt_7: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_14 > 0;  gt_7 = None\n",
            "    pad_29: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg32_1, [0, 0, sym_max_14, sym_max_14, 0, 0]);  arg32_1 = sym_max_14 = None\n",
            "    slice_28: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_29, 1, sym_max_15, sub_43);  pad_29 = sym_max_15 = sub_43 = None\n",
            "    unsqueeze_12: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_28, 0);  slice_28 = None\n",
            "    matmul_15: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_27, unsqueeze_12);  slice_27 = unsqueeze_12 = None\n",
            "    add_29: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_14, matmul_15);  matmul_14 = matmul_15 = None\n",
            "    transpose_36: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_29, 2, 3)\n",
            "    sym_numel_default_1: \"Sym(192*s12*s31)\" = torch.ops.aten.sym_numel.default(transpose_36)\n",
            "    eq_18: \"Sym(False)\" = sym_numel_default_1 == 0;  eq_18 = None\n",
            "    eq_19: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1;  eq_19 = None\n",
            "    eq_20: \"Sym(False)\" = sym_numel_default_1 == 0\n",
            "    eq_21: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1\n",
            "    or__5: \"Sym(Eq(s12, 1))\" = eq_21 | False;  eq_21 = None\n",
            "    and__4: \"Sym(Eq(s12, 1))\" = True & or__5;  or__5 = None\n",
            "    eq_22: \"Sym(Eq(s12, 1))\" = 1 == sym_size_int_4\n",
            "    or__6: \"Sym(Eq(s12, 1))\" = False | eq_22;  eq_22 = None\n",
            "    and__5: \"Sym(Eq(s12, 1))\" = and__4 & or__6;  and__4 = or__6 = None\n",
            "    mul_48: \"Sym(96*s12)\" = sym_size_int_4 * 96\n",
            "    sym_stride_int_2: \"Sym(96*s12)\" = torch.ops.aten.sym_stride.int(add_29, 1)\n",
            "    eq_23: \"Sym(True)\" = sym_stride_int_2 == mul_48;  sym_stride_int_2 = None\n",
            "    or__7: \"Sym(True)\" = False | eq_23;  eq_23 = None\n",
            "    and__6: \"Sym(Eq(s12, 1))\" = and__5 & or__7;  and__5 = or__7 = None\n",
            "    mul_49: \"Sym(192*s12)\" = mul_48 * 2;  mul_48 = None\n",
            "    eq_24: \"Sym(Eq(s31, 1))\" = sym_size_int_5 == 1\n",
            "    sym_stride_int_3: \"Sym(192*s12)\" = torch.ops.aten.sym_stride.int(add_29, 0);  add_29 = None\n",
            "    eq_25: \"Sym(True)\" = sym_stride_int_3 == mul_49;  sym_stride_int_3 = None\n",
            "    or__8: \"Sym(True)\" = eq_24 | eq_25;  eq_24 = eq_25 = None\n",
            "    and__7: \"Sym(Eq(s12, 1))\" = and__6 & or__8;  and__6 = or__8 = None\n",
            "    mul_50: \"Sym(192*s12*s31)\" = mul_49 * sym_size_int_5;  mul_49 = mul_50 = None\n",
            "    or__9: \"Sym(Eq(s12, 1))\" = and__7 | eq_20;  and__7 = eq_20 = or__9 = None\n",
            "    eq_26: \"Sym(False)\" = sym_numel_default_1 == 0;  sym_numel_default_1 = eq_26 = None\n",
            "    eq_27: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1;  eq_27 = None\n",
            "    contiguous_3: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_36);  transpose_36 = None\n",
            "    view_31: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_3, [sym_size_int_1, 192, sym_size_int_10]);  contiguous_3 = sym_size_int_10 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_21: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_31, arg39_1, arg40_1);  view_31 = arg39_1 = arg40_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_13: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_21, 0.1, False);  conv1d_21 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_30: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_30, dropout_13);  transpose_30 = dropout_13 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_37: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_30, 1, -1);  add_30 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_6: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_37, [192], arg67_1, arg68_1);  transpose_37 = arg67_1 = arg68_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_38: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_6, 1, -1);  layer_norm_6 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_51: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_38, type_as)\n",
            "    pad_30: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_51, [1, 1, 0, 0, 0, 0]);  mul_51 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_22: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_30, arg85_1, arg86_1);  pad_30 = arg85_1 = arg86_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_3: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_22);  conv1d_22 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_14: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_3, 0.1, False);  relu_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_52: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_14, type_as);  dropout_14 = None\n",
            "    pad_31: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_52, [1, 1, 0, 0, 0, 0]);  mul_52 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_23: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_31, arg87_1, arg88_1);  pad_31 = arg87_1 = arg88_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_53: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_23, type_as);  conv1d_23 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_15: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_53, 0.1, False);  mul_53 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_31: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_38, dropout_15);  transpose_38 = dropout_15 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_39: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_31, 1, -1);  add_31 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_7: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_39, [192], arg103_1, arg104_1);  transpose_39 = arg103_1 = arg104_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_40: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_7, 1, -1);  layer_norm_7 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_24: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_40, arg43_1, arg44_1);  arg43_1 = arg44_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_25: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_40, arg45_1, arg46_1);  arg45_1 = arg46_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_26: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_40, arg47_1, arg48_1);  arg47_1 = arg48_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_12: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_24, 2)\n",
            "    view_32: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_24, [sym_size_int_1, 2, 96, sym_size_int_12]);  conv1d_24 = None\n",
            "    transpose_41: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_32, 2, 3);  view_32 = None\n",
            "    sym_size_int_13: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_25, 2)\n",
            "    view_33: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_25, [sym_size_int_1, 2, 96, sym_size_int_13]);  conv1d_25 = None\n",
            "    transpose_42: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_33, 2, 3);  view_33 = None\n",
            "    view_34: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_26, [sym_size_int_1, 2, 96, sym_size_int_13]);  conv1d_26 = None\n",
            "    transpose_43: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_34, 2, 3);  view_34 = None\n",
            "    div_8: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_41, 9.797958971132712)\n",
            "    transpose_44: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_42, -2, -1);  transpose_42 = None\n",
            "    matmul_16: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_8, transpose_44);  div_8 = transpose_44 = None\n",
            "    eq_28: \"Sym(True)\" = sym_size_int_13 == sym_size_int_12;  eq_28 = None\n",
            "    sub_44: \"Sym(s12 - 5)\" = sym_size_int_13 - 5\n",
            "    sym_max_16: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_44, 0);  sub_44 = None\n",
            "    sub_45: \"Sym(5 - s12)\" = 5 - sym_size_int_13\n",
            "    sym_max_17: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_45, 0);  sub_45 = None\n",
            "    mul_54: \"Sym(2*s12)\" = 2 * sym_size_int_13\n",
            "    add_32: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_17 + mul_54;  mul_54 = None\n",
            "    sub_46: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_32 - 1;  add_32 = None\n",
            "    gt_8: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_16 > 0;  gt_8 = None\n",
            "    pad_32: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg41_1, [0, 0, sym_max_16, sym_max_16, 0, 0]);  arg41_1 = sym_max_16 = None\n",
            "    slice_29: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_32, 1, sym_max_17, sub_46);  pad_32 = sym_max_17 = sub_46 = None\n",
            "    div_9: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_41, 9.797958971132712);  transpose_41 = None\n",
            "    unsqueeze_13: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_29, 0);  slice_29 = None\n",
            "    transpose_45: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_13, -2, -1);  unsqueeze_13 = None\n",
            "    matmul_17: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_9, transpose_45);  div_9 = transpose_45 = None\n",
            "    pad_33: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_17, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_17 = None\n",
            "    mul_55: \"Sym(2*s12)\" = sym_size_int_12 * 2\n",
            "    mul_56: \"Sym(2*s12**2)\" = mul_55 * sym_size_int_12;  mul_55 = None\n",
            "    view_35: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_33, [sym_size_int_1, 2, mul_56]);  pad_33 = mul_56 = None\n",
            "    sub_47: \"Sym(s12 - 1)\" = sym_size_int_12 - 1\n",
            "    pad_34: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_35, [0, sub_47, 0, 0, 0, 0]);  view_35 = sub_47 = None\n",
            "    add_33: \"Sym(s12 + 1)\" = sym_size_int_12 + 1\n",
            "    mul_57: \"Sym(2*s12)\" = 2 * sym_size_int_12\n",
            "    sub_48: \"Sym(2*s12 - 1)\" = mul_57 - 1;  mul_57 = None\n",
            "    view_36: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_34, [sym_size_int_1, 2, add_33, sub_48]);  pad_34 = add_33 = sub_48 = None\n",
            "    sub_49: \"Sym(s12 - 1)\" = sym_size_int_12 - 1\n",
            "    slice_30: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_36, 2, None, sym_size_int_12);  view_36 = None\n",
            "    slice_31: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_30, 3, sub_49);  slice_30 = sub_49 = None\n",
            "    add_34: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_16, slice_31);  matmul_16 = slice_31 = None\n",
            "    eq_29: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill_4: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_34, eq_29, -10000.0);  add_34 = eq_29 = None\n",
            "    softmax_4: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_4, -1);  masked_fill_4 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_16: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_4, 0.1, False);  softmax_4 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_18: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_16, transpose_43);  transpose_43 = None\n",
            "    sub_50: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_35: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_16, [0, sub_50, 0, 0, 0, 0, 0, 0]);  dropout_16 = sub_50 = None\n",
            "    mul_58: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_51: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_59: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_51;  sub_51 = None\n",
            "    add_35: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_58 + mul_59;  mul_58 = mul_59 = None\n",
            "    view_37: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_35, [sym_size_int_5, 2, add_35]);  pad_35 = add_35 = None\n",
            "    pad_36: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_37, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_37 = None\n",
            "    mul_60: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_38: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_36, [sym_size_int_5, 2, sym_size_int_4, mul_60]);  pad_36 = mul_60 = None\n",
            "    le_8: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le_8 = None\n",
            "    slice_32: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_38, 0, 0, 9223372036854775807);  view_38 = None\n",
            "    le_9: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_9 = None\n",
            "    slice_33: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_32, 2, 0, 9223372036854775807);  slice_32 = None\n",
            "    slice_34: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_33, 3, 1, 9223372036854775807);  slice_33 = None\n",
            "    sub_52: \"Sym(s12 - 5)\" = sym_size_int_13 - 5\n",
            "    sym_max_18: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_52, 0);  sub_52 = None\n",
            "    sub_53: \"Sym(5 - s12)\" = 5 - sym_size_int_13\n",
            "    sym_max_19: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_53, 0);  sub_53 = None\n",
            "    mul_61: \"Sym(2*s12)\" = 2 * sym_size_int_13;  sym_size_int_13 = None\n",
            "    add_36: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_19 + mul_61;  mul_61 = None\n",
            "    sub_54: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_36 - 1;  add_36 = None\n",
            "    gt_9: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_18 > 0;  gt_9 = None\n",
            "    pad_37: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg42_1, [0, 0, sym_max_18, sym_max_18, 0, 0]);  arg42_1 = sym_max_18 = None\n",
            "    slice_35: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_37, 1, sym_max_19, sub_54);  pad_37 = sym_max_19 = sub_54 = None\n",
            "    unsqueeze_14: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_35, 0);  slice_35 = None\n",
            "    matmul_19: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_34, unsqueeze_14);  slice_34 = unsqueeze_14 = None\n",
            "    add_37: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_18, matmul_19);  matmul_18 = matmul_19 = None\n",
            "    transpose_46: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_37, 2, 3);  add_37 = None\n",
            "    contiguous_4: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_46);  transpose_46 = None\n",
            "    view_39: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_4, [sym_size_int_1, 192, sym_size_int_12]);  contiguous_4 = sym_size_int_12 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_27: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_39, arg49_1, arg50_1);  view_39 = arg49_1 = arg50_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_17: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_27, 0.1, False);  conv1d_27 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_38: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_40, dropout_17);  transpose_40 = dropout_17 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_47: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_38, 1, -1);  add_38 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_8: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_47, [192], arg69_1, arg70_1);  transpose_47 = arg69_1 = arg70_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_48: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_8, 1, -1);  layer_norm_8 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_62: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_48, type_as)\n",
            "    pad_38: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_62, [1, 1, 0, 0, 0, 0]);  mul_62 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_28: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_38, arg89_1, arg90_1);  pad_38 = arg89_1 = arg90_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_4: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_28);  conv1d_28 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_18: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_4, 0.1, False);  relu_4 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_63: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_18, type_as);  dropout_18 = None\n",
            "    pad_39: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_63, [1, 1, 0, 0, 0, 0]);  mul_63 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_29: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_39, arg91_1, arg92_1);  pad_39 = arg91_1 = arg92_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_64: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_29, type_as);  conv1d_29 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_19: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_64, 0.1, False);  mul_64 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_39: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_48, dropout_19);  transpose_48 = dropout_19 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_49: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_39, 1, -1);  add_39 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_9: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_49, [192], arg105_1, arg106_1);  transpose_49 = arg105_1 = arg106_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_50: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_9, 1, -1);  layer_norm_9 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_30: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_50, arg53_1, arg54_1);  arg53_1 = arg54_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_31: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_50, arg55_1, arg56_1);  arg55_1 = arg56_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_32: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_50, arg57_1, arg58_1);  arg57_1 = arg58_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_14: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_30, 2)\n",
            "    view_40: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_30, [sym_size_int_1, 2, 96, sym_size_int_14]);  conv1d_30 = None\n",
            "    transpose_51: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_40, 2, 3);  view_40 = None\n",
            "    sym_size_int_15: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_31, 2)\n",
            "    view_41: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_31, [sym_size_int_1, 2, 96, sym_size_int_15]);  conv1d_31 = None\n",
            "    transpose_52: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_41, 2, 3);  view_41 = None\n",
            "    view_42: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_32, [sym_size_int_1, 2, 96, sym_size_int_15]);  conv1d_32 = None\n",
            "    transpose_53: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_42, 2, 3);  view_42 = None\n",
            "    div_10: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_51, 9.797958971132712)\n",
            "    transpose_54: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_52, -2, -1);  transpose_52 = None\n",
            "    matmul_20: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_10, transpose_54);  div_10 = transpose_54 = None\n",
            "    eq_30: \"Sym(True)\" = sym_size_int_15 == sym_size_int_14;  eq_30 = None\n",
            "    sub_55: \"Sym(s12 - 5)\" = sym_size_int_15 - 5\n",
            "    sym_max_20: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_55, 0);  sub_55 = None\n",
            "    sub_56: \"Sym(5 - s12)\" = 5 - sym_size_int_15\n",
            "    sym_max_21: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_56, 0);  sub_56 = None\n",
            "    mul_65: \"Sym(2*s12)\" = 2 * sym_size_int_15\n",
            "    add_40: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_21 + mul_65;  mul_65 = None\n",
            "    sub_57: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_40 - 1;  add_40 = None\n",
            "    gt_10: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_20 > 0;  gt_10 = None\n",
            "    pad_40: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg51_1, [0, 0, sym_max_20, sym_max_20, 0, 0]);  arg51_1 = sym_max_20 = None\n",
            "    slice_36: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_40, 1, sym_max_21, sub_57);  pad_40 = sym_max_21 = sub_57 = None\n",
            "    div_11: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_51, 9.797958971132712);  transpose_51 = None\n",
            "    unsqueeze_15: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_36, 0);  slice_36 = None\n",
            "    transpose_55: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_15, -2, -1);  unsqueeze_15 = None\n",
            "    matmul_21: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_11, transpose_55);  div_11 = transpose_55 = None\n",
            "    pad_41: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_21, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_21 = None\n",
            "    mul_66: \"Sym(2*s12)\" = sym_size_int_14 * 2\n",
            "    mul_67: \"Sym(2*s12**2)\" = mul_66 * sym_size_int_14;  mul_66 = None\n",
            "    view_43: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_41, [sym_size_int_1, 2, mul_67]);  pad_41 = mul_67 = None\n",
            "    sub_58: \"Sym(s12 - 1)\" = sym_size_int_14 - 1\n",
            "    pad_42: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_43, [0, sub_58, 0, 0, 0, 0]);  view_43 = sub_58 = None\n",
            "    add_41: \"Sym(s12 + 1)\" = sym_size_int_14 + 1\n",
            "    mul_68: \"Sym(2*s12)\" = 2 * sym_size_int_14\n",
            "    sub_59: \"Sym(2*s12 - 1)\" = mul_68 - 1;  mul_68 = None\n",
            "    view_44: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_42, [sym_size_int_1, 2, add_41, sub_59]);  pad_42 = add_41 = sub_59 = None\n",
            "    sub_60: \"Sym(s12 - 1)\" = sym_size_int_14 - 1\n",
            "    slice_37: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_44, 2, None, sym_size_int_14);  view_44 = None\n",
            "    slice_38: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_37, 3, sub_60);  slice_37 = sub_60 = None\n",
            "    add_42: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_20, slice_38);  matmul_20 = slice_38 = None\n",
            "    eq_31: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0);  mul_2 = None\n",
            "    masked_fill_5: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_42, eq_31, -10000.0);  add_42 = eq_31 = None\n",
            "    softmax_5: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_5, -1);  masked_fill_5 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_20: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_5, 0.1, False);  softmax_5 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_22: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_20, transpose_53);  transpose_53 = None\n",
            "    sub_61: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_43: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_20, [0, sub_61, 0, 0, 0, 0, 0, 0]);  dropout_20 = sub_61 = None\n",
            "    mul_69: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_62: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_70: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_62;  sub_62 = None\n",
            "    add_43: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_69 + mul_70;  mul_69 = mul_70 = None\n",
            "    view_45: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_43, [sym_size_int_5, 2, add_43]);  pad_43 = add_43 = None\n",
            "    pad_44: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_45, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_45 = None\n",
            "    mul_71: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_46: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_44, [sym_size_int_5, 2, sym_size_int_4, mul_71]);  pad_44 = mul_71 = None\n",
            "    le_10: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  sym_size_int_5 = le_10 = None\n",
            "    slice_39: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_46, 0, 0, 9223372036854775807);  view_46 = None\n",
            "    le_11: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  sym_size_int_4 = le_11 = None\n",
            "    slice_40: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_39, 2, 0, 9223372036854775807);  slice_39 = None\n",
            "    slice_41: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_40, 3, 1, 9223372036854775807);  slice_40 = None\n",
            "    sub_63: \"Sym(s12 - 5)\" = sym_size_int_15 - 5\n",
            "    sym_max_22: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_63, 0);  sub_63 = None\n",
            "    sub_64: \"Sym(5 - s12)\" = 5 - sym_size_int_15\n",
            "    sym_max_23: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_64, 0);  sub_64 = None\n",
            "    mul_72: \"Sym(2*s12)\" = 2 * sym_size_int_15;  sym_size_int_15 = None\n",
            "    add_44: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_23 + mul_72;  mul_72 = None\n",
            "    sub_65: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_44 - 1;  add_44 = None\n",
            "    gt_11: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_22 > 0;  gt_11 = None\n",
            "    pad_45: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg52_1, [0, 0, sym_max_22, sym_max_22, 0, 0]);  arg52_1 = sym_max_22 = None\n",
            "    slice_42: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_45, 1, sym_max_23, sub_65);  pad_45 = sym_max_23 = sub_65 = None\n",
            "    unsqueeze_16: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_42, 0);  slice_42 = None\n",
            "    matmul_23: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_41, unsqueeze_16);  slice_41 = unsqueeze_16 = None\n",
            "    add_45: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_22, matmul_23);  matmul_22 = matmul_23 = None\n",
            "    transpose_56: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_45, 2, 3);  add_45 = None\n",
            "    contiguous_5: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_56);  transpose_56 = None\n",
            "    view_47: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_5, [sym_size_int_1, 192, sym_size_int_14]);  contiguous_5 = sym_size_int_14 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_33: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_47, arg59_1, arg60_1);  view_47 = arg59_1 = arg60_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_21: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_33, 0.1, False);  conv1d_33 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_46: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_50, dropout_21);  transpose_50 = dropout_21 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_57: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_46, 1, -1);  add_46 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_10: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_57, [192], arg71_1, arg72_1);  transpose_57 = arg71_1 = arg72_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_58: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_10, 1, -1);  layer_norm_10 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_73: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_58, type_as)\n",
            "    pad_46: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_73, [1, 1, 0, 0, 0, 0]);  mul_73 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_34: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_46, arg93_1, arg94_1);  pad_46 = arg93_1 = arg94_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_5: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_34);  conv1d_34 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_22: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_5, 0.1, False);  relu_5 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_74: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_22, type_as);  dropout_22 = None\n",
            "    pad_47: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_74, [1, 1, 0, 0, 0, 0]);  mul_74 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_35: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_47, arg95_1, arg96_1);  pad_47 = arg95_1 = arg96_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_75: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_35, type_as);  conv1d_35 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_23: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_75, 0.1, False);  mul_75 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_47: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_58, dropout_23);  transpose_58 = dropout_23 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_59: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_47, 1, -1);  add_47 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_11: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_59, [192], arg107_1, arg108_1);  transpose_59 = arg107_1 = arg108_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_60: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_11, 1, -1);  layer_norm_11 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:73 in forward, code: x = x * x_mask\n",
            "    mul_76: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_60, type_as);  transpose_60 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_36: \"f32[s31, 384, s12]\" = torch.ops.aten.conv1d.default(mul_76, arg109_1, arg110_1);  arg109_1 = arg110_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:206 in forward, code: stats = self.proj(x) * x_mask\n",
            "    mul_77: \"f32[s31, 384, s12]\" = torch.ops.aten.mul.Tensor(conv1d_36, type_as);  conv1d_36 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:208 in forward, code: m, logs = torch.split(stats, self.out_channels, dim=1)\n",
            "    split = torch.ops.aten.split.Tensor(mul_77, 192, 1);  mul_77 = None\n",
            "    getitem: \"f32[s31, 192, s12]\" = split[0];  getitem = None\n",
            "    getitem_1: \"f32[s31, 192, s12]\" = split[1];  split = getitem_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:64 in forward, code: x = torch.detach(x)\n",
            "    detach: \"f32[s31, 192, s12]\" = torch.ops.aten.detach.default(mul_76);  mul_76 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_37: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(detach, arg624_1, arg625_1);  detach = arg624_1 = arg625_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_78: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_37, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_38: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_78, arg628_1, arg629_1, [1], [1], [1], 192);  mul_78 = arg628_1 = arg629_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_61: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_38, 1, -1);  conv1d_38 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_12: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_61, [192], arg640_1, arg641_1);  transpose_61 = arg640_1 = arg641_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_62: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_12, 1, -1);  layer_norm_12 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_62);  transpose_62 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_39: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu, arg634_1, arg635_1);  gelu = arg634_1 = arg635_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_63: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_39, 1, -1);  conv1d_39 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_13: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_63, [192], arg646_1, arg647_1);  transpose_63 = arg646_1 = arg647_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_64: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_13, 1, -1);  layer_norm_13 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_1: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_64);  transpose_64 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_24: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_1, 0.5, False);  gelu_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_48: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(conv1d_37, dropout_24);  conv1d_37 = dropout_24 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_79: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_48, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_40: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_79, arg630_1, arg631_1, [1], [3], [3], 192);  mul_79 = arg630_1 = arg631_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_65: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_40, 1, -1);  conv1d_40 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_14: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_65, [192], arg642_1, arg643_1);  transpose_65 = arg642_1 = arg643_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_66: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_14, 1, -1);  layer_norm_14 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_2: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_66);  transpose_66 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_41: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_2, arg636_1, arg637_1);  gelu_2 = arg636_1 = arg637_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_67: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_41, 1, -1);  conv1d_41 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_15: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_67, [192], arg648_1, arg649_1);  transpose_67 = arg648_1 = arg649_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_68: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_15, 1, -1);  layer_norm_15 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_3: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_68);  transpose_68 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_25: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_3, 0.5, False);  gelu_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_49: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_48, dropout_25);  add_48 = dropout_25 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_80: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_49, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_42: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_80, arg632_1, arg633_1, [1], [9], [9], 192);  mul_80 = arg632_1 = arg633_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_69: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_42, 1, -1);  conv1d_42 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_16: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_69, [192], arg644_1, arg645_1);  transpose_69 = arg644_1 = arg645_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_70: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_16, 1, -1);  layer_norm_16 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_4: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_70);  transpose_70 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_43: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_4, arg638_1, arg639_1);  gelu_4 = arg638_1 = arg639_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_71: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_43, 1, -1);  conv1d_43 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_17: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_71, [192], arg650_1, arg651_1);  transpose_71 = arg650_1 = arg651_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_72: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_17, 1, -1);  layer_norm_17 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_5: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_72);  transpose_72 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_26: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_5, 0.5, False);  gelu_5 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_50: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_49, dropout_26);  add_49 = dropout_26 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:129 in forward, code: return x * x_mask\n",
            "    mul_81: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_50, type_as);  add_50 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_44: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_81, arg626_1, arg627_1);  mul_81 = arg626_1 = arg627_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:70 in forward, code: x = self.proj(x) * x_mask\n",
            "    mul_82: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_44, type_as)\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:111 in forward, code: z = torch.randn(x.size(0), 2, x.size(2)).type_as(x) * noise_scale\n",
            "    sym_size_int_16: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_44, 2);  conv1d_44 = None\n",
            "    randn: \"f32[s31, 2, s12]\" = torch.ops.aten.randn.default([sym_size_int_1, 2, sym_size_int_16], device = device(type='cpu'), pin_memory = False)\n",
            "    type_as_1: \"f32[s31, 2, s12]\" = torch.ops.aten.type_as.default(randn, mul_82);  randn = None\n",
            "    mul_83: \"f32[s31, 2, s12]\" = torch.ops.aten.mul.Tensor(type_as_1, select_2);  type_as_1 = select_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:386 in forward, code: x = torch.flip(x, [1])\n",
            "    flip: \"f32[s31, 2, s12]\" = torch.ops.aten.flip.default(mul_83, [1]);  mul_83 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:497 in forward, code: x0, x1 = torch.split(x, [self.half_channels] * 2, 1)\n",
            "    split_with_sizes = torch.ops.aten.split_with_sizes.default(flip, [1, 1], 1);  flip = None\n",
            "    getitem_2: \"f32[s31, 1, s12]\" = split_with_sizes[0]\n",
            "    getitem_3: \"f32[s31, 1, s12]\" = split_with_sizes[1];  split_with_sizes = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_45: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(getitem_2, arg454_1, arg455_1);  getitem_2 = arg454_1 = arg455_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:119 in forward, code: x = x + g\n",
            "    add_51: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(conv1d_45, mul_82);  conv1d_45 = mul_82 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_84: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_51, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_46: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_84, arg456_1, arg457_1, [1], [1], [1], 192);  mul_84 = arg456_1 = arg457_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_73: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_46, 1, -1);  conv1d_46 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_18: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_73, [192], arg468_1, arg469_1);  transpose_73 = arg468_1 = arg469_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_74: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_18, 1, -1);  layer_norm_18 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_6: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_74);  transpose_74 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_47: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_6, arg462_1, arg463_1);  gelu_6 = arg462_1 = arg463_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_75: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_47, 1, -1);  conv1d_47 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_19: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_75, [192], arg474_1, arg475_1);  transpose_75 = arg474_1 = arg475_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_76: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_19, 1, -1);  layer_norm_19 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_7: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_76);  transpose_76 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_27: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_7, 0.0, False);  gelu_7 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_52: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_51, dropout_27);  add_51 = dropout_27 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_85: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_52, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_48: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_85, arg458_1, arg459_1, [1], [3], [3], 192);  mul_85 = arg458_1 = arg459_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_77: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_48, 1, -1);  conv1d_48 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_20: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_77, [192], arg470_1, arg471_1);  transpose_77 = arg470_1 = arg471_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_78: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_20, 1, -1);  layer_norm_20 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_8: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_78);  transpose_78 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_49: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_8, arg464_1, arg465_1);  gelu_8 = arg464_1 = arg465_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_79: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_49, 1, -1);  conv1d_49 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_21: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_79, [192], arg476_1, arg477_1);  transpose_79 = arg476_1 = arg477_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_80: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_21, 1, -1);  layer_norm_21 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_9: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_80);  transpose_80 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_28: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_9, 0.0, False);  gelu_9 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_53: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_52, dropout_28);  add_52 = dropout_28 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_86: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_53, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_50: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_86, arg460_1, arg461_1, [1], [9], [9], 192);  mul_86 = arg460_1 = arg461_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_81: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_50, 1, -1);  conv1d_50 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_22: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_81, [192], arg472_1, arg473_1);  transpose_81 = arg472_1 = arg473_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_82: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_22, 1, -1);  layer_norm_22 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_10: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_82);  transpose_82 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_51: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_10, arg466_1, arg467_1);  gelu_10 = arg466_1 = arg467_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_83: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_51, 1, -1);  conv1d_51 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_23: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_83, [192], arg478_1, arg479_1);  transpose_83 = arg478_1 = arg479_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_84: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_23, 1, -1);  layer_norm_23 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_11: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_84);  transpose_84 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_29: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_11, 0.0, False);  gelu_11 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_54: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_53, dropout_29);  add_53 = dropout_29 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:129 in forward, code: return x * x_mask\n",
            "    mul_87: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_54, type_as);  add_54 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_52: \"f32[s31, 29, s12]\" = torch.ops.aten.conv1d.default(mul_87, arg480_1, arg481_1);  mul_87 = arg480_1 = arg481_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:500 in forward, code: h = self.proj(h) * x_mask\n",
            "    mul_88: \"f32[s31, 29, s12]\" = torch.ops.aten.mul.Tensor(conv1d_52, type_as);  conv1d_52 = type_as = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:503 in forward, code: h = h.reshape(b, c, -1, t).permute(0, 1, 3, 2)  # [b, cx?, t] -> [b, c, t, ?]\n",
            "    reshape: \"f32[s31, 1, 29, s12]\" = torch.ops.aten.reshape.default(mul_88, [sym_size_int_1, 1, -1, sym_size_int_16]);  mul_88 = sym_size_int_1 = sym_size_int_16 = None\n",
            "    permute: \"f32[s31, 1, s12, 29]\" = torch.ops.aten.permute.default(reshape, [0, 1, 3, 2]);  reshape = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:505 in forward, code: unnormalized_widths = h[..., : self.num_bins] / math.sqrt(self.filter_channels)\n",
            "    slice_43: \"f32[s31, 1, s12, 10]\" = torch.ops.aten.slice.Tensor(permute, 3, 0, 10)\n",
            "    div_12: \"f32[s31, 1, s12, 10]\" = torch.ops.aten.div.Tensor(slice_43, 13.856406460551018);  slice_43 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:506 in forward, code: unnormalized_heights = h[..., self.num_bins : 2 * self.num_bins] / math.sqrt(\n",
            "    slice_44: \"f32[s31, 1, s12, 10]\" = torch.ops.aten.slice.Tensor(permute, 3, 10, 20)\n",
            "    div_13: \"f32[s31, 1, s12, 10]\" = torch.ops.aten.div.Tensor(slice_44, 13.856406460551018);  slice_44 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:509 in forward, code: unnormalized_derivatives = h[..., 2 * self.num_bins :]\n",
            "    slice_45: \"f32[s31, 1, s12, 9]\" = torch.ops.aten.slice.Tensor(permute, 3, 20, 9223372036854775807);  permute = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:511 in forward, code: x1, logabsdet = piecewise_rational_quadratic_transform(\n",
            "    ge: \"b8[s31, 1, s12]\" = torch.ops.aten.ge.Scalar(getitem_3, -5.0)\n",
            "    le_12: \"b8[s31, 1, s12]\" = torch.ops.aten.le.Scalar(getitem_3, 5.0)\n",
            "    and_1: \"b8[s31, 1, s12]\" = torch.ops.aten.__and__.Tensor(ge, le_12);  ge = le_12 = None\n",
            "    bitwise_not: \"b8[s31, 1, s12]\" = torch.ops.aten.bitwise_not.default(and_1)\n",
            "    zeros_like: \"f32[s31, 1, s12]\" = torch.ops.aten.zeros_like.default(getitem_3, pin_memory = False)\n",
            "    zeros_like_1: \"f32[s31, 1, s12]\" = torch.ops.aten.zeros_like.default(getitem_3, pin_memory = False)\n",
            "    pad_48: \"f32[s31, 1, s12, 11]\" = torch.ops.aten.pad.default(slice_45, [1, 1]);  slice_45 = None\n",
            "    _tensor_constant0: \"f32[]\" = self._tensor_constant0\n",
            "    lift_fresh_copy: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None\n",
            "    select_3: \"f32[s31, 1, s12]\" = torch.ops.aten.select.int(pad_48, 3, 0)\n",
            "    fill_: \"f32[s31, 1, s12]\" = torch.ops.aten.fill_.Tensor(select_3, lift_fresh_copy);  select_3 = lift_fresh_copy = fill_ = None\n",
            "    _tensor_constant1: \"f32[]\" = self._tensor_constant1\n",
            "    lift_fresh_copy_1: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None\n",
            "    select_4: \"f32[s31, 1, s12]\" = torch.ops.aten.select.int(pad_48, 3, 10)\n",
            "    fill__1: \"f32[s31, 1, s12]\" = torch.ops.aten.fill_.Tensor(select_4, lift_fresh_copy_1);  select_4 = lift_fresh_copy_1 = fill__1 = None\n",
            "    index: \"f32[u0]\" = torch.ops.aten.index.Tensor(getitem_3, [bitwise_not])\n",
            "    sym_size_int_17: \"Sym(u0)\" = torch.ops.aten.sym_size.int(index, 0)\n",
            "    eq_32: \"Sym(True)\" = sym_size_int_17 == sym_size_int_17;  sym_size_int_17 = eq_32 = None\n",
            "    index_put_: \"f32[s31, 1, s12]\" = torch.ops.aten.index_put_.default(zeros_like, [bitwise_not], index);  zeros_like = index = index_put_ = None\n",
            "    _tensor_constant2: \"f32[]\" = self._tensor_constant2\n",
            "    lift_fresh_copy_2: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant2);  _tensor_constant2 = None\n",
            "    index_put__1: \"f32[s31, 1, s12]\" = torch.ops.aten.index_put_.default(zeros_like_1, [bitwise_not], lift_fresh_copy_2);  zeros_like_1 = bitwise_not = lift_fresh_copy_2 = index_put__1 = None\n",
            "    index_1: \"f32[u1]\" = torch.ops.aten.index.Tensor(getitem_3, [and_1]);  getitem_3 = None\n",
            "    index_2: \"f32[u1, 10]\" = torch.ops.aten.index.Tensor(div_12, [and_1]);  div_12 = None\n",
            "    index_3: \"f32[u1, 10]\" = torch.ops.aten.index.Tensor(div_13, [and_1]);  div_13 = None\n",
            "    index_4: \"f32[u1, 11]\" = torch.ops.aten.index.Tensor(pad_48, [and_1]);  pad_48 = and_1 = None\n",
            "    softmax_6: \"f32[u1, 10]\" = torch.ops.aten.softmax.int(index_2, -1);  index_2 = None\n",
            "    mul_89: \"f32[u1, 10]\" = torch.ops.aten.mul.Tensor(softmax_6, 0.99);  softmax_6 = None\n",
            "    add_55: \"f32[u1, 10]\" = torch.ops.aten.add.Tensor(mul_89, 0.001);  mul_89 = None\n",
            "    cumsum: \"f32[u1, 10]\" = torch.ops.aten.cumsum.default(add_55, -1);  add_55 = None\n",
            "    pad_49: \"f32[u1, 11]\" = torch.ops.aten.pad.default(cumsum, [1, 0], 'constant', 0.0);  cumsum = None\n",
            "    mul_90: \"f32[u1, 11]\" = torch.ops.aten.mul.Tensor(pad_49, 10.0);  pad_49 = None\n",
            "    add_56: \"f32[u1, 11]\" = torch.ops.aten.add.Tensor(mul_90, -5.0);  mul_90 = None\n",
            "    _tensor_constant3: \"f32[]\" = self._tensor_constant3\n",
            "    lift_fresh_copy_3: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant3);  _tensor_constant3 = None\n",
            "    select_5: \"f32[u1]\" = torch.ops.aten.select.int(add_56, 1, 0)\n",
            "    fill__2: \"f32[u1]\" = torch.ops.aten.fill_.Tensor(select_5, lift_fresh_copy_3);  select_5 = lift_fresh_copy_3 = fill__2 = None\n",
            "    _tensor_constant4: \"f32[]\" = self._tensor_constant4\n",
            "    lift_fresh_copy_4: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant4);  _tensor_constant4 = None\n",
            "    select_6: \"f32[u1]\" = torch.ops.aten.select.int(add_56, 1, 10)\n",
            "    fill__3: \"f32[u1]\" = torch.ops.aten.fill_.Tensor(select_6, lift_fresh_copy_4);  select_6 = lift_fresh_copy_4 = fill__3 = None\n",
            "    slice_46: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_56, 1, 1, 9223372036854775807)\n",
            "    slice_47: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_56, 1, 0, -1)\n",
            "    sub_66: \"f32[u1, 10]\" = torch.ops.aten.sub.Tensor(slice_46, slice_47);  slice_46 = slice_47 = None\n",
            "    softplus: \"f32[u1, 11]\" = torch.ops.aten.softplus.default(index_4);  index_4 = None\n",
            "    add_57: \"f32[u1, 11]\" = torch.ops.aten.add.Tensor(softplus, 0.001);  softplus = None\n",
            "    softmax_7: \"f32[u1, 10]\" = torch.ops.aten.softmax.int(index_3, -1);  index_3 = None\n",
            "    mul_91: \"f32[u1, 10]\" = torch.ops.aten.mul.Tensor(softmax_7, 0.99);  softmax_7 = None\n",
            "    add_58: \"f32[u1, 10]\" = torch.ops.aten.add.Tensor(mul_91, 0.001);  mul_91 = None\n",
            "    cumsum_1: \"f32[u1, 10]\" = torch.ops.aten.cumsum.default(add_58, -1);  add_58 = None\n",
            "    pad_50: \"f32[u1, 11]\" = torch.ops.aten.pad.default(cumsum_1, [1, 0], 'constant', 0.0);  cumsum_1 = None\n",
            "    mul_92: \"f32[u1, 11]\" = torch.ops.aten.mul.Tensor(pad_50, 10.0);  pad_50 = None\n",
            "    add_59: \"f32[u1, 11]\" = torch.ops.aten.add.Tensor(mul_92, -5.0);  mul_92 = None\n",
            "    _tensor_constant5: \"f32[]\" = self._tensor_constant5\n",
            "    lift_fresh_copy_5: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant5);  _tensor_constant5 = None\n",
            "    select_7: \"f32[u1]\" = torch.ops.aten.select.int(add_59, 1, 0)\n",
            "    fill__4: \"f32[u1]\" = torch.ops.aten.fill_.Tensor(select_7, lift_fresh_copy_5);  select_7 = lift_fresh_copy_5 = fill__4 = None\n",
            "    _tensor_constant6: \"f32[]\" = self._tensor_constant6\n",
            "    lift_fresh_copy_6: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant6);  _tensor_constant6 = None\n",
            "    select_8: \"f32[u1]\" = torch.ops.aten.select.int(add_59, 1, 10)\n",
            "    fill__5: \"f32[u1]\" = torch.ops.aten.fill_.Tensor(select_8, lift_fresh_copy_6);  select_8 = lift_fresh_copy_6 = fill__5 = None\n",
            "    slice_48: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_59, 1, 1, 9223372036854775807)\n",
            "    slice_49: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_59, 1, 0, -1)\n",
            "    sub_67: \"f32[u1, 10]\" = torch.ops.aten.sub.Tensor(slice_48, slice_49);  slice_48 = slice_49 = None\n",
            "    select_9: \"f32[u1]\" = torch.ops.aten.select.int(add_59, 1, 10)\n",
            "    add_: \"f32[u1]\" = torch.ops.aten.add_.Tensor(select_9, 1e-06);  select_9 = None\n",
            "    select_10: \"f32[u1]\" = torch.ops.aten.select.int(add_59, 1, 10)\n",
            "    sym_size_int_18: \"Sym(u1)\" = torch.ops.aten.sym_size.int(index_1, 0)\n",
            "    eq_33: \"Sym(True)\" = sym_size_int_18 == sym_size_int_18;  sym_size_int_18 = eq_33 = None\n",
            "    copy_: \"f32[u1]\" = torch.ops.aten.copy_.default(select_10, add_);  select_10 = add_ = copy_ = None\n",
            "    unsqueeze_17: \"f32[u1, 1]\" = torch.ops.aten.unsqueeze.default(index_1, 1)\n",
            "    ge_1: \"b8[u1, 11]\" = torch.ops.aten.ge.Tensor(unsqueeze_17, add_59);  unsqueeze_17 = None\n",
            "    sum_1: \"i64[u1]\" = torch.ops.aten.sum.dim_IntList(ge_1, [-1]);  ge_1 = None\n",
            "    sub_68: \"i64[u1]\" = torch.ops.aten.sub.Tensor(sum_1, 1);  sum_1 = None\n",
            "    unsqueeze_18: \"i64[u1, 1]\" = torch.ops.aten.unsqueeze.default(sub_68, 1);  sub_68 = None\n",
            "    gather: \"f32[u1, 1]\" = torch.ops.aten.gather.default(add_56, -1, unsqueeze_18);  add_56 = None\n",
            "    select_11: \"f32[u1]\" = torch.ops.aten.select.int(gather, 1, 0);  gather = select_11 = None\n",
            "    gather_1: \"f32[u1, 1]\" = torch.ops.aten.gather.default(sub_66, -1, unsqueeze_18)\n",
            "    select_12: \"f32[u1]\" = torch.ops.aten.select.int(gather_1, 1, 0);  gather_1 = select_12 = None\n",
            "    gather_2: \"f32[u1, 1]\" = torch.ops.aten.gather.default(add_59, -1, unsqueeze_18);  add_59 = None\n",
            "    select_13: \"f32[u1]\" = torch.ops.aten.select.int(gather_2, 1, 0);  gather_2 = None\n",
            "    div_14: \"f32[u1, 10]\" = torch.ops.aten.div.Tensor(sub_67, sub_66);  sub_66 = None\n",
            "    gather_3: \"f32[u1, 1]\" = torch.ops.aten.gather.default(div_14, -1, unsqueeze_18);  div_14 = None\n",
            "    select_14: \"f32[u1]\" = torch.ops.aten.select.int(gather_3, 1, 0);  gather_3 = None\n",
            "    gather_4: \"f32[u1, 1]\" = torch.ops.aten.gather.default(add_57, -1, unsqueeze_18)\n",
            "    select_15: \"f32[u1]\" = torch.ops.aten.select.int(gather_4, 1, 0);  gather_4 = None\n",
            "    slice_50: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_57, 1, 1, 9223372036854775807);  add_57 = None\n",
            "    gather_5: \"f32[u1, 1]\" = torch.ops.aten.gather.default(slice_50, -1, unsqueeze_18);  slice_50 = None\n",
            "    select_16: \"f32[u1]\" = torch.ops.aten.select.int(gather_5, 1, 0);  gather_5 = None\n",
            "    gather_6: \"f32[u1, 1]\" = torch.ops.aten.gather.default(sub_67, -1, unsqueeze_18);  sub_67 = unsqueeze_18 = None\n",
            "    select_17: \"f32[u1]\" = torch.ops.aten.select.int(gather_6, 1, 0);  gather_6 = None\n",
            "    sub_69: \"f32[u1]\" = torch.ops.aten.sub.Tensor(index_1, select_13)\n",
            "    add_60: \"f32[u1]\" = torch.ops.aten.add.Tensor(select_15, select_16)\n",
            "    mul_93: \"f32[u1]\" = torch.ops.aten.mul.Tensor(select_14, 2)\n",
            "    sub_70: \"f32[u1]\" = torch.ops.aten.sub.Tensor(add_60, mul_93);  add_60 = mul_93 = None\n",
            "    mul_94: \"f32[u1]\" = torch.ops.aten.mul.Tensor(sub_69, sub_70);  sub_69 = sub_70 = None\n",
            "    sub_71: \"f32[u1]\" = torch.ops.aten.sub.Tensor(select_14, select_15)\n",
            "    mul_95: \"f32[u1]\" = torch.ops.aten.mul.Tensor(select_17, sub_71);  sub_71 = None\n",
            "    add_61: \"f32[u1]\" = torch.ops.aten.add.Tensor(mul_94, mul_95);  mul_94 = mul_95 = None\n",
            "    mul_96: \"f32[u1]\" = torch.ops.aten.mul.Tensor(select_17, select_15);  select_17 = None\n",
            "    sub_72: \"f32[u1]\" = torch.ops.aten.sub.Tensor(index_1, select_13)\n",
            "    add_62: \"f32[u1]\" = torch.ops.aten.add.Tensor(select_15, select_16);  select_15 = select_16 = None\n",
            "    mul_97: \"f32[u1]\" = torch.ops.aten.mul.Tensor(select_14, 2)\n",
            "    sub_73: \"f32[u1]\" = torch.ops.aten.sub.Tensor(add_62, mul_97);  add_62 = mul_97 = None\n",
            "    mul_98: \"f32[u1]\" = torch.ops.aten.mul.Tensor(sub_72, sub_73);  sub_72 = sub_73 = None\n",
            "    sub_74: \"f32[u1]\" = torch.ops.aten.sub.Tensor(mul_96, mul_98);  mul_96 = mul_98 = None\n",
            "    neg: \"f32[u1]\" = torch.ops.aten.neg.default(select_14);  select_14 = None\n",
            "    sub_75: \"f32[u1]\" = torch.ops.aten.sub.Tensor(index_1, select_13);  index_1 = select_13 = None\n",
            "    mul_99: \"f32[u1]\" = torch.ops.aten.mul.Tensor(neg, sub_75);  neg = sub_75 = None\n",
            "    pow_1: \"f32[u1]\" = torch.ops.aten.pow.Tensor_Scalar(sub_74, 2);  sub_74 = None\n",
            "    mul_100: \"f32[u1]\" = torch.ops.aten.mul.Tensor(add_61, 4);  add_61 = None\n",
            "    mul_101: \"f32[u1]\" = torch.ops.aten.mul.Tensor(mul_100, mul_99);  mul_100 = mul_99 = None\n",
            "    sub_76: \"f32[u1]\" = torch.ops.aten.sub.Tensor(pow_1, mul_101);  pow_1 = mul_101 = None\n",
            "    ge_2: \"b8[u1]\" = torch.ops.aten.ge.Scalar(sub_76, 0);  sub_76 = None\n",
            "    all_1: \"b8[]\" = torch.ops.aten.all.default(ge_2);  ge_2 = None\n",
            "    ne: \"b8[]\" = torch.ops.aten.ne.Scalar(all_1, 0);  all_1 = None\n",
            "    item: \"Sym(Eq(u2, 1))\" = torch.ops.aten.item.default(ne);  ne = item = None\n",
            "    \n",
            "\n",
            "\n",
            "\n",
            "def forward(self, arg0_1: \"f32[256, 192]\", arg1_1: \"f32[1, 9, 96]\", arg2_1: \"f32[1, 9, 96]\", arg3_1: \"f32[192, 192, 1]\", arg4_1: \"f32[192]\", arg5_1: \"f32[192, 192, 1]\", arg6_1: \"f32[192]\", arg7_1: \"f32[192, 192, 1]\", arg8_1: \"f32[192]\", arg9_1: \"f32[192, 192, 1]\", arg10_1: \"f32[192]\", arg11_1: \"f32[1, 9, 96]\", arg12_1: \"f32[1, 9, 96]\", arg13_1: \"f32[192, 192, 1]\", arg14_1: \"f32[192]\", arg15_1: \"f32[192, 192, 1]\", arg16_1: \"f32[192]\", arg17_1: \"f32[192, 192, 1]\", arg18_1: \"f32[192]\", arg19_1: \"f32[192, 192, 1]\", arg20_1: \"f32[192]\", arg21_1: \"f32[1, 9, 96]\", arg22_1: \"f32[1, 9, 96]\", arg23_1: \"f32[192, 192, 1]\", arg24_1: \"f32[192]\", arg25_1: \"f32[192, 192, 1]\", arg26_1: \"f32[192]\", arg27_1: \"f32[192, 192, 1]\", arg28_1: \"f32[192]\", arg29_1: \"f32[192, 192, 1]\", arg30_1: \"f32[192]\", arg31_1: \"f32[1, 9, 96]\", arg32_1: \"f32[1, 9, 96]\", arg33_1: \"f32[192, 192, 1]\", arg34_1: \"f32[192]\", arg35_1: \"f32[192, 192, 1]\", arg36_1: \"f32[192]\", arg37_1: \"f32[192, 192, 1]\", arg38_1: \"f32[192]\", arg39_1: \"f32[192, 192, 1]\", arg40_1: \"f32[192]\", arg41_1: \"f32[1, 9, 96]\", arg42_1: \"f32[1, 9, 96]\", arg43_1: \"f32[192, 192, 1]\", arg44_1: \"f32[192]\", arg45_1: \"f32[192, 192, 1]\", arg46_1: \"f32[192]\", arg47_1: \"f32[192, 192, 1]\", arg48_1: \"f32[192]\", arg49_1: \"f32[192, 192, 1]\", arg50_1: \"f32[192]\", arg51_1: \"f32[1, 9, 96]\", arg52_1: \"f32[1, 9, 96]\", arg53_1: \"f32[192, 192, 1]\", arg54_1: \"f32[192]\", arg55_1: \"f32[192, 192, 1]\", arg56_1: \"f32[192]\", arg57_1: \"f32[192, 192, 1]\", arg58_1: \"f32[192]\", arg59_1: \"f32[192, 192, 1]\", arg60_1: \"f32[192]\", arg61_1: \"f32[192]\", arg62_1: \"f32[192]\", arg63_1: \"f32[192]\", arg64_1: \"f32[192]\", arg65_1: \"f32[192]\", arg66_1: \"f32[192]\", arg67_1: \"f32[192]\", arg68_1: \"f32[192]\", arg69_1: \"f32[192]\", arg70_1: \"f32[192]\", arg71_1: \"f32[192]\", arg72_1: \"f32[192]\", arg73_1: \"f32[768, 192, 3]\", arg74_1: \"f32[768]\", arg75_1: \"f32[192, 768, 3]\", arg76_1: \"f32[192]\", arg77_1: \"f32[768, 192, 3]\", arg78_1: \"f32[768]\", arg79_1: \"f32[192, 768, 3]\", arg80_1: \"f32[192]\", arg81_1: \"f32[768, 192, 3]\", arg82_1: \"f32[768]\", arg83_1: \"f32[192, 768, 3]\", arg84_1: \"f32[192]\", arg85_1: \"f32[768, 192, 3]\", arg86_1: \"f32[768]\", arg87_1: \"f32[192, 768, 3]\", arg88_1: \"f32[192]\", arg89_1: \"f32[768, 192, 3]\", arg90_1: \"f32[768]\", arg91_1: \"f32[192, 768, 3]\", arg92_1: \"f32[192]\", arg93_1: \"f32[768, 192, 3]\", arg94_1: \"f32[768]\", arg95_1: \"f32[192, 768, 3]\", arg96_1: \"f32[192]\", arg97_1: \"f32[192]\", arg98_1: \"f32[192]\", arg99_1: \"f32[192]\", arg100_1: \"f32[192]\", arg101_1: \"f32[192]\", arg102_1: \"f32[192]\", arg103_1: \"f32[192]\", arg104_1: \"f32[192]\", arg105_1: \"f32[192]\", arg106_1: \"f32[192]\", arg107_1: \"f32[192]\", arg108_1: \"f32[192]\", arg109_1: \"f32[384, 192, 1]\", arg110_1: \"f32[384]\", arg111_1: \"f32[256, 192, 7]\", arg112_1: \"f32[256]\", arg113_1: \"f32[128]\", arg114_1: \"f32[256, 128, 16]\", arg115_1: \"f32[64]\", arg116_1: \"f32[128, 64, 16]\", arg117_1: \"f32[32]\", arg118_1: \"f32[64, 32, 8]\", arg119_1: \"f32[128]\", arg120_1: \"f32[128, 128, 3]\", arg121_1: \"f32[128]\", arg122_1: \"f32[128, 128, 3]\", arg123_1: \"f32[128]\", arg124_1: \"f32[128, 128, 5]\", arg125_1: \"f32[128]\", arg126_1: \"f32[128, 128, 5]\", arg127_1: \"f32[128]\", arg128_1: \"f32[128, 128, 7]\", arg129_1: \"f32[128]\", arg130_1: \"f32[128, 128, 7]\", arg131_1: \"f32[64]\", arg132_1: \"f32[64, 64, 3]\", arg133_1: \"f32[64]\", arg134_1: \"f32[64, 64, 3]\", arg135_1: \"f32[64]\", arg136_1: \"f32[64, 64, 5]\", arg137_1: \"f32[64]\", arg138_1: \"f32[64, 64, 5]\", arg139_1: \"f32[64]\", arg140_1: \"f32[64, 64, 7]\", arg141_1: \"f32[64]\", arg142_1: \"f32[64, 64, 7]\", arg143_1: \"f32[32]\", arg144_1: \"f32[32, 32, 3]\", arg145_1: \"f32[32]\", arg146_1: \"f32[32, 32, 3]\", arg147_1: \"f32[32]\", arg148_1: \"f32[32, 32, 5]\", arg149_1: \"f32[32]\", arg150_1: \"f32[32, 32, 5]\", arg151_1: \"f32[32]\", arg152_1: \"f32[32, 32, 7]\", arg153_1: \"f32[32]\", arg154_1: \"f32[32, 32, 7]\", arg155_1: \"f32[1, 32, 7]\", arg156_1: \"f32[192, 513, 1]\", arg157_1: \"f32[192]\", arg158_1: \"f32[384]\", arg159_1: \"f32[384, 1, 1]\", arg160_1: \"f32[384, 192, 5]\", arg161_1: \"f32[384]\", arg162_1: \"f32[384, 1, 1]\", arg163_1: \"f32[384, 192, 5]\", arg164_1: \"f32[384]\", arg165_1: \"f32[384, 1, 1]\", arg166_1: \"f32[384, 192, 5]\", arg167_1: \"f32[384]\", arg168_1: \"f32[384, 1, 1]\", arg169_1: \"f32[384, 192, 5]\", arg170_1: \"f32[384]\", arg171_1: \"f32[384, 1, 1]\", arg172_1: \"f32[384, 192, 5]\", arg173_1: \"f32[384]\", arg174_1: \"f32[384, 1, 1]\", arg175_1: \"f32[384, 192, 5]\", arg176_1: \"f32[384]\", arg177_1: \"f32[384, 1, 1]\", arg178_1: \"f32[384, 192, 5]\", arg179_1: \"f32[384]\", arg180_1: \"f32[384, 1, 1]\", arg181_1: \"f32[384, 192, 5]\", arg182_1: \"f32[384]\", arg183_1: \"f32[384, 1, 1]\", arg184_1: \"f32[384, 192, 5]\", arg185_1: \"f32[384]\", arg186_1: \"f32[384, 1, 1]\", arg187_1: \"f32[384, 192, 5]\", arg188_1: \"f32[384]\", arg189_1: \"f32[384, 1, 1]\", arg190_1: \"f32[384, 192, 5]\", arg191_1: \"f32[384]\", arg192_1: \"f32[384, 1, 1]\", arg193_1: \"f32[384, 192, 5]\", arg194_1: \"f32[384]\", arg195_1: \"f32[384, 1, 1]\", arg196_1: \"f32[384, 192, 5]\", arg197_1: \"f32[384]\", arg198_1: \"f32[384, 1, 1]\", arg199_1: \"f32[384, 192, 5]\", arg200_1: \"f32[384]\", arg201_1: \"f32[384, 1, 1]\", arg202_1: \"f32[384, 192, 5]\", arg203_1: \"f32[384]\", arg204_1: \"f32[384, 1, 1]\", arg205_1: \"f32[384, 192, 5]\", arg206_1: \"f32[384]\", arg207_1: \"f32[384, 1, 1]\", arg208_1: \"f32[384, 192, 1]\", arg209_1: \"f32[384]\", arg210_1: \"f32[384, 1, 1]\", arg211_1: \"f32[384, 192, 1]\", arg212_1: \"f32[384]\", arg213_1: \"f32[384, 1, 1]\", arg214_1: \"f32[384, 192, 1]\", arg215_1: \"f32[384]\", arg216_1: \"f32[384, 1, 1]\", arg217_1: \"f32[384, 192, 1]\", arg218_1: \"f32[384]\", arg219_1: \"f32[384, 1, 1]\", arg220_1: \"f32[384, 192, 1]\", arg221_1: \"f32[384]\", arg222_1: \"f32[384, 1, 1]\", arg223_1: \"f32[384, 192, 1]\", arg224_1: \"f32[384]\", arg225_1: \"f32[384, 1, 1]\", arg226_1: \"f32[384, 192, 1]\", arg227_1: \"f32[384]\", arg228_1: \"f32[384, 1, 1]\", arg229_1: \"f32[384, 192, 1]\", arg230_1: \"f32[384]\", arg231_1: \"f32[384, 1, 1]\", arg232_1: \"f32[384, 192, 1]\", arg233_1: \"f32[384]\", arg234_1: \"f32[384, 1, 1]\", arg235_1: \"f32[384, 192, 1]\", arg236_1: \"f32[384]\", arg237_1: \"f32[384, 1, 1]\", arg238_1: \"f32[384, 192, 1]\", arg239_1: \"f32[384]\", arg240_1: \"f32[384, 1, 1]\", arg241_1: \"f32[384, 192, 1]\", arg242_1: \"f32[384]\", arg243_1: \"f32[384, 1, 1]\", arg244_1: \"f32[384, 192, 1]\", arg245_1: \"f32[384]\", arg246_1: \"f32[384, 1, 1]\", arg247_1: \"f32[384, 192, 1]\", arg248_1: \"f32[384]\", arg249_1: \"f32[384, 1, 1]\", arg250_1: \"f32[384, 192, 1]\", arg251_1: \"f32[192]\", arg252_1: \"f32[192, 1, 1]\", arg253_1: \"f32[192, 192, 1]\", arg254_1: \"f32[384, 192, 1]\", arg255_1: \"f32[384]\", arg256_1: \"f32[192, 96, 1]\", arg257_1: \"f32[192]\", arg258_1: \"f32[384]\", arg259_1: \"f32[384, 1, 1]\", arg260_1: \"f32[384, 192, 5]\", arg261_1: \"f32[384]\", arg262_1: \"f32[384, 1, 1]\", arg263_1: \"f32[384, 192, 5]\", arg264_1: \"f32[384]\", arg265_1: \"f32[384, 1, 1]\", arg266_1: \"f32[384, 192, 5]\", arg267_1: \"f32[384]\", arg268_1: \"f32[384, 1, 1]\", arg269_1: \"f32[384, 192, 5]\", arg270_1: \"f32[384]\", arg271_1: \"f32[384, 1, 1]\", arg272_1: \"f32[384, 192, 1]\", arg273_1: \"f32[384]\", arg274_1: \"f32[384, 1, 1]\", arg275_1: \"f32[384, 192, 1]\", arg276_1: \"f32[384]\", arg277_1: \"f32[384, 1, 1]\", arg278_1: \"f32[384, 192, 1]\", arg279_1: \"f32[192]\", arg280_1: \"f32[192, 1, 1]\", arg281_1: \"f32[192, 192, 1]\", arg282_1: \"f32[96, 192, 1]\", arg283_1: \"f32[96]\", arg284_1: \"f32[192, 96, 1]\", arg285_1: \"f32[192]\", arg286_1: \"f32[384]\", arg287_1: \"f32[384, 1, 1]\", arg288_1: \"f32[384, 192, 5]\", arg289_1: \"f32[384]\", arg290_1: \"f32[384, 1, 1]\", arg291_1: \"f32[384, 192, 5]\", arg292_1: \"f32[384]\", arg293_1: \"f32[384, 1, 1]\", arg294_1: \"f32[384, 192, 5]\", arg295_1: \"f32[384]\", arg296_1: \"f32[384, 1, 1]\", arg297_1: \"f32[384, 192, 5]\", arg298_1: \"f32[384]\", arg299_1: \"f32[384, 1, 1]\", arg300_1: \"f32[384, 192, 1]\", arg301_1: \"f32[384]\", arg302_1: \"f32[384, 1, 1]\", arg303_1: \"f32[384, 192, 1]\", arg304_1: \"f32[384]\", arg305_1: \"f32[384, 1, 1]\", arg306_1: \"f32[384, 192, 1]\", arg307_1: \"f32[192]\", arg308_1: \"f32[192, 1, 1]\", arg309_1: \"f32[192, 192, 1]\", arg310_1: \"f32[96, 192, 1]\", arg311_1: \"f32[96]\", arg312_1: \"f32[192, 96, 1]\", arg313_1: \"f32[192]\", arg314_1: \"f32[384]\", arg315_1: \"f32[384, 1, 1]\", arg316_1: \"f32[384, 192, 5]\", arg317_1: \"f32[384]\", arg318_1: \"f32[384, 1, 1]\", arg319_1: \"f32[384, 192, 5]\", arg320_1: \"f32[384]\", arg321_1: \"f32[384, 1, 1]\", arg322_1: \"f32[384, 192, 5]\", arg323_1: \"f32[384]\", arg324_1: \"f32[384, 1, 1]\", arg325_1: \"f32[384, 192, 5]\", arg326_1: \"f32[384]\", arg327_1: \"f32[384, 1, 1]\", arg328_1: \"f32[384, 192, 1]\", arg329_1: \"f32[384]\", arg330_1: \"f32[384, 1, 1]\", arg331_1: \"f32[384, 192, 1]\", arg332_1: \"f32[384]\", arg333_1: \"f32[384, 1, 1]\", arg334_1: \"f32[384, 192, 1]\", arg335_1: \"f32[192]\", arg336_1: \"f32[192, 1, 1]\", arg337_1: \"f32[192, 192, 1]\", arg338_1: \"f32[96, 192, 1]\", arg339_1: \"f32[96]\", arg340_1: \"f32[192, 96, 1]\", arg341_1: \"f32[192]\", arg342_1: \"f32[384]\", arg343_1: \"f32[384, 1, 1]\", arg344_1: \"f32[384, 192, 5]\", arg345_1: \"f32[384]\", arg346_1: \"f32[384, 1, 1]\", arg347_1: \"f32[384, 192, 5]\", arg348_1: \"f32[384]\", arg349_1: \"f32[384, 1, 1]\", arg350_1: \"f32[384, 192, 5]\", arg351_1: \"f32[384]\", arg352_1: \"f32[384, 1, 1]\", arg353_1: \"f32[384, 192, 5]\", arg354_1: \"f32[384]\", arg355_1: \"f32[384, 1, 1]\", arg356_1: \"f32[384, 192, 1]\", arg357_1: \"f32[384]\", arg358_1: \"f32[384, 1, 1]\", arg359_1: \"f32[384, 192, 1]\", arg360_1: \"f32[384]\", arg361_1: \"f32[384, 1, 1]\", arg362_1: \"f32[384, 192, 1]\", arg363_1: \"f32[192]\", arg364_1: \"f32[192, 1, 1]\", arg365_1: \"f32[192, 192, 1]\", arg366_1: \"f32[96, 192, 1]\", arg367_1: \"f32[96]\", arg368_1: \"f32[2, 1]\", arg369_1: \"f32[2, 1]\", arg370_1: \"f32[192, 1, 1]\", arg371_1: \"f32[192]\", arg372_1: \"f32[192, 1, 3]\", arg373_1: \"f32[192]\", arg374_1: \"f32[192, 1, 3]\", arg375_1: \"f32[192]\", arg376_1: \"f32[192, 1, 3]\", arg377_1: \"f32[192]\", arg378_1: \"f32[192, 192, 1]\", arg379_1: \"f32[192]\", arg380_1: \"f32[192, 192, 1]\", arg381_1: \"f32[192]\", arg382_1: \"f32[192, 192, 1]\", arg383_1: \"f32[192]\", arg384_1: \"f32[192]\", arg385_1: \"f32[192]\", arg386_1: \"f32[192]\", arg387_1: \"f32[192]\", arg388_1: \"f32[192]\", arg389_1: \"f32[192]\", arg390_1: \"f32[192]\", arg391_1: \"f32[192]\", arg392_1: \"f32[192]\", arg393_1: \"f32[192]\", arg394_1: \"f32[192]\", arg395_1: \"f32[192]\", arg396_1: \"f32[29, 192, 1]\", arg397_1: \"f32[29]\", arg398_1: \"f32[192, 1, 1]\", arg399_1: \"f32[192]\", arg400_1: \"f32[192, 1, 3]\", arg401_1: \"f32[192]\", arg402_1: \"f32[192, 1, 3]\", arg403_1: \"f32[192]\", arg404_1: \"f32[192, 1, 3]\", arg405_1: \"f32[192]\", arg406_1: \"f32[192, 192, 1]\", arg407_1: \"f32[192]\", arg408_1: \"f32[192, 192, 1]\", arg409_1: \"f32[192]\", arg410_1: \"f32[192, 192, 1]\", arg411_1: \"f32[192]\", arg412_1: \"f32[192]\", arg413_1: \"f32[192]\", arg414_1: \"f32[192]\", arg415_1: \"f32[192]\", arg416_1: \"f32[192]\", arg417_1: \"f32[192]\", arg418_1: \"f32[192]\", arg419_1: \"f32[192]\", arg420_1: \"f32[192]\", arg421_1: \"f32[192]\", arg422_1: \"f32[192]\", arg423_1: \"f32[192]\", arg424_1: \"f32[29, 192, 1]\", arg425_1: \"f32[29]\", arg426_1: \"f32[192, 1, 1]\", arg427_1: \"f32[192]\", arg428_1: \"f32[192, 1, 3]\", arg429_1: \"f32[192]\", arg430_1: \"f32[192, 1, 3]\", arg431_1: \"f32[192]\", arg432_1: \"f32[192, 1, 3]\", arg433_1: \"f32[192]\", arg434_1: \"f32[192, 192, 1]\", arg435_1: \"f32[192]\", arg436_1: \"f32[192, 192, 1]\", arg437_1: \"f32[192]\", arg438_1: \"f32[192, 192, 1]\", arg439_1: \"f32[192]\", arg440_1: \"f32[192]\", arg441_1: \"f32[192]\", arg442_1: \"f32[192]\", arg443_1: \"f32[192]\", arg444_1: \"f32[192]\", arg445_1: \"f32[192]\", arg446_1: \"f32[192]\", arg447_1: \"f32[192]\", arg448_1: \"f32[192]\", arg449_1: \"f32[192]\", arg450_1: \"f32[192]\", arg451_1: \"f32[192]\", arg452_1: \"f32[29, 192, 1]\", arg453_1: \"f32[29]\", arg454_1: \"f32[192, 1, 1]\", arg455_1: \"f32[192]\", arg456_1: \"f32[192, 1, 3]\", arg457_1: \"f32[192]\", arg458_1: \"f32[192, 1, 3]\", arg459_1: \"f32[192]\", arg460_1: \"f32[192, 1, 3]\", arg461_1: \"f32[192]\", arg462_1: \"f32[192, 192, 1]\", arg463_1: \"f32[192]\", arg464_1: \"f32[192, 192, 1]\", arg465_1: \"f32[192]\", arg466_1: \"f32[192, 192, 1]\", arg467_1: \"f32[192]\", arg468_1: \"f32[192]\", arg469_1: \"f32[192]\", arg470_1: \"f32[192]\", arg471_1: \"f32[192]\", arg472_1: \"f32[192]\", arg473_1: \"f32[192]\", arg474_1: \"f32[192]\", arg475_1: \"f32[192]\", arg476_1: \"f32[192]\", arg477_1: \"f32[192]\", arg478_1: \"f32[192]\", arg479_1: \"f32[192]\", arg480_1: \"f32[29, 192, 1]\", arg481_1: \"f32[29]\", arg482_1: \"f32[192, 1, 1]\", arg483_1: \"f32[192]\", arg484_1: \"f32[192, 192, 1]\", arg485_1: \"f32[192]\", arg486_1: \"f32[192, 1, 3]\", arg487_1: \"f32[192]\", arg488_1: \"f32[192, 1, 3]\", arg489_1: \"f32[192]\", arg490_1: \"f32[192, 1, 3]\", arg491_1: \"f32[192]\", arg492_1: \"f32[192, 192, 1]\", arg493_1: \"f32[192]\", arg494_1: \"f32[192, 192, 1]\", arg495_1: \"f32[192]\", arg496_1: \"f32[192, 192, 1]\", arg497_1: \"f32[192]\", arg498_1: \"f32[192]\", arg499_1: \"f32[192]\", arg500_1: \"f32[192]\", arg501_1: \"f32[192]\", arg502_1: \"f32[192]\", arg503_1: \"f32[192]\", arg504_1: \"f32[192]\", arg505_1: \"f32[192]\", arg506_1: \"f32[192]\", arg507_1: \"f32[192]\", arg508_1: \"f32[192]\", arg509_1: \"f32[192]\", arg510_1: \"f32[2, 1]\", arg511_1: \"f32[2, 1]\", arg512_1: \"f32[192, 1, 1]\", arg513_1: \"f32[192]\", arg514_1: \"f32[192, 1, 3]\", arg515_1: \"f32[192]\", arg516_1: \"f32[192, 1, 3]\", arg517_1: \"f32[192]\", arg518_1: \"f32[192, 1, 3]\", arg519_1: \"f32[192]\", arg520_1: \"f32[192, 192, 1]\", arg521_1: \"f32[192]\", arg522_1: \"f32[192, 192, 1]\", arg523_1: \"f32[192]\", arg524_1: \"f32[192, 192, 1]\", arg525_1: \"f32[192]\", arg526_1: \"f32[192]\", arg527_1: \"f32[192]\", arg528_1: \"f32[192]\", arg529_1: \"f32[192]\", arg530_1: \"f32[192]\", arg531_1: \"f32[192]\", arg532_1: \"f32[192]\", arg533_1: \"f32[192]\", arg534_1: \"f32[192]\", arg535_1: \"f32[192]\", arg536_1: \"f32[192]\", arg537_1: \"f32[192]\", arg538_1: \"f32[29, 192, 1]\", arg539_1: \"f32[29]\", arg540_1: \"f32[192, 1, 1]\", arg541_1: \"f32[192]\", arg542_1: \"f32[192, 1, 3]\", arg543_1: \"f32[192]\", arg544_1: \"f32[192, 1, 3]\", arg545_1: \"f32[192]\", arg546_1: \"f32[192, 1, 3]\", arg547_1: \"f32[192]\", arg548_1: \"f32[192, 192, 1]\", arg549_1: \"f32[192]\", arg550_1: \"f32[192, 192, 1]\", arg551_1: \"f32[192]\", arg552_1: \"f32[192, 192, 1]\", arg553_1: \"f32[192]\", arg554_1: \"f32[192]\", arg555_1: \"f32[192]\", arg556_1: \"f32[192]\", arg557_1: \"f32[192]\", arg558_1: \"f32[192]\", arg559_1: \"f32[192]\", arg560_1: \"f32[192]\", arg561_1: \"f32[192]\", arg562_1: \"f32[192]\", arg563_1: \"f32[192]\", arg564_1: \"f32[192]\", arg565_1: \"f32[192]\", arg566_1: \"f32[29, 192, 1]\", arg567_1: \"f32[29]\", arg568_1: \"f32[192, 1, 1]\", arg569_1: \"f32[192]\", arg570_1: \"f32[192, 1, 3]\", arg571_1: \"f32[192]\", arg572_1: \"f32[192, 1, 3]\", arg573_1: \"f32[192]\", arg574_1: \"f32[192, 1, 3]\", arg575_1: \"f32[192]\", arg576_1: \"f32[192, 192, 1]\", arg577_1: \"f32[192]\", arg578_1: \"f32[192, 192, 1]\", arg579_1: \"f32[192]\", arg580_1: \"f32[192, 192, 1]\", arg581_1: \"f32[192]\", arg582_1: \"f32[192]\", arg583_1: \"f32[192]\", arg584_1: \"f32[192]\", arg585_1: \"f32[192]\", arg586_1: \"f32[192]\", arg587_1: \"f32[192]\", arg588_1: \"f32[192]\", arg589_1: \"f32[192]\", arg590_1: \"f32[192]\", arg591_1: \"f32[192]\", arg592_1: \"f32[192]\", arg593_1: \"f32[192]\", arg594_1: \"f32[29, 192, 1]\", arg595_1: \"f32[29]\", arg596_1: \"f32[192, 1, 1]\", arg597_1: \"f32[192]\", arg598_1: \"f32[192, 1, 3]\", arg599_1: \"f32[192]\", arg600_1: \"f32[192, 1, 3]\", arg601_1: \"f32[192]\", arg602_1: \"f32[192, 1, 3]\", arg603_1: \"f32[192]\", arg604_1: \"f32[192, 192, 1]\", arg605_1: \"f32[192]\", arg606_1: \"f32[192, 192, 1]\", arg607_1: \"f32[192]\", arg608_1: \"f32[192, 192, 1]\", arg609_1: \"f32[192]\", arg610_1: \"f32[192]\", arg611_1: \"f32[192]\", arg612_1: \"f32[192]\", arg613_1: \"f32[192]\", arg614_1: \"f32[192]\", arg615_1: \"f32[192]\", arg616_1: \"f32[192]\", arg617_1: \"f32[192]\", arg618_1: \"f32[192]\", arg619_1: \"f32[192]\", arg620_1: \"f32[192]\", arg621_1: \"f32[192]\", arg622_1: \"f32[29, 192, 1]\", arg623_1: \"f32[29]\", arg624_1: \"f32[192, 192, 1]\", arg625_1: \"f32[192]\", arg626_1: \"f32[192, 192, 1]\", arg627_1: \"f32[192]\", arg628_1: \"f32[192, 1, 3]\", arg629_1: \"f32[192]\", arg630_1: \"f32[192, 1, 3]\", arg631_1: \"f32[192]\", arg632_1: \"f32[192, 1, 3]\", arg633_1: \"f32[192]\", arg634_1: \"f32[192, 192, 1]\", arg635_1: \"f32[192]\", arg636_1: \"f32[192, 192, 1]\", arg637_1: \"f32[192]\", arg638_1: \"f32[192, 192, 1]\", arg639_1: \"f32[192]\", arg640_1: \"f32[192]\", arg641_1: \"f32[192]\", arg642_1: \"f32[192]\", arg643_1: \"f32[192]\", arg644_1: \"f32[192]\", arg645_1: \"f32[192]\", arg646_1: \"f32[192]\", arg647_1: \"f32[192]\", arg648_1: \"f32[192]\", arg649_1: \"f32[192]\", arg650_1: \"f32[192]\", arg651_1: \"f32[192]\", arg652_1: \"i64[s31, s12]\", arg653_1: \"i64[s31]\", arg654_1: \"f32[3]\", arg655_1):\n",
            "    # No stacktrace found for following nodes\n",
            "    select: \"f32[]\" = torch.ops.aten.select.int(arg654_1, 0, 0);  select = None\n",
            "    select_1: \"f32[]\" = torch.ops.aten.select.int(arg654_1, 0, 1);  select_1 = None\n",
            "    select_2: \"f32[]\" = torch.ops.aten.select.int(arg654_1, 0, 2);  arg654_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(\n",
            "    embedding: \"f32[s31, s12, 192]\" = torch.ops.aten.embedding.default(arg0_1, arg652_1);  arg0_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:199 in forward, code: x = self.emb(x) * math.sqrt(self.hidden_channels)  # [b, t, h]\n",
            "    mul: \"f32[s31, s12, 192]\" = torch.ops.aten.mul.Tensor(embedding, 13.856406460551018);  embedding = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:200 in forward, code: x = torch.transpose(x, 1, -1)  # [b, h, t]\n",
            "    transpose: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(mul, 1, -1);  mul = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:202 in forward, code: commons.sequence_mask(x_lengths, x.size(2)), 1\n",
            "    sym_size_int: \"Sym(s12)\" = torch.ops.aten.sym_size.int(arg652_1, 1)\n",
            "    arange: \"i64[s12]\" = torch.ops.aten.arange.default(sym_size_int, dtype = torch.int64, device = device(type='cpu'), pin_memory = False);  sym_size_int = None\n",
            "    unsqueeze: \"i64[1, s12]\" = torch.ops.aten.unsqueeze.default(arange, 0)\n",
            "    unsqueeze_1: \"i64[s31, 1]\" = torch.ops.aten.unsqueeze.default(arg653_1, 1)\n",
            "    lt: \"b8[s31, s12]\" = torch.ops.aten.lt.Tensor(unsqueeze, unsqueeze_1);  unsqueeze = unsqueeze_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:201 in forward, code: x_mask = torch.unsqueeze(\n",
            "    unsqueeze_2: \"b8[s31, 1, s12]\" = torch.ops.aten.unsqueeze.default(lt, 1);  lt = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:203 in forward, code: ).type_as(x)\n",
            "    type_as: \"f32[s31, 1, s12]\" = torch.ops.aten.type_as.default(unsqueeze_2, transpose);  unsqueeze_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:205 in forward, code: x = self.encoder(x * x_mask, x_mask)\n",
            "    mul_1: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose, type_as);  transpose = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:61 in forward, code: attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n",
            "    unsqueeze_3: \"f32[s31, 1, 1, s12]\" = torch.ops.aten.unsqueeze.default(type_as, 2)\n",
            "    unsqueeze_4: \"f32[s31, 1, s12, 1]\" = torch.ops.aten.unsqueeze.default(type_as, -1)\n",
            "    mul_2: \"f32[s31, 1, s12, s12]\" = torch.ops.aten.mul.Tensor(unsqueeze_3, unsqueeze_4);  unsqueeze_3 = unsqueeze_4 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:62 in forward, code: x = x * x_mask\n",
            "    mul_3: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(mul_1, type_as);  mul_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_3, arg3_1, arg4_1);  arg3_1 = arg4_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_1: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_3, arg5_1, arg6_1);  arg5_1 = arg6_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_2: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_3, arg7_1, arg8_1);  arg7_1 = arg8_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_1: \"Sym(s31)\" = torch.ops.aten.sym_size.int(arg652_1, 0);  arg652_1 = None\n",
            "    sym_size_int_2: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d, 2)\n",
            "    view: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d, [sym_size_int_1, 2, 96, sym_size_int_2]);  conv1d = None\n",
            "    transpose_1: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view, 2, 3);  view = None\n",
            "    sym_size_int_3: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_1, 2)\n",
            "    view_1: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_1, [sym_size_int_1, 2, 96, sym_size_int_3]);  conv1d_1 = None\n",
            "    transpose_2: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_1, 2, 3);  view_1 = None\n",
            "    view_2: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_2, [sym_size_int_1, 2, 96, sym_size_int_3]);  conv1d_2 = None\n",
            "    transpose_3: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_2, 2, 3);  view_2 = None\n",
            "    div: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_1, 9.797958971132712)\n",
            "    transpose_4: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_2, -2, -1);  transpose_2 = None\n",
            "    matmul: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div, transpose_4);  div = transpose_4 = None\n",
            "    eq: \"Sym(True)\" = sym_size_int_3 == sym_size_int_2;  eq = None\n",
            "    sub: \"Sym(s12 - 5)\" = sym_size_int_3 - 5\n",
            "    sym_max: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub, 0);  sub = None\n",
            "    sub_1: \"Sym(5 - s12)\" = 5 - sym_size_int_3\n",
            "    sym_max_1: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_1, 0);  sub_1 = None\n",
            "    mul_4: \"Sym(2*s12)\" = 2 * sym_size_int_3\n",
            "    add: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_1 + mul_4;  mul_4 = None\n",
            "    sub_2: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add - 1;  add = None\n",
            "    gt: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max > 0;  gt = None\n",
            "    pad: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg1_1, [0, 0, sym_max, sym_max, 0, 0]);  arg1_1 = sym_max = None\n",
            "    slice_1: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad, 1, sym_max_1, sub_2);  pad = sym_max_1 = sub_2 = None\n",
            "    div_1: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_1, 9.797958971132712);  transpose_1 = None\n",
            "    unsqueeze_5: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_1, 0);  slice_1 = None\n",
            "    transpose_5: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_5, -2, -1);  unsqueeze_5 = None\n",
            "    matmul_1: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_1, transpose_5);  div_1 = transpose_5 = None\n",
            "    pad_1: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_1, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_1 = None\n",
            "    mul_5: \"Sym(2*s12)\" = sym_size_int_2 * 2\n",
            "    mul_6: \"Sym(2*s12**2)\" = mul_5 * sym_size_int_2;  mul_5 = None\n",
            "    view_3: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_1, [sym_size_int_1, 2, mul_6]);  pad_1 = mul_6 = None\n",
            "    sub_3: \"Sym(s12 - 1)\" = sym_size_int_2 - 1\n",
            "    pad_2: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_3, [0, sub_3, 0, 0, 0, 0]);  view_3 = sub_3 = None\n",
            "    add_1: \"Sym(s12 + 1)\" = sym_size_int_2 + 1\n",
            "    mul_7: \"Sym(2*s12)\" = 2 * sym_size_int_2\n",
            "    sub_4: \"Sym(2*s12 - 1)\" = mul_7 - 1;  mul_7 = None\n",
            "    view_4: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_2, [sym_size_int_1, 2, add_1, sub_4]);  pad_2 = add_1 = sub_4 = None\n",
            "    sub_5: \"Sym(s12 - 1)\" = sym_size_int_2 - 1\n",
            "    slice_2: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_4, 2, None, sym_size_int_2);  view_4 = None\n",
            "    slice_3: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_2, 3, sub_5);  slice_2 = sub_5 = None\n",
            "    add_2: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul, slice_3);  matmul = slice_3 = None\n",
            "    eq_1: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_2, eq_1, -10000.0);  add_2 = eq_1 = None\n",
            "    softmax: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill, -1);  masked_fill = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax, 0.1, False);  softmax = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_2: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout, transpose_3);  transpose_3 = None\n",
            "    sym_size_int_4: \"Sym(s12)\" = torch.ops.aten.sym_size.int(arange, 0);  arange = None\n",
            "    sub_6: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_3: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout, [0, sub_6, 0, 0, 0, 0, 0, 0]);  dropout = sub_6 = None\n",
            "    mul_8: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_7: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_9: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_7;  sub_7 = None\n",
            "    add_3: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_8 + mul_9;  mul_8 = mul_9 = None\n",
            "    sym_size_int_5: \"Sym(s31)\" = torch.ops.aten.sym_size.int(arg653_1, 0);  arg653_1 = None\n",
            "    view_5: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_3, [sym_size_int_5, 2, add_3]);  pad_3 = add_3 = None\n",
            "    pad_4: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_5, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_5 = None\n",
            "    mul_10: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_6: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_4, [sym_size_int_5, 2, sym_size_int_4, mul_10]);  pad_4 = mul_10 = None\n",
            "    le: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le = None\n",
            "    slice_4: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_6, 0, 0, 9223372036854775807);  view_6 = None\n",
            "    le_1: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_1 = None\n",
            "    slice_5: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_4, 2, 0, 9223372036854775807);  slice_4 = None\n",
            "    slice_6: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_5, 3, 1, 9223372036854775807);  slice_5 = None\n",
            "    sub_8: \"Sym(s12 - 5)\" = sym_size_int_3 - 5\n",
            "    sym_max_2: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_8, 0);  sub_8 = None\n",
            "    sub_9: \"Sym(5 - s12)\" = 5 - sym_size_int_3\n",
            "    sym_max_3: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_9, 0);  sub_9 = None\n",
            "    mul_11: \"Sym(2*s12)\" = 2 * sym_size_int_3;  sym_size_int_3 = None\n",
            "    add_4: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_3 + mul_11;  mul_11 = None\n",
            "    sub_10: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_4 - 1;  add_4 = None\n",
            "    gt_1: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_2 > 0;  gt_1 = None\n",
            "    pad_5: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg2_1, [0, 0, sym_max_2, sym_max_2, 0, 0]);  arg2_1 = sym_max_2 = None\n",
            "    slice_7: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_5, 1, sym_max_3, sub_10);  pad_5 = sym_max_3 = sub_10 = None\n",
            "    unsqueeze_6: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_7, 0);  slice_7 = None\n",
            "    matmul_3: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_6, unsqueeze_6);  slice_6 = unsqueeze_6 = None\n",
            "    add_5: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_2, matmul_3);  matmul_2 = matmul_3 = None\n",
            "    transpose_6: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_5, 2, 3)\n",
            "    sym_numel_default: \"Sym(192*s12*s31)\" = torch.ops.aten.sym_numel.default(transpose_6)\n",
            "    eq_2: \"Sym(False)\" = sym_numel_default == 0;  eq_2 = None\n",
            "    eq_3: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1;  eq_3 = None\n",
            "    eq_4: \"Sym(False)\" = sym_numel_default == 0\n",
            "    eq_5: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1\n",
            "    or_: \"Sym(Eq(s12, 1))\" = eq_5 | False;  eq_5 = None\n",
            "    and_: \"Sym(Eq(s12, 1))\" = True & or_;  or_ = None\n",
            "    eq_6: \"Sym(Eq(s12, 1))\" = 1 == sym_size_int_4\n",
            "    or__1: \"Sym(Eq(s12, 1))\" = False | eq_6;  eq_6 = None\n",
            "    and__1: \"Sym(Eq(s12, 1))\" = and_ & or__1;  and_ = or__1 = None\n",
            "    mul_12: \"Sym(96*s12)\" = sym_size_int_4 * 96\n",
            "    sym_stride_int: \"Sym(96*s12)\" = torch.ops.aten.sym_stride.int(add_5, 1)\n",
            "    eq_7: \"Sym(True)\" = sym_stride_int == mul_12;  sym_stride_int = None\n",
            "    or__2: \"Sym(True)\" = False | eq_7;  eq_7 = None\n",
            "    and__2: \"Sym(Eq(s12, 1))\" = and__1 & or__2;  and__1 = or__2 = None\n",
            "    mul_13: \"Sym(192*s12)\" = mul_12 * 2;  mul_12 = None\n",
            "    eq_8: \"Sym(Eq(s31, 1))\" = sym_size_int_5 == 1\n",
            "    sym_stride_int_1: \"Sym(192*s12)\" = torch.ops.aten.sym_stride.int(add_5, 0);  add_5 = None\n",
            "    eq_9: \"Sym(True)\" = sym_stride_int_1 == mul_13;  sym_stride_int_1 = None\n",
            "    or__3: \"Sym(True)\" = eq_8 | eq_9;  eq_8 = eq_9 = None\n",
            "    and__3: \"Sym(Eq(s12, 1))\" = and__2 & or__3;  and__2 = or__3 = None\n",
            "    mul_14: \"Sym(192*s12*s31)\" = mul_13 * sym_size_int_5;  mul_13 = mul_14 = None\n",
            "    or__4: \"Sym(Eq(s12, 1))\" = and__3 | eq_4;  and__3 = eq_4 = or__4 = None\n",
            "    eq_10: \"Sym(False)\" = sym_numel_default == 0;  sym_numel_default = eq_10 = None\n",
            "    eq_11: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1;  eq_11 = None\n",
            "    contiguous: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_6);  transpose_6 = None\n",
            "    view_7: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous, [sym_size_int_1, 192, sym_size_int_2]);  contiguous = sym_size_int_2 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_3: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_7, arg9_1, arg10_1);  view_7 = arg9_1 = arg10_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_1: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_3, 0.1, False);  conv1d_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_6: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(mul_3, dropout_1);  mul_3 = dropout_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_7: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_6, 1, -1);  add_6 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_7, [192], arg61_1, arg62_1);  transpose_7 = arg61_1 = arg62_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_8: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm, 1, -1);  layer_norm = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_15: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_8, type_as)\n",
            "    pad_6: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_15, [1, 1, 0, 0, 0, 0]);  mul_15 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_4: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_6, arg73_1, arg74_1);  pad_6 = arg73_1 = arg74_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_4);  conv1d_4 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_2: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu, 0.1, False);  relu = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_16: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_2, type_as);  dropout_2 = None\n",
            "    pad_7: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_16, [1, 1, 0, 0, 0, 0]);  mul_16 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_5: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_7, arg75_1, arg76_1);  pad_7 = arg75_1 = arg76_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_17: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_5, type_as);  conv1d_5 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_3: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_17, 0.1, False);  mul_17 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_7: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_8, dropout_3);  transpose_8 = dropout_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_9: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_7, 1, -1);  add_7 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_1: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_9, [192], arg97_1, arg98_1);  transpose_9 = arg97_1 = arg98_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_10: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_1, 1, -1);  layer_norm_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_6: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_10, arg13_1, arg14_1);  arg13_1 = arg14_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_7: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_10, arg15_1, arg16_1);  arg15_1 = arg16_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_8: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_10, arg17_1, arg18_1);  arg17_1 = arg18_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_6: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_6, 2)\n",
            "    view_8: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_6, [sym_size_int_1, 2, 96, sym_size_int_6]);  conv1d_6 = None\n",
            "    transpose_11: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_8, 2, 3);  view_8 = None\n",
            "    sym_size_int_7: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_7, 2)\n",
            "    view_9: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_7, [sym_size_int_1, 2, 96, sym_size_int_7]);  conv1d_7 = None\n",
            "    transpose_12: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_9, 2, 3);  view_9 = None\n",
            "    view_10: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_8, [sym_size_int_1, 2, 96, sym_size_int_7]);  conv1d_8 = None\n",
            "    transpose_13: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_10, 2, 3);  view_10 = None\n",
            "    div_2: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_11, 9.797958971132712)\n",
            "    transpose_14: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_12, -2, -1);  transpose_12 = None\n",
            "    matmul_4: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_2, transpose_14);  div_2 = transpose_14 = None\n",
            "    eq_12: \"Sym(True)\" = sym_size_int_7 == sym_size_int_6;  eq_12 = None\n",
            "    sub_11: \"Sym(s12 - 5)\" = sym_size_int_7 - 5\n",
            "    sym_max_4: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_11, 0);  sub_11 = None\n",
            "    sub_12: \"Sym(5 - s12)\" = 5 - sym_size_int_7\n",
            "    sym_max_5: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_12, 0);  sub_12 = None\n",
            "    mul_18: \"Sym(2*s12)\" = 2 * sym_size_int_7\n",
            "    add_8: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_5 + mul_18;  mul_18 = None\n",
            "    sub_13: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_8 - 1;  add_8 = None\n",
            "    gt_2: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_4 > 0;  gt_2 = None\n",
            "    pad_8: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg11_1, [0, 0, sym_max_4, sym_max_4, 0, 0]);  arg11_1 = sym_max_4 = None\n",
            "    slice_8: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_8, 1, sym_max_5, sub_13);  pad_8 = sym_max_5 = sub_13 = None\n",
            "    div_3: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_11, 9.797958971132712);  transpose_11 = None\n",
            "    unsqueeze_7: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_8, 0);  slice_8 = None\n",
            "    transpose_15: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_7, -2, -1);  unsqueeze_7 = None\n",
            "    matmul_5: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_3, transpose_15);  div_3 = transpose_15 = None\n",
            "    pad_9: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_5, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_5 = None\n",
            "    mul_19: \"Sym(2*s12)\" = sym_size_int_6 * 2\n",
            "    mul_20: \"Sym(2*s12**2)\" = mul_19 * sym_size_int_6;  mul_19 = None\n",
            "    view_11: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_9, [sym_size_int_1, 2, mul_20]);  pad_9 = mul_20 = None\n",
            "    sub_14: \"Sym(s12 - 1)\" = sym_size_int_6 - 1\n",
            "    pad_10: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_11, [0, sub_14, 0, 0, 0, 0]);  view_11 = sub_14 = None\n",
            "    add_9: \"Sym(s12 + 1)\" = sym_size_int_6 + 1\n",
            "    mul_21: \"Sym(2*s12)\" = 2 * sym_size_int_6\n",
            "    sub_15: \"Sym(2*s12 - 1)\" = mul_21 - 1;  mul_21 = None\n",
            "    view_12: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_10, [sym_size_int_1, 2, add_9, sub_15]);  pad_10 = add_9 = sub_15 = None\n",
            "    sub_16: \"Sym(s12 - 1)\" = sym_size_int_6 - 1\n",
            "    slice_9: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_12, 2, None, sym_size_int_6);  view_12 = None\n",
            "    slice_10: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_9, 3, sub_16);  slice_9 = sub_16 = None\n",
            "    add_10: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_4, slice_10);  matmul_4 = slice_10 = None\n",
            "    eq_13: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill_1: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_10, eq_13, -10000.0);  add_10 = eq_13 = None\n",
            "    softmax_1: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_1, -1);  masked_fill_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_4: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_1, 0.1, False);  softmax_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_6: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_4, transpose_13);  transpose_13 = None\n",
            "    sub_17: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_11: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_4, [0, sub_17, 0, 0, 0, 0, 0, 0]);  dropout_4 = sub_17 = None\n",
            "    mul_22: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_18: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_23: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_18;  sub_18 = None\n",
            "    add_11: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_22 + mul_23;  mul_22 = mul_23 = None\n",
            "    view_13: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_11, [sym_size_int_5, 2, add_11]);  pad_11 = add_11 = None\n",
            "    pad_12: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_13, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_13 = None\n",
            "    mul_24: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_14: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_12, [sym_size_int_5, 2, sym_size_int_4, mul_24]);  pad_12 = mul_24 = None\n",
            "    le_2: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le_2 = None\n",
            "    slice_11: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_14, 0, 0, 9223372036854775807);  view_14 = None\n",
            "    le_3: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_3 = None\n",
            "    slice_12: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_11, 2, 0, 9223372036854775807);  slice_11 = None\n",
            "    slice_13: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_12, 3, 1, 9223372036854775807);  slice_12 = None\n",
            "    sub_19: \"Sym(s12 - 5)\" = sym_size_int_7 - 5\n",
            "    sym_max_6: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_19, 0);  sub_19 = None\n",
            "    sub_20: \"Sym(5 - s12)\" = 5 - sym_size_int_7\n",
            "    sym_max_7: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_20, 0);  sub_20 = None\n",
            "    mul_25: \"Sym(2*s12)\" = 2 * sym_size_int_7;  sym_size_int_7 = None\n",
            "    add_12: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_7 + mul_25;  mul_25 = None\n",
            "    sub_21: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_12 - 1;  add_12 = None\n",
            "    gt_3: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_6 > 0;  gt_3 = None\n",
            "    pad_13: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg12_1, [0, 0, sym_max_6, sym_max_6, 0, 0]);  arg12_1 = sym_max_6 = None\n",
            "    slice_14: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_13, 1, sym_max_7, sub_21);  pad_13 = sym_max_7 = sub_21 = None\n",
            "    unsqueeze_8: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_14, 0);  slice_14 = None\n",
            "    matmul_7: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_13, unsqueeze_8);  slice_13 = unsqueeze_8 = None\n",
            "    add_13: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_6, matmul_7);  matmul_6 = matmul_7 = None\n",
            "    transpose_16: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_13, 2, 3);  add_13 = None\n",
            "    contiguous_1: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_16);  transpose_16 = None\n",
            "    view_15: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_1, [sym_size_int_1, 192, sym_size_int_6]);  contiguous_1 = sym_size_int_6 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_9: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_15, arg19_1, arg20_1);  view_15 = arg19_1 = arg20_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_5: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_9, 0.1, False);  conv1d_9 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_14: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_10, dropout_5);  transpose_10 = dropout_5 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_17: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_14, 1, -1);  add_14 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_2: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_17, [192], arg63_1, arg64_1);  transpose_17 = arg63_1 = arg64_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_18: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_2, 1, -1);  layer_norm_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_26: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_18, type_as)\n",
            "    pad_14: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_26, [1, 1, 0, 0, 0, 0]);  mul_26 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_10: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_14, arg77_1, arg78_1);  pad_14 = arg77_1 = arg78_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_1: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_10);  conv1d_10 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_6: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_1, 0.1, False);  relu_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_27: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_6, type_as);  dropout_6 = None\n",
            "    pad_15: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_27, [1, 1, 0, 0, 0, 0]);  mul_27 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_11: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_15, arg79_1, arg80_1);  pad_15 = arg79_1 = arg80_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_28: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_11, type_as);  conv1d_11 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_7: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_28, 0.1, False);  mul_28 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_15: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_18, dropout_7);  transpose_18 = dropout_7 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_19: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_15, 1, -1);  add_15 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_3: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_19, [192], arg99_1, arg100_1);  transpose_19 = arg99_1 = arg100_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_20: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_3, 1, -1);  layer_norm_3 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_12: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_20, arg23_1, arg24_1);  arg23_1 = arg24_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_13: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_20, arg25_1, arg26_1);  arg25_1 = arg26_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_14: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_20, arg27_1, arg28_1);  arg27_1 = arg28_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_8: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_12, 2)\n",
            "    view_16: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_12, [sym_size_int_1, 2, 96, sym_size_int_8]);  conv1d_12 = None\n",
            "    transpose_21: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_16, 2, 3);  view_16 = None\n",
            "    sym_size_int_9: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_13, 2)\n",
            "    view_17: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_13, [sym_size_int_1, 2, 96, sym_size_int_9]);  conv1d_13 = None\n",
            "    transpose_22: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_17, 2, 3);  view_17 = None\n",
            "    view_18: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_14, [sym_size_int_1, 2, 96, sym_size_int_9]);  conv1d_14 = None\n",
            "    transpose_23: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_18, 2, 3);  view_18 = None\n",
            "    div_4: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_21, 9.797958971132712)\n",
            "    transpose_24: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_22, -2, -1);  transpose_22 = None\n",
            "    matmul_8: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_4, transpose_24);  div_4 = transpose_24 = None\n",
            "    eq_14: \"Sym(True)\" = sym_size_int_9 == sym_size_int_8;  eq_14 = None\n",
            "    sub_22: \"Sym(s12 - 5)\" = sym_size_int_9 - 5\n",
            "    sym_max_8: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_22, 0);  sub_22 = None\n",
            "    sub_23: \"Sym(5 - s12)\" = 5 - sym_size_int_9\n",
            "    sym_max_9: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_23, 0);  sub_23 = None\n",
            "    mul_29: \"Sym(2*s12)\" = 2 * sym_size_int_9\n",
            "    add_16: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_9 + mul_29;  mul_29 = None\n",
            "    sub_24: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_16 - 1;  add_16 = None\n",
            "    gt_4: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_8 > 0;  gt_4 = None\n",
            "    pad_16: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg21_1, [0, 0, sym_max_8, sym_max_8, 0, 0]);  arg21_1 = sym_max_8 = None\n",
            "    slice_15: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_16, 1, sym_max_9, sub_24);  pad_16 = sym_max_9 = sub_24 = None\n",
            "    div_5: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_21, 9.797958971132712);  transpose_21 = None\n",
            "    unsqueeze_9: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_15, 0);  slice_15 = None\n",
            "    transpose_25: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_9, -2, -1);  unsqueeze_9 = None\n",
            "    matmul_9: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_5, transpose_25);  div_5 = transpose_25 = None\n",
            "    pad_17: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_9, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_9 = None\n",
            "    mul_30: \"Sym(2*s12)\" = sym_size_int_8 * 2\n",
            "    mul_31: \"Sym(2*s12**2)\" = mul_30 * sym_size_int_8;  mul_30 = None\n",
            "    view_19: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_17, [sym_size_int_1, 2, mul_31]);  pad_17 = mul_31 = None\n",
            "    sub_25: \"Sym(s12 - 1)\" = sym_size_int_8 - 1\n",
            "    pad_18: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_19, [0, sub_25, 0, 0, 0, 0]);  view_19 = sub_25 = None\n",
            "    add_17: \"Sym(s12 + 1)\" = sym_size_int_8 + 1\n",
            "    mul_32: \"Sym(2*s12)\" = 2 * sym_size_int_8\n",
            "    sub_26: \"Sym(2*s12 - 1)\" = mul_32 - 1;  mul_32 = None\n",
            "    view_20: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_18, [sym_size_int_1, 2, add_17, sub_26]);  pad_18 = add_17 = sub_26 = None\n",
            "    sub_27: \"Sym(s12 - 1)\" = sym_size_int_8 - 1\n",
            "    slice_16: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_20, 2, None, sym_size_int_8);  view_20 = None\n",
            "    slice_17: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_16, 3, sub_27);  slice_16 = sub_27 = None\n",
            "    add_18: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_8, slice_17);  matmul_8 = slice_17 = None\n",
            "    eq_15: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill_2: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_18, eq_15, -10000.0);  add_18 = eq_15 = None\n",
            "    softmax_2: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_2, -1);  masked_fill_2 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_8: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_2, 0.1, False);  softmax_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_10: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_8, transpose_23);  transpose_23 = None\n",
            "    sub_28: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_19: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_8, [0, sub_28, 0, 0, 0, 0, 0, 0]);  dropout_8 = sub_28 = None\n",
            "    mul_33: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_29: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_34: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_29;  sub_29 = None\n",
            "    add_19: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_33 + mul_34;  mul_33 = mul_34 = None\n",
            "    view_21: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_19, [sym_size_int_5, 2, add_19]);  pad_19 = add_19 = None\n",
            "    pad_20: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_21, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_21 = None\n",
            "    mul_35: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_22: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_20, [sym_size_int_5, 2, sym_size_int_4, mul_35]);  pad_20 = mul_35 = None\n",
            "    le_4: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le_4 = None\n",
            "    slice_18: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_22, 0, 0, 9223372036854775807);  view_22 = None\n",
            "    le_5: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_5 = None\n",
            "    slice_19: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_18, 2, 0, 9223372036854775807);  slice_18 = None\n",
            "    slice_20: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_19, 3, 1, 9223372036854775807);  slice_19 = None\n",
            "    sub_30: \"Sym(s12 - 5)\" = sym_size_int_9 - 5\n",
            "    sym_max_10: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_30, 0);  sub_30 = None\n",
            "    sub_31: \"Sym(5 - s12)\" = 5 - sym_size_int_9\n",
            "    sym_max_11: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_31, 0);  sub_31 = None\n",
            "    mul_36: \"Sym(2*s12)\" = 2 * sym_size_int_9;  sym_size_int_9 = None\n",
            "    add_20: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_11 + mul_36;  mul_36 = None\n",
            "    sub_32: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_20 - 1;  add_20 = None\n",
            "    gt_5: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_10 > 0;  gt_5 = None\n",
            "    pad_21: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg22_1, [0, 0, sym_max_10, sym_max_10, 0, 0]);  arg22_1 = sym_max_10 = None\n",
            "    slice_21: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_21, 1, sym_max_11, sub_32);  pad_21 = sym_max_11 = sub_32 = None\n",
            "    unsqueeze_10: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_21, 0);  slice_21 = None\n",
            "    matmul_11: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_20, unsqueeze_10);  slice_20 = unsqueeze_10 = None\n",
            "    add_21: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_10, matmul_11);  matmul_10 = matmul_11 = None\n",
            "    transpose_26: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_21, 2, 3);  add_21 = None\n",
            "    contiguous_2: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_26);  transpose_26 = None\n",
            "    view_23: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_2, [sym_size_int_1, 192, sym_size_int_8]);  contiguous_2 = sym_size_int_8 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_15: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_23, arg29_1, arg30_1);  view_23 = arg29_1 = arg30_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_9: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_15, 0.1, False);  conv1d_15 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_22: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_20, dropout_9);  transpose_20 = dropout_9 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_27: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_22, 1, -1);  add_22 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_4: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_27, [192], arg65_1, arg66_1);  transpose_27 = arg65_1 = arg66_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_28: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_4, 1, -1);  layer_norm_4 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_37: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_28, type_as)\n",
            "    pad_22: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_37, [1, 1, 0, 0, 0, 0]);  mul_37 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_16: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_22, arg81_1, arg82_1);  pad_22 = arg81_1 = arg82_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_2: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_16);  conv1d_16 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_10: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_2, 0.1, False);  relu_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_38: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_10, type_as);  dropout_10 = None\n",
            "    pad_23: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_38, [1, 1, 0, 0, 0, 0]);  mul_38 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_17: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_23, arg83_1, arg84_1);  pad_23 = arg83_1 = arg84_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_39: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_17, type_as);  conv1d_17 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_11: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_39, 0.1, False);  mul_39 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_23: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_28, dropout_11);  transpose_28 = dropout_11 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_29: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_23, 1, -1);  add_23 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_5: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_29, [192], arg101_1, arg102_1);  transpose_29 = arg101_1 = arg102_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_30: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_5, 1, -1);  layer_norm_5 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_18: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_30, arg33_1, arg34_1);  arg33_1 = arg34_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_19: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_30, arg35_1, arg36_1);  arg35_1 = arg36_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_20: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_30, arg37_1, arg38_1);  arg37_1 = arg38_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_10: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_18, 2)\n",
            "    view_24: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_18, [sym_size_int_1, 2, 96, sym_size_int_10]);  conv1d_18 = None\n",
            "    transpose_31: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_24, 2, 3);  view_24 = None\n",
            "    sym_size_int_11: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_19, 2)\n",
            "    view_25: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_19, [sym_size_int_1, 2, 96, sym_size_int_11]);  conv1d_19 = None\n",
            "    transpose_32: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_25, 2, 3);  view_25 = None\n",
            "    view_26: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_20, [sym_size_int_1, 2, 96, sym_size_int_11]);  conv1d_20 = None\n",
            "    transpose_33: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_26, 2, 3);  view_26 = None\n",
            "    div_6: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_31, 9.797958971132712)\n",
            "    transpose_34: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_32, -2, -1);  transpose_32 = None\n",
            "    matmul_12: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_6, transpose_34);  div_6 = transpose_34 = None\n",
            "    eq_16: \"Sym(True)\" = sym_size_int_11 == sym_size_int_10;  eq_16 = None\n",
            "    sub_33: \"Sym(s12 - 5)\" = sym_size_int_11 - 5\n",
            "    sym_max_12: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_33, 0);  sub_33 = None\n",
            "    sub_34: \"Sym(5 - s12)\" = 5 - sym_size_int_11\n",
            "    sym_max_13: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_34, 0);  sub_34 = None\n",
            "    mul_40: \"Sym(2*s12)\" = 2 * sym_size_int_11\n",
            "    add_24: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_13 + mul_40;  mul_40 = None\n",
            "    sub_35: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_24 - 1;  add_24 = None\n",
            "    gt_6: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_12 > 0;  gt_6 = None\n",
            "    pad_24: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg31_1, [0, 0, sym_max_12, sym_max_12, 0, 0]);  arg31_1 = sym_max_12 = None\n",
            "    slice_22: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_24, 1, sym_max_13, sub_35);  pad_24 = sym_max_13 = sub_35 = None\n",
            "    div_7: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_31, 9.797958971132712);  transpose_31 = None\n",
            "    unsqueeze_11: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_22, 0);  slice_22 = None\n",
            "    transpose_35: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_11, -2, -1);  unsqueeze_11 = None\n",
            "    matmul_13: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_7, transpose_35);  div_7 = transpose_35 = None\n",
            "    pad_25: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_13, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_13 = None\n",
            "    mul_41: \"Sym(2*s12)\" = sym_size_int_10 * 2\n",
            "    mul_42: \"Sym(2*s12**2)\" = mul_41 * sym_size_int_10;  mul_41 = None\n",
            "    view_27: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_25, [sym_size_int_1, 2, mul_42]);  pad_25 = mul_42 = None\n",
            "    sub_36: \"Sym(s12 - 1)\" = sym_size_int_10 - 1\n",
            "    pad_26: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_27, [0, sub_36, 0, 0, 0, 0]);  view_27 = sub_36 = None\n",
            "    add_25: \"Sym(s12 + 1)\" = sym_size_int_10 + 1\n",
            "    mul_43: \"Sym(2*s12)\" = 2 * sym_size_int_10\n",
            "    sub_37: \"Sym(2*s12 - 1)\" = mul_43 - 1;  mul_43 = None\n",
            "    view_28: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_26, [sym_size_int_1, 2, add_25, sub_37]);  pad_26 = add_25 = sub_37 = None\n",
            "    sub_38: \"Sym(s12 - 1)\" = sym_size_int_10 - 1\n",
            "    slice_23: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_28, 2, None, sym_size_int_10);  view_28 = None\n",
            "    slice_24: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_23, 3, sub_38);  slice_23 = sub_38 = None\n",
            "    add_26: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_12, slice_24);  matmul_12 = slice_24 = None\n",
            "    eq_17: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill_3: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_26, eq_17, -10000.0);  add_26 = eq_17 = None\n",
            "    softmax_3: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_3, -1);  masked_fill_3 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_12: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_3, 0.1, False);  softmax_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_14: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_12, transpose_33);  transpose_33 = None\n",
            "    sub_39: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_27: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_12, [0, sub_39, 0, 0, 0, 0, 0, 0]);  dropout_12 = sub_39 = None\n",
            "    mul_44: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_40: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_45: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_40;  sub_40 = None\n",
            "    add_27: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_44 + mul_45;  mul_44 = mul_45 = None\n",
            "    view_29: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_27, [sym_size_int_5, 2, add_27]);  pad_27 = add_27 = None\n",
            "    pad_28: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_29, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_29 = None\n",
            "    mul_46: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_30: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_28, [sym_size_int_5, 2, sym_size_int_4, mul_46]);  pad_28 = mul_46 = None\n",
            "    le_6: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le_6 = None\n",
            "    slice_25: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_30, 0, 0, 9223372036854775807);  view_30 = None\n",
            "    le_7: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_7 = None\n",
            "    slice_26: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_25, 2, 0, 9223372036854775807);  slice_25 = None\n",
            "    slice_27: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_26, 3, 1, 9223372036854775807);  slice_26 = None\n",
            "    sub_41: \"Sym(s12 - 5)\" = sym_size_int_11 - 5\n",
            "    sym_max_14: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_41, 0);  sub_41 = None\n",
            "    sub_42: \"Sym(5 - s12)\" = 5 - sym_size_int_11\n",
            "    sym_max_15: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_42, 0);  sub_42 = None\n",
            "    mul_47: \"Sym(2*s12)\" = 2 * sym_size_int_11;  sym_size_int_11 = None\n",
            "    add_28: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_15 + mul_47;  mul_47 = None\n",
            "    sub_43: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_28 - 1;  add_28 = None\n",
            "    gt_7: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_14 > 0;  gt_7 = None\n",
            "    pad_29: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg32_1, [0, 0, sym_max_14, sym_max_14, 0, 0]);  arg32_1 = sym_max_14 = None\n",
            "    slice_28: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_29, 1, sym_max_15, sub_43);  pad_29 = sym_max_15 = sub_43 = None\n",
            "    unsqueeze_12: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_28, 0);  slice_28 = None\n",
            "    matmul_15: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_27, unsqueeze_12);  slice_27 = unsqueeze_12 = None\n",
            "    add_29: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_14, matmul_15);  matmul_14 = matmul_15 = None\n",
            "    transpose_36: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_29, 2, 3)\n",
            "    sym_numel_default_1: \"Sym(192*s12*s31)\" = torch.ops.aten.sym_numel.default(transpose_36)\n",
            "    eq_18: \"Sym(False)\" = sym_numel_default_1 == 0;  eq_18 = None\n",
            "    eq_19: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1;  eq_19 = None\n",
            "    eq_20: \"Sym(False)\" = sym_numel_default_1 == 0\n",
            "    eq_21: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1\n",
            "    or__5: \"Sym(Eq(s12, 1))\" = eq_21 | False;  eq_21 = None\n",
            "    and__4: \"Sym(Eq(s12, 1))\" = True & or__5;  or__5 = None\n",
            "    eq_22: \"Sym(Eq(s12, 1))\" = 1 == sym_size_int_4\n",
            "    or__6: \"Sym(Eq(s12, 1))\" = False | eq_22;  eq_22 = None\n",
            "    and__5: \"Sym(Eq(s12, 1))\" = and__4 & or__6;  and__4 = or__6 = None\n",
            "    mul_48: \"Sym(96*s12)\" = sym_size_int_4 * 96\n",
            "    sym_stride_int_2: \"Sym(96*s12)\" = torch.ops.aten.sym_stride.int(add_29, 1)\n",
            "    eq_23: \"Sym(True)\" = sym_stride_int_2 == mul_48;  sym_stride_int_2 = None\n",
            "    or__7: \"Sym(True)\" = False | eq_23;  eq_23 = None\n",
            "    and__6: \"Sym(Eq(s12, 1))\" = and__5 & or__7;  and__5 = or__7 = None\n",
            "    mul_49: \"Sym(192*s12)\" = mul_48 * 2;  mul_48 = None\n",
            "    eq_24: \"Sym(Eq(s31, 1))\" = sym_size_int_5 == 1\n",
            "    sym_stride_int_3: \"Sym(192*s12)\" = torch.ops.aten.sym_stride.int(add_29, 0);  add_29 = None\n",
            "    eq_25: \"Sym(True)\" = sym_stride_int_3 == mul_49;  sym_stride_int_3 = None\n",
            "    or__8: \"Sym(True)\" = eq_24 | eq_25;  eq_24 = eq_25 = None\n",
            "    and__7: \"Sym(Eq(s12, 1))\" = and__6 & or__8;  and__6 = or__8 = None\n",
            "    mul_50: \"Sym(192*s12*s31)\" = mul_49 * sym_size_int_5;  mul_49 = mul_50 = None\n",
            "    or__9: \"Sym(Eq(s12, 1))\" = and__7 | eq_20;  and__7 = eq_20 = or__9 = None\n",
            "    eq_26: \"Sym(False)\" = sym_numel_default_1 == 0;  sym_numel_default_1 = eq_26 = None\n",
            "    eq_27: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1;  eq_27 = None\n",
            "    contiguous_3: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_36);  transpose_36 = None\n",
            "    view_31: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_3, [sym_size_int_1, 192, sym_size_int_10]);  contiguous_3 = sym_size_int_10 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_21: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_31, arg39_1, arg40_1);  view_31 = arg39_1 = arg40_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_13: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_21, 0.1, False);  conv1d_21 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_30: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_30, dropout_13);  transpose_30 = dropout_13 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_37: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_30, 1, -1);  add_30 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_6: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_37, [192], arg67_1, arg68_1);  transpose_37 = arg67_1 = arg68_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_38: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_6, 1, -1);  layer_norm_6 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_51: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_38, type_as)\n",
            "    pad_30: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_51, [1, 1, 0, 0, 0, 0]);  mul_51 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_22: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_30, arg85_1, arg86_1);  pad_30 = arg85_1 = arg86_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_3: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_22);  conv1d_22 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_14: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_3, 0.1, False);  relu_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_52: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_14, type_as);  dropout_14 = None\n",
            "    pad_31: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_52, [1, 1, 0, 0, 0, 0]);  mul_52 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_23: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_31, arg87_1, arg88_1);  pad_31 = arg87_1 = arg88_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_53: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_23, type_as);  conv1d_23 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_15: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_53, 0.1, False);  mul_53 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_31: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_38, dropout_15);  transpose_38 = dropout_15 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_39: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_31, 1, -1);  add_31 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_7: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_39, [192], arg103_1, arg104_1);  transpose_39 = arg103_1 = arg104_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_40: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_7, 1, -1);  layer_norm_7 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_24: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_40, arg43_1, arg44_1);  arg43_1 = arg44_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_25: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_40, arg45_1, arg46_1);  arg45_1 = arg46_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_26: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_40, arg47_1, arg48_1);  arg47_1 = arg48_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_12: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_24, 2)\n",
            "    view_32: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_24, [sym_size_int_1, 2, 96, sym_size_int_12]);  conv1d_24 = None\n",
            "    transpose_41: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_32, 2, 3);  view_32 = None\n",
            "    sym_size_int_13: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_25, 2)\n",
            "    view_33: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_25, [sym_size_int_1, 2, 96, sym_size_int_13]);  conv1d_25 = None\n",
            "    transpose_42: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_33, 2, 3);  view_33 = None\n",
            "    view_34: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_26, [sym_size_int_1, 2, 96, sym_size_int_13]);  conv1d_26 = None\n",
            "    transpose_43: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_34, 2, 3);  view_34 = None\n",
            "    div_8: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_41, 9.797958971132712)\n",
            "    transpose_44: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_42, -2, -1);  transpose_42 = None\n",
            "    matmul_16: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_8, transpose_44);  div_8 = transpose_44 = None\n",
            "    eq_28: \"Sym(True)\" = sym_size_int_13 == sym_size_int_12;  eq_28 = None\n",
            "    sub_44: \"Sym(s12 - 5)\" = sym_size_int_13 - 5\n",
            "    sym_max_16: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_44, 0);  sub_44 = None\n",
            "    sub_45: \"Sym(5 - s12)\" = 5 - sym_size_int_13\n",
            "    sym_max_17: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_45, 0);  sub_45 = None\n",
            "    mul_54: \"Sym(2*s12)\" = 2 * sym_size_int_13\n",
            "    add_32: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_17 + mul_54;  mul_54 = None\n",
            "    sub_46: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_32 - 1;  add_32 = None\n",
            "    gt_8: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_16 > 0;  gt_8 = None\n",
            "    pad_32: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg41_1, [0, 0, sym_max_16, sym_max_16, 0, 0]);  arg41_1 = sym_max_16 = None\n",
            "    slice_29: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_32, 1, sym_max_17, sub_46);  pad_32 = sym_max_17 = sub_46 = None\n",
            "    div_9: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_41, 9.797958971132712);  transpose_41 = None\n",
            "    unsqueeze_13: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_29, 0);  slice_29 = None\n",
            "    transpose_45: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_13, -2, -1);  unsqueeze_13 = None\n",
            "    matmul_17: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_9, transpose_45);  div_9 = transpose_45 = None\n",
            "    pad_33: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_17, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_17 = None\n",
            "    mul_55: \"Sym(2*s12)\" = sym_size_int_12 * 2\n",
            "    mul_56: \"Sym(2*s12**2)\" = mul_55 * sym_size_int_12;  mul_55 = None\n",
            "    view_35: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_33, [sym_size_int_1, 2, mul_56]);  pad_33 = mul_56 = None\n",
            "    sub_47: \"Sym(s12 - 1)\" = sym_size_int_12 - 1\n",
            "    pad_34: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_35, [0, sub_47, 0, 0, 0, 0]);  view_35 = sub_47 = None\n",
            "    add_33: \"Sym(s12 + 1)\" = sym_size_int_12 + 1\n",
            "    mul_57: \"Sym(2*s12)\" = 2 * sym_size_int_12\n",
            "    sub_48: \"Sym(2*s12 - 1)\" = mul_57 - 1;  mul_57 = None\n",
            "    view_36: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_34, [sym_size_int_1, 2, add_33, sub_48]);  pad_34 = add_33 = sub_48 = None\n",
            "    sub_49: \"Sym(s12 - 1)\" = sym_size_int_12 - 1\n",
            "    slice_30: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_36, 2, None, sym_size_int_12);  view_36 = None\n",
            "    slice_31: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_30, 3, sub_49);  slice_30 = sub_49 = None\n",
            "    add_34: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_16, slice_31);  matmul_16 = slice_31 = None\n",
            "    eq_29: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill_4: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_34, eq_29, -10000.0);  add_34 = eq_29 = None\n",
            "    softmax_4: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_4, -1);  masked_fill_4 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_16: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_4, 0.1, False);  softmax_4 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_18: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_16, transpose_43);  transpose_43 = None\n",
            "    sub_50: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_35: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_16, [0, sub_50, 0, 0, 0, 0, 0, 0]);  dropout_16 = sub_50 = None\n",
            "    mul_58: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_51: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_59: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_51;  sub_51 = None\n",
            "    add_35: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_58 + mul_59;  mul_58 = mul_59 = None\n",
            "    view_37: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_35, [sym_size_int_5, 2, add_35]);  pad_35 = add_35 = None\n",
            "    pad_36: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_37, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_37 = None\n",
            "    mul_60: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_38: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_36, [sym_size_int_5, 2, sym_size_int_4, mul_60]);  pad_36 = mul_60 = None\n",
            "    le_8: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le_8 = None\n",
            "    slice_32: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_38, 0, 0, 9223372036854775807);  view_38 = None\n",
            "    le_9: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_9 = None\n",
            "    slice_33: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_32, 2, 0, 9223372036854775807);  slice_32 = None\n",
            "    slice_34: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_33, 3, 1, 9223372036854775807);  slice_33 = None\n",
            "    sub_52: \"Sym(s12 - 5)\" = sym_size_int_13 - 5\n",
            "    sym_max_18: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_52, 0);  sub_52 = None\n",
            "    sub_53: \"Sym(5 - s12)\" = 5 - sym_size_int_13\n",
            "    sym_max_19: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_53, 0);  sub_53 = None\n",
            "    mul_61: \"Sym(2*s12)\" = 2 * sym_size_int_13;  sym_size_int_13 = None\n",
            "    add_36: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_19 + mul_61;  mul_61 = None\n",
            "    sub_54: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_36 - 1;  add_36 = None\n",
            "    gt_9: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_18 > 0;  gt_9 = None\n",
            "    pad_37: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg42_1, [0, 0, sym_max_18, sym_max_18, 0, 0]);  arg42_1 = sym_max_18 = None\n",
            "    slice_35: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_37, 1, sym_max_19, sub_54);  pad_37 = sym_max_19 = sub_54 = None\n",
            "    unsqueeze_14: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_35, 0);  slice_35 = None\n",
            "    matmul_19: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_34, unsqueeze_14);  slice_34 = unsqueeze_14 = None\n",
            "    add_37: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_18, matmul_19);  matmul_18 = matmul_19 = None\n",
            "    transpose_46: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_37, 2, 3);  add_37 = None\n",
            "    contiguous_4: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_46);  transpose_46 = None\n",
            "    view_39: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_4, [sym_size_int_1, 192, sym_size_int_12]);  contiguous_4 = sym_size_int_12 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_27: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_39, arg49_1, arg50_1);  view_39 = arg49_1 = arg50_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_17: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_27, 0.1, False);  conv1d_27 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_38: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_40, dropout_17);  transpose_40 = dropout_17 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_47: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_38, 1, -1);  add_38 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_8: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_47, [192], arg69_1, arg70_1);  transpose_47 = arg69_1 = arg70_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_48: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_8, 1, -1);  layer_norm_8 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_62: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_48, type_as)\n",
            "    pad_38: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_62, [1, 1, 0, 0, 0, 0]);  mul_62 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_28: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_38, arg89_1, arg90_1);  pad_38 = arg89_1 = arg90_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_4: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_28);  conv1d_28 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_18: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_4, 0.1, False);  relu_4 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_63: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_18, type_as);  dropout_18 = None\n",
            "    pad_39: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_63, [1, 1, 0, 0, 0, 0]);  mul_63 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_29: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_39, arg91_1, arg92_1);  pad_39 = arg91_1 = arg92_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_64: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_29, type_as);  conv1d_29 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_19: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_64, 0.1, False);  mul_64 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_39: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_48, dropout_19);  transpose_48 = dropout_19 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_49: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_39, 1, -1);  add_39 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_9: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_49, [192], arg105_1, arg106_1);  transpose_49 = arg105_1 = arg106_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_50: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_9, 1, -1);  layer_norm_9 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_30: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_50, arg53_1, arg54_1);  arg53_1 = arg54_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_31: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_50, arg55_1, arg56_1);  arg55_1 = arg56_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_32: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_50, arg57_1, arg58_1);  arg57_1 = arg58_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_14: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_30, 2)\n",
            "    view_40: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_30, [sym_size_int_1, 2, 96, sym_size_int_14]);  conv1d_30 = None\n",
            "    transpose_51: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_40, 2, 3);  view_40 = None\n",
            "    sym_size_int_15: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_31, 2)\n",
            "    view_41: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_31, [sym_size_int_1, 2, 96, sym_size_int_15]);  conv1d_31 = None\n",
            "    transpose_52: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_41, 2, 3);  view_41 = None\n",
            "    view_42: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_32, [sym_size_int_1, 2, 96, sym_size_int_15]);  conv1d_32 = None\n",
            "    transpose_53: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_42, 2, 3);  view_42 = None\n",
            "    div_10: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_51, 9.797958971132712)\n",
            "    transpose_54: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_52, -2, -1);  transpose_52 = None\n",
            "    matmul_20: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_10, transpose_54);  div_10 = transpose_54 = None\n",
            "    eq_30: \"Sym(True)\" = sym_size_int_15 == sym_size_int_14;  eq_30 = None\n",
            "    sub_55: \"Sym(s12 - 5)\" = sym_size_int_15 - 5\n",
            "    sym_max_20: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_55, 0);  sub_55 = None\n",
            "    sub_56: \"Sym(5 - s12)\" = 5 - sym_size_int_15\n",
            "    sym_max_21: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_56, 0);  sub_56 = None\n",
            "    mul_65: \"Sym(2*s12)\" = 2 * sym_size_int_15\n",
            "    add_40: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_21 + mul_65;  mul_65 = None\n",
            "    sub_57: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_40 - 1;  add_40 = None\n",
            "    gt_10: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_20 > 0;  gt_10 = None\n",
            "    pad_40: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg51_1, [0, 0, sym_max_20, sym_max_20, 0, 0]);  arg51_1 = sym_max_20 = None\n",
            "    slice_36: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_40, 1, sym_max_21, sub_57);  pad_40 = sym_max_21 = sub_57 = None\n",
            "    div_11: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_51, 9.797958971132712);  transpose_51 = None\n",
            "    unsqueeze_15: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_36, 0);  slice_36 = None\n",
            "    transpose_55: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_15, -2, -1);  unsqueeze_15 = None\n",
            "    matmul_21: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_11, transpose_55);  div_11 = transpose_55 = None\n",
            "    pad_41: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_21, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_21 = None\n",
            "    mul_66: \"Sym(2*s12)\" = sym_size_int_14 * 2\n",
            "    mul_67: \"Sym(2*s12**2)\" = mul_66 * sym_size_int_14;  mul_66 = None\n",
            "    view_43: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_41, [sym_size_int_1, 2, mul_67]);  pad_41 = mul_67 = None\n",
            "    sub_58: \"Sym(s12 - 1)\" = sym_size_int_14 - 1\n",
            "    pad_42: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_43, [0, sub_58, 0, 0, 0, 0]);  view_43 = sub_58 = None\n",
            "    add_41: \"Sym(s12 + 1)\" = sym_size_int_14 + 1\n",
            "    mul_68: \"Sym(2*s12)\" = 2 * sym_size_int_14\n",
            "    sub_59: \"Sym(2*s12 - 1)\" = mul_68 - 1;  mul_68 = None\n",
            "    view_44: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_42, [sym_size_int_1, 2, add_41, sub_59]);  pad_42 = add_41 = sub_59 = None\n",
            "    sub_60: \"Sym(s12 - 1)\" = sym_size_int_14 - 1\n",
            "    slice_37: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_44, 2, None, sym_size_int_14);  view_44 = None\n",
            "    slice_38: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_37, 3, sub_60);  slice_37 = sub_60 = None\n",
            "    add_42: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_20, slice_38);  matmul_20 = slice_38 = None\n",
            "    eq_31: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0);  mul_2 = None\n",
            "    masked_fill_5: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_42, eq_31, -10000.0);  add_42 = eq_31 = None\n",
            "    softmax_5: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_5, -1);  masked_fill_5 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_20: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_5, 0.1, False);  softmax_5 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_22: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_20, transpose_53);  transpose_53 = None\n",
            "    sub_61: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_43: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_20, [0, sub_61, 0, 0, 0, 0, 0, 0]);  dropout_20 = sub_61 = None\n",
            "    mul_69: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_62: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_70: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_62;  sub_62 = None\n",
            "    add_43: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_69 + mul_70;  mul_69 = mul_70 = None\n",
            "    view_45: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_43, [sym_size_int_5, 2, add_43]);  pad_43 = add_43 = None\n",
            "    pad_44: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_45, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_45 = None\n",
            "    mul_71: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_46: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_44, [sym_size_int_5, 2, sym_size_int_4, mul_71]);  pad_44 = mul_71 = None\n",
            "    le_10: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  sym_size_int_5 = le_10 = None\n",
            "    slice_39: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_46, 0, 0, 9223372036854775807);  view_46 = None\n",
            "    le_11: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  sym_size_int_4 = le_11 = None\n",
            "    slice_40: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_39, 2, 0, 9223372036854775807);  slice_39 = None\n",
            "    slice_41: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_40, 3, 1, 9223372036854775807);  slice_40 = None\n",
            "    sub_63: \"Sym(s12 - 5)\" = sym_size_int_15 - 5\n",
            "    sym_max_22: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_63, 0);  sub_63 = None\n",
            "    sub_64: \"Sym(5 - s12)\" = 5 - sym_size_int_15\n",
            "    sym_max_23: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_64, 0);  sub_64 = None\n",
            "    mul_72: \"Sym(2*s12)\" = 2 * sym_size_int_15;  sym_size_int_15 = None\n",
            "    add_44: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_23 + mul_72;  mul_72 = None\n",
            "    sub_65: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_44 - 1;  add_44 = None\n",
            "    gt_11: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_22 > 0;  gt_11 = None\n",
            "    pad_45: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg52_1, [0, 0, sym_max_22, sym_max_22, 0, 0]);  arg52_1 = sym_max_22 = None\n",
            "    slice_42: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_45, 1, sym_max_23, sub_65);  pad_45 = sym_max_23 = sub_65 = None\n",
            "    unsqueeze_16: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_42, 0);  slice_42 = None\n",
            "    matmul_23: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_41, unsqueeze_16);  slice_41 = unsqueeze_16 = None\n",
            "    add_45: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_22, matmul_23);  matmul_22 = matmul_23 = None\n",
            "    transpose_56: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_45, 2, 3);  add_45 = None\n",
            "    contiguous_5: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_56);  transpose_56 = None\n",
            "    view_47: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_5, [sym_size_int_1, 192, sym_size_int_14]);  contiguous_5 = sym_size_int_14 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_33: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_47, arg59_1, arg60_1);  view_47 = arg59_1 = arg60_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_21: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_33, 0.1, False);  conv1d_33 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_46: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_50, dropout_21);  transpose_50 = dropout_21 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_57: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_46, 1, -1);  add_46 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_10: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_57, [192], arg71_1, arg72_1);  transpose_57 = arg71_1 = arg72_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_58: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_10, 1, -1);  layer_norm_10 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_73: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_58, type_as)\n",
            "    pad_46: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_73, [1, 1, 0, 0, 0, 0]);  mul_73 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_34: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_46, arg93_1, arg94_1);  pad_46 = arg93_1 = arg94_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_5: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_34);  conv1d_34 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_22: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_5, 0.1, False);  relu_5 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_74: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_22, type_as);  dropout_22 = None\n",
            "    pad_47: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_74, [1, 1, 0, 0, 0, 0]);  mul_74 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_35: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_47, arg95_1, arg96_1);  pad_47 = arg95_1 = arg96_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_75: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_35, type_as);  conv1d_35 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_23: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_75, 0.1, False);  mul_75 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_47: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_58, dropout_23);  transpose_58 = dropout_23 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_59: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_47, 1, -1);  add_47 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_11: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_59, [192], arg107_1, arg108_1);  transpose_59 = arg107_1 = arg108_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_60: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_11, 1, -1);  layer_norm_11 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:73 in forward, code: x = x * x_mask\n",
            "    mul_76: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_60, type_as);  transpose_60 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_36: \"f32[s31, 384, s12]\" = torch.ops.aten.conv1d.default(mul_76, arg109_1, arg110_1);  arg109_1 = arg110_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:206 in forward, code: stats = self.proj(x) * x_mask\n",
            "    mul_77: \"f32[s31, 384, s12]\" = torch.ops.aten.mul.Tensor(conv1d_36, type_as);  conv1d_36 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:208 in forward, code: m, logs = torch.split(stats, self.out_channels, dim=1)\n",
            "    split = torch.ops.aten.split.Tensor(mul_77, 192, 1);  mul_77 = None\n",
            "    getitem: \"f32[s31, 192, s12]\" = split[0];  getitem = None\n",
            "    getitem_1: \"f32[s31, 192, s12]\" = split[1];  split = getitem_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:64 in forward, code: x = torch.detach(x)\n",
            "    detach: \"f32[s31, 192, s12]\" = torch.ops.aten.detach.default(mul_76);  mul_76 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_37: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(detach, arg624_1, arg625_1);  detach = arg624_1 = arg625_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_78: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_37, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_38: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_78, arg628_1, arg629_1, [1], [1], [1], 192);  mul_78 = arg628_1 = arg629_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_61: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_38, 1, -1);  conv1d_38 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_12: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_61, [192], arg640_1, arg641_1);  transpose_61 = arg640_1 = arg641_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_62: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_12, 1, -1);  layer_norm_12 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_62);  transpose_62 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_39: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu, arg634_1, arg635_1);  gelu = arg634_1 = arg635_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_63: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_39, 1, -1);  conv1d_39 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_13: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_63, [192], arg646_1, arg647_1);  transpose_63 = arg646_1 = arg647_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_64: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_13, 1, -1);  layer_norm_13 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_1: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_64);  transpose_64 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_24: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_1, 0.5, False);  gelu_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_48: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(conv1d_37, dropout_24);  conv1d_37 = dropout_24 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_79: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_48, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_40: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_79, arg630_1, arg631_1, [1], [3], [3], 192);  mul_79 = arg630_1 = arg631_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_65: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_40, 1, -1);  conv1d_40 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_14: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_65, [192], arg642_1, arg643_1);  transpose_65 = arg642_1 = arg643_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_66: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_14, 1, -1);  layer_norm_14 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_2: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_66);  transpose_66 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_41: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_2, arg636_1, arg637_1);  gelu_2 = arg636_1 = arg637_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_67: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_41, 1, -1);  conv1d_41 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_15: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_67, [192], arg648_1, arg649_1);  transpose_67 = arg648_1 = arg649_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_68: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_15, 1, -1);  layer_norm_15 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_3: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_68);  transpose_68 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_25: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_3, 0.5, False);  gelu_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_49: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_48, dropout_25);  add_48 = dropout_25 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_80: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_49, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_42: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_80, arg632_1, arg633_1, [1], [9], [9], 192);  mul_80 = arg632_1 = arg633_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_69: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_42, 1, -1);  conv1d_42 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_16: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_69, [192], arg644_1, arg645_1);  transpose_69 = arg644_1 = arg645_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_70: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_16, 1, -1);  layer_norm_16 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_4: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_70);  transpose_70 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_43: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_4, arg638_1, arg639_1);  gelu_4 = arg638_1 = arg639_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_71: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_43, 1, -1);  conv1d_43 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_17: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_71, [192], arg650_1, arg651_1);  transpose_71 = arg650_1 = arg651_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_72: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_17, 1, -1);  layer_norm_17 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_5: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_72);  transpose_72 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_26: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_5, 0.5, False);  gelu_5 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_50: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_49, dropout_26);  add_49 = dropout_26 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:129 in forward, code: return x * x_mask\n",
            "    mul_81: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_50, type_as);  add_50 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_44: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_81, arg626_1, arg627_1);  mul_81 = arg626_1 = arg627_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:70 in forward, code: x = self.proj(x) * x_mask\n",
            "    mul_82: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_44, type_as)\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:111 in forward, code: z = torch.randn(x.size(0), 2, x.size(2)).type_as(x) * noise_scale\n",
            "    sym_size_int_16: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_44, 2);  conv1d_44 = None\n",
            "    randn: \"f32[s31, 2, s12]\" = torch.ops.aten.randn.default([sym_size_int_1, 2, sym_size_int_16], device = device(type='cpu'), pin_memory = False)\n",
            "    type_as_1: \"f32[s31, 2, s12]\" = torch.ops.aten.type_as.default(randn, mul_82);  randn = None\n",
            "    mul_83: \"f32[s31, 2, s12]\" = torch.ops.aten.mul.Tensor(type_as_1, select_2);  type_as_1 = select_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:386 in forward, code: x = torch.flip(x, [1])\n",
            "    flip: \"f32[s31, 2, s12]\" = torch.ops.aten.flip.default(mul_83, [1]);  mul_83 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:497 in forward, code: x0, x1 = torch.split(x, [self.half_channels] * 2, 1)\n",
            "    split_with_sizes = torch.ops.aten.split_with_sizes.default(flip, [1, 1], 1);  flip = None\n",
            "    getitem_2: \"f32[s31, 1, s12]\" = split_with_sizes[0]\n",
            "    getitem_3: \"f32[s31, 1, s12]\" = split_with_sizes[1];  split_with_sizes = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_45: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(getitem_2, arg454_1, arg455_1);  getitem_2 = arg454_1 = arg455_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:119 in forward, code: x = x + g\n",
            "    add_51: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(conv1d_45, mul_82);  conv1d_45 = mul_82 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_84: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_51, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_46: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_84, arg456_1, arg457_1, [1], [1], [1], 192);  mul_84 = arg456_1 = arg457_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_73: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_46, 1, -1);  conv1d_46 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_18: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_73, [192], arg468_1, arg469_1);  transpose_73 = arg468_1 = arg469_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_74: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_18, 1, -1);  layer_norm_18 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_6: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_74);  transpose_74 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_47: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_6, arg462_1, arg463_1);  gelu_6 = arg462_1 = arg463_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_75: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_47, 1, -1);  conv1d_47 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_19: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_75, [192], arg474_1, arg475_1);  transpose_75 = arg474_1 = arg475_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_76: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_19, 1, -1);  layer_norm_19 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_7: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_76);  transpose_76 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_27: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_7, 0.0, False);  gelu_7 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_52: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_51, dropout_27);  add_51 = dropout_27 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_85: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_52, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_48: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_85, arg458_1, arg459_1, [1], [3], [3], 192);  mul_85 = arg458_1 = arg459_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_77: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_48, 1, -1);  conv1d_48 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_20: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_77, [192], arg470_1, arg471_1);  transpose_77 = arg470_1 = arg471_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_78: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_20, 1, -1);  layer_norm_20 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_8: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_78);  transpose_78 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_49: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_8, arg464_1, arg465_1);  gelu_8 = arg464_1 = arg465_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_79: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_49, 1, -1);  conv1d_49 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_21: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_79, [192], arg476_1, arg477_1);  transpose_79 = arg476_1 = arg477_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_80: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_21, 1, -1);  layer_norm_21 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_9: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_80);  transpose_80 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_28: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_9, 0.0, False);  gelu_9 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_53: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_52, dropout_28);  add_52 = dropout_28 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_86: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_53, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_50: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_86, arg460_1, arg461_1, [1], [9], [9], 192);  mul_86 = arg460_1 = arg461_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_81: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_50, 1, -1);  conv1d_50 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_22: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_81, [192], arg472_1, arg473_1);  transpose_81 = arg472_1 = arg473_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_82: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_22, 1, -1);  layer_norm_22 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_10: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_82);  transpose_82 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_51: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_10, arg466_1, arg467_1);  gelu_10 = arg466_1 = arg467_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_83: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_51, 1, -1);  conv1d_51 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_23: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_83, [192], arg478_1, arg479_1);  transpose_83 = arg478_1 = arg479_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_84: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_23, 1, -1);  layer_norm_23 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_11: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_84);  transpose_84 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_29: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_11, 0.0, False);  gelu_11 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_54: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_53, dropout_29);  add_53 = dropout_29 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:129 in forward, code: return x * x_mask\n",
            "    mul_87: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_54, type_as);  add_54 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_52: \"f32[s31, 29, s12]\" = torch.ops.aten.conv1d.default(mul_87, arg480_1, arg481_1);  mul_87 = arg480_1 = arg481_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:500 in forward, code: h = self.proj(h) * x_mask\n",
            "    mul_88: \"f32[s31, 29, s12]\" = torch.ops.aten.mul.Tensor(conv1d_52, type_as);  conv1d_52 = type_as = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:503 in forward, code: h = h.reshape(b, c, -1, t).permute(0, 1, 3, 2)  # [b, cx?, t] -> [b, c, t, ?]\n",
            "    reshape: \"f32[s31, 1, 29, s12]\" = torch.ops.aten.reshape.default(mul_88, [sym_size_int_1, 1, -1, sym_size_int_16]);  mul_88 = sym_size_int_1 = sym_size_int_16 = None\n",
            "    permute: \"f32[s31, 1, s12, 29]\" = torch.ops.aten.permute.default(reshape, [0, 1, 3, 2]);  reshape = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:505 in forward, code: unnormalized_widths = h[..., : self.num_bins] / math.sqrt(self.filter_channels)\n",
            "    slice_43: \"f32[s31, 1, s12, 10]\" = torch.ops.aten.slice.Tensor(permute, 3, 0, 10)\n",
            "    div_12: \"f32[s31, 1, s12, 10]\" = torch.ops.aten.div.Tensor(slice_43, 13.856406460551018);  slice_43 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:506 in forward, code: unnormalized_heights = h[..., self.num_bins : 2 * self.num_bins] / math.sqrt(\n",
            "    slice_44: \"f32[s31, 1, s12, 10]\" = torch.ops.aten.slice.Tensor(permute, 3, 10, 20)\n",
            "    div_13: \"f32[s31, 1, s12, 10]\" = torch.ops.aten.div.Tensor(slice_44, 13.856406460551018);  slice_44 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:509 in forward, code: unnormalized_derivatives = h[..., 2 * self.num_bins :]\n",
            "    slice_45: \"f32[s31, 1, s12, 9]\" = torch.ops.aten.slice.Tensor(permute, 3, 20, 9223372036854775807);  permute = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:511 in forward, code: x1, logabsdet = piecewise_rational_quadratic_transform(\n",
            "    ge: \"b8[s31, 1, s12]\" = torch.ops.aten.ge.Scalar(getitem_3, -5.0)\n",
            "    le_12: \"b8[s31, 1, s12]\" = torch.ops.aten.le.Scalar(getitem_3, 5.0)\n",
            "    and_1: \"b8[s31, 1, s12]\" = torch.ops.aten.__and__.Tensor(ge, le_12);  ge = le_12 = None\n",
            "    bitwise_not: \"b8[s31, 1, s12]\" = torch.ops.aten.bitwise_not.default(and_1)\n",
            "    zeros_like: \"f32[s31, 1, s12]\" = torch.ops.aten.zeros_like.default(getitem_3, pin_memory = False)\n",
            "    zeros_like_1: \"f32[s31, 1, s12]\" = torch.ops.aten.zeros_like.default(getitem_3, pin_memory = False)\n",
            "    pad_48: \"f32[s31, 1, s12, 11]\" = torch.ops.aten.pad.default(slice_45, [1, 1]);  slice_45 = None\n",
            "    _tensor_constant0: \"f32[]\" = self._tensor_constant0\n",
            "    lift_fresh_copy: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None\n",
            "    select_3: \"f32[s31, 1, s12]\" = torch.ops.aten.select.int(pad_48, 3, 0)\n",
            "    fill_: \"f32[s31, 1, s12]\" = torch.ops.aten.fill_.Tensor(select_3, lift_fresh_copy);  select_3 = lift_fresh_copy = fill_ = None\n",
            "    _tensor_constant1: \"f32[]\" = self._tensor_constant1\n",
            "    lift_fresh_copy_1: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None\n",
            "    select_4: \"f32[s31, 1, s12]\" = torch.ops.aten.select.int(pad_48, 3, 10)\n",
            "    fill__1: \"f32[s31, 1, s12]\" = torch.ops.aten.fill_.Tensor(select_4, lift_fresh_copy_1);  select_4 = lift_fresh_copy_1 = fill__1 = None\n",
            "    index: \"f32[u0]\" = torch.ops.aten.index.Tensor(getitem_3, [bitwise_not])\n",
            "    sym_size_int_17: \"Sym(u0)\" = torch.ops.aten.sym_size.int(index, 0)\n",
            "    eq_32: \"Sym(True)\" = sym_size_int_17 == sym_size_int_17;  sym_size_int_17 = eq_32 = None\n",
            "    index_put_: \"f32[s31, 1, s12]\" = torch.ops.aten.index_put_.default(zeros_like, [bitwise_not], index);  zeros_like = index = index_put_ = None\n",
            "    _tensor_constant2: \"f32[]\" = self._tensor_constant2\n",
            "    lift_fresh_copy_2: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant2);  _tensor_constant2 = None\n",
            "    index_put__1: \"f32[s31, 1, s12]\" = torch.ops.aten.index_put_.default(zeros_like_1, [bitwise_not], lift_fresh_copy_2);  zeros_like_1 = bitwise_not = lift_fresh_copy_2 = index_put__1 = None\n",
            "    index_1: \"f32[u1]\" = torch.ops.aten.index.Tensor(getitem_3, [and_1]);  getitem_3 = None\n",
            "    index_2: \"f32[u1, 10]\" = torch.ops.aten.index.Tensor(div_12, [and_1]);  div_12 = None\n",
            "    index_3: \"f32[u1, 10]\" = torch.ops.aten.index.Tensor(div_13, [and_1]);  div_13 = None\n",
            "    index_4: \"f32[u1, 11]\" = torch.ops.aten.index.Tensor(pad_48, [and_1]);  pad_48 = and_1 = None\n",
            "    softmax_6: \"f32[u1, 10]\" = torch.ops.aten.softmax.int(index_2, -1);  index_2 = None\n",
            "    mul_89: \"f32[u1, 10]\" = torch.ops.aten.mul.Tensor(softmax_6, 0.99);  softmax_6 = None\n",
            "    add_55: \"f32[u1, 10]\" = torch.ops.aten.add.Tensor(mul_89, 0.001);  mul_89 = None\n",
            "    cumsum: \"f32[u1, 10]\" = torch.ops.aten.cumsum.default(add_55, -1);  add_55 = None\n",
            "    pad_49: \"f32[u1, 11]\" = torch.ops.aten.pad.default(cumsum, [1, 0], 'constant', 0.0);  cumsum = None\n",
            "    mul_90: \"f32[u1, 11]\" = torch.ops.aten.mul.Tensor(pad_49, 10.0);  pad_49 = None\n",
            "    add_56: \"f32[u1, 11]\" = torch.ops.aten.add.Tensor(mul_90, -5.0);  mul_90 = None\n",
            "    _tensor_constant3: \"f32[]\" = self._tensor_constant3\n",
            "    lift_fresh_copy_3: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant3);  _tensor_constant3 = None\n",
            "    select_5: \"f32[u1]\" = torch.ops.aten.select.int(add_56, 1, 0)\n",
            "    fill__2: \"f32[u1]\" = torch.ops.aten.fill_.Tensor(select_5, lift_fresh_copy_3);  select_5 = lift_fresh_copy_3 = fill__2 = None\n",
            "    _tensor_constant4: \"f32[]\" = self._tensor_constant4\n",
            "    lift_fresh_copy_4: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant4);  _tensor_constant4 = None\n",
            "    select_6: \"f32[u1]\" = torch.ops.aten.select.int(add_56, 1, 10)\n",
            "    fill__3: \"f32[u1]\" = torch.ops.aten.fill_.Tensor(select_6, lift_fresh_copy_4);  select_6 = lift_fresh_copy_4 = fill__3 = None\n",
            "    slice_46: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_56, 1, 1, 9223372036854775807)\n",
            "    slice_47: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_56, 1, 0, -1)\n",
            "    sub_66: \"f32[u1, 10]\" = torch.ops.aten.sub.Tensor(slice_46, slice_47);  slice_46 = slice_47 = None\n",
            "    softplus: \"f32[u1, 11]\" = torch.ops.aten.softplus.default(index_4);  index_4 = None\n",
            "    add_57: \"f32[u1, 11]\" = torch.ops.aten.add.Tensor(softplus, 0.001);  softplus = None\n",
            "    softmax_7: \"f32[u1, 10]\" = torch.ops.aten.softmax.int(index_3, -1);  index_3 = None\n",
            "    mul_91: \"f32[u1, 10]\" = torch.ops.aten.mul.Tensor(softmax_7, 0.99);  softmax_7 = None\n",
            "    add_58: \"f32[u1, 10]\" = torch.ops.aten.add.Tensor(mul_91, 0.001);  mul_91 = None\n",
            "    cumsum_1: \"f32[u1, 10]\" = torch.ops.aten.cumsum.default(add_58, -1);  add_58 = None\n",
            "    pad_50: \"f32[u1, 11]\" = torch.ops.aten.pad.default(cumsum_1, [1, 0], 'constant', 0.0);  cumsum_1 = None\n",
            "    mul_92: \"f32[u1, 11]\" = torch.ops.aten.mul.Tensor(pad_50, 10.0);  pad_50 = None\n",
            "    add_59: \"f32[u1, 11]\" = torch.ops.aten.add.Tensor(mul_92, -5.0);  mul_92 = None\n",
            "    _tensor_constant5: \"f32[]\" = self._tensor_constant5\n",
            "    lift_fresh_copy_5: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant5);  _tensor_constant5 = None\n",
            "    select_7: \"f32[u1]\" = torch.ops.aten.select.int(add_59, 1, 0)\n",
            "    fill__4: \"f32[u1]\" = torch.ops.aten.fill_.Tensor(select_7, lift_fresh_copy_5);  select_7 = lift_fresh_copy_5 = fill__4 = None\n",
            "    _tensor_constant6: \"f32[]\" = self._tensor_constant6\n",
            "    lift_fresh_copy_6: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant6);  _tensor_constant6 = None\n",
            "    select_8: \"f32[u1]\" = torch.ops.aten.select.int(add_59, 1, 10)\n",
            "    fill__5: \"f32[u1]\" = torch.ops.aten.fill_.Tensor(select_8, lift_fresh_copy_6);  select_8 = lift_fresh_copy_6 = fill__5 = None\n",
            "    slice_48: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_59, 1, 1, 9223372036854775807)\n",
            "    slice_49: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_59, 1, 0, -1)\n",
            "    sub_67: \"f32[u1, 10]\" = torch.ops.aten.sub.Tensor(slice_48, slice_49);  slice_48 = slice_49 = None\n",
            "    select_9: \"f32[u1]\" = torch.ops.aten.select.int(add_59, 1, 10)\n",
            "    add_: \"f32[u1]\" = torch.ops.aten.add_.Tensor(select_9, 1e-06);  select_9 = None\n",
            "    select_10: \"f32[u1]\" = torch.ops.aten.select.int(add_59, 1, 10)\n",
            "    sym_size_int_18: \"Sym(u1)\" = torch.ops.aten.sym_size.int(index_1, 0)\n",
            "    eq_33: \"Sym(True)\" = sym_size_int_18 == sym_size_int_18;  sym_size_int_18 = eq_33 = None\n",
            "    copy_: \"f32[u1]\" = torch.ops.aten.copy_.default(select_10, add_);  select_10 = add_ = copy_ = None\n",
            "    unsqueeze_17: \"f32[u1, 1]\" = torch.ops.aten.unsqueeze.default(index_1, 1)\n",
            "    ge_1: \"b8[u1, 11]\" = torch.ops.aten.ge.Tensor(unsqueeze_17, add_59);  unsqueeze_17 = None\n",
            "    sum_1: \"i64[u1]\" = torch.ops.aten.sum.dim_IntList(ge_1, [-1]);  ge_1 = None\n",
            "    sub_68: \"i64[u1]\" = torch.ops.aten.sub.Tensor(sum_1, 1);  sum_1 = None\n",
            "    unsqueeze_18: \"i64[u1, 1]\" = torch.ops.aten.unsqueeze.default(sub_68, 1);  sub_68 = None\n",
            "    gather: \"f32[u1, 1]\" = torch.ops.aten.gather.default(add_56, -1, unsqueeze_18);  add_56 = None\n",
            "    select_11: \"f32[u1]\" = torch.ops.aten.select.int(gather, 1, 0);  gather = select_11 = None\n",
            "    gather_1: \"f32[u1, 1]\" = torch.ops.aten.gather.default(sub_66, -1, unsqueeze_18)\n",
            "    select_12: \"f32[u1]\" = torch.ops.aten.select.int(gather_1, 1, 0);  gather_1 = select_12 = None\n",
            "    gather_2: \"f32[u1, 1]\" = torch.ops.aten.gather.default(add_59, -1, unsqueeze_18);  add_59 = None\n",
            "    select_13: \"f32[u1]\" = torch.ops.aten.select.int(gather_2, 1, 0);  gather_2 = None\n",
            "    div_14: \"f32[u1, 10]\" = torch.ops.aten.div.Tensor(sub_67, sub_66);  sub_66 = None\n",
            "    gather_3: \"f32[u1, 1]\" = torch.ops.aten.gather.default(div_14, -1, unsqueeze_18);  div_14 = None\n",
            "    select_14: \"f32[u1]\" = torch.ops.aten.select.int(gather_3, 1, 0);  gather_3 = None\n",
            "    gather_4: \"f32[u1, 1]\" = torch.ops.aten.gather.default(add_57, -1, unsqueeze_18)\n",
            "    select_15: \"f32[u1]\" = torch.ops.aten.select.int(gather_4, 1, 0);  gather_4 = None\n",
            "    slice_50: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_57, 1, 1, 9223372036854775807);  add_57 = None\n",
            "    gather_5: \"f32[u1, 1]\" = torch.ops.aten.gather.default(slice_50, -1, unsqueeze_18);  slice_50 = None\n",
            "    select_16: \"f32[u1]\" = torch.ops.aten.select.int(gather_5, 1, 0);  gather_5 = None\n",
            "    gather_6: \"f32[u1, 1]\" = torch.ops.aten.gather.default(sub_67, -1, unsqueeze_18);  sub_67 = unsqueeze_18 = None\n",
            "    select_17: \"f32[u1]\" = torch.ops.aten.select.int(gather_6, 1, 0);  gather_6 = None\n",
            "    sub_69: \"f32[u1]\" = torch.ops.aten.sub.Tensor(index_1, select_13)\n",
            "    add_60: \"f32[u1]\" = torch.ops.aten.add.Tensor(select_15, select_16)\n",
            "    mul_93: \"f32[u1]\" = torch.ops.aten.mul.Tensor(select_14, 2)\n",
            "    sub_70: \"f32[u1]\" = torch.ops.aten.sub.Tensor(add_60, mul_93);  add_60 = mul_93 = None\n",
            "    mul_94: \"f32[u1]\" = torch.ops.aten.mul.Tensor(sub_69, sub_70);  sub_69 = sub_70 = None\n",
            "    sub_71: \"f32[u1]\" = torch.ops.aten.sub.Tensor(select_14, select_15)\n",
            "    mul_95: \"f32[u1]\" = torch.ops.aten.mul.Tensor(select_17, sub_71);  sub_71 = None\n",
            "    add_61: \"f32[u1]\" = torch.ops.aten.add.Tensor(mul_94, mul_95);  mul_94 = mul_95 = None\n",
            "    mul_96: \"f32[u1]\" = torch.ops.aten.mul.Tensor(select_17, select_15);  select_17 = None\n",
            "    sub_72: \"f32[u1]\" = torch.ops.aten.sub.Tensor(index_1, select_13)\n",
            "    add_62: \"f32[u1]\" = torch.ops.aten.add.Tensor(select_15, select_16);  select_15 = select_16 = None\n",
            "    mul_97: \"f32[u1]\" = torch.ops.aten.mul.Tensor(select_14, 2)\n",
            "    sub_73: \"f32[u1]\" = torch.ops.aten.sub.Tensor(add_62, mul_97);  add_62 = mul_97 = None\n",
            "    mul_98: \"f32[u1]\" = torch.ops.aten.mul.Tensor(sub_72, sub_73);  sub_72 = sub_73 = None\n",
            "    sub_74: \"f32[u1]\" = torch.ops.aten.sub.Tensor(mul_96, mul_98);  mul_96 = mul_98 = None\n",
            "    neg: \"f32[u1]\" = torch.ops.aten.neg.default(select_14);  select_14 = None\n",
            "    sub_75: \"f32[u1]\" = torch.ops.aten.sub.Tensor(index_1, select_13);  index_1 = select_13 = None\n",
            "    mul_99: \"f32[u1]\" = torch.ops.aten.mul.Tensor(neg, sub_75);  neg = sub_75 = None\n",
            "    pow_1: \"f32[u1]\" = torch.ops.aten.pow.Tensor_Scalar(sub_74, 2);  sub_74 = None\n",
            "    mul_100: \"f32[u1]\" = torch.ops.aten.mul.Tensor(add_61, 4);  add_61 = None\n",
            "    mul_101: \"f32[u1]\" = torch.ops.aten.mul.Tensor(mul_100, mul_99);  mul_100 = mul_99 = None\n",
            "    sub_76: \"f32[u1]\" = torch.ops.aten.sub.Tensor(pow_1, mul_101);  pow_1 = mul_101 = None\n",
            "    ge_2: \"b8[u1]\" = torch.ops.aten.ge.Scalar(sub_76, 0);  sub_76 = None\n",
            "    all_1: \"b8[]\" = torch.ops.aten.all.default(ge_2);  ge_2 = None\n",
            "    ne: \"b8[]\" = torch.ops.aten.ne.Scalar(all_1, 0);  all_1 = None\n",
            "    item: \"Sym(Eq(u2, 1))\" = torch.ops.aten.item.default(ne);  ne = item = None\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Error during export: Failed to export the model with torch.export. \u001b[96mThis is step 1/3\u001b[0m of exporting the model to ONNX. Next steps:\n",
            "- Modify the model code for `torch.export.export` to succeed. Refer to https://pytorch.org/docs/stable/generated/exportdb/index.html for more information.\n",
            "- Debug `torch.export.export` and submit a PR to PyTorch.\n",
            "- Create an issue in the PyTorch GitHub repository against the \u001b[96m*torch.export*\u001b[0m component and attach the full error stack as well as reproduction scripts.\n",
            "\n",
            "## Exception summary\n",
            "\n",
            "<class 'torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode'>: Could not guard on data-dependent expression Eq(u2, 1) (unhinted: Eq(u2, 1)).  (Size-like symbols: none)\n",
            "\n",
            "consider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (_export/non_strict_utils.py:1066 in __torch_function__)\n",
            "For more information, run with TORCH_LOGS=\"dynamic\"\n",
            "For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u2\"\n",
            "If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n",
            "For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n",
            "\n",
            "For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n",
            "\n",
            "The following call raised this error:\n",
            "  File \"/content/piper1-gpl/src/piper/train/vits/transforms.py\", line 174, in rational_quadratic_spline\n",
            "    assert (discriminant >= 0).all(), discriminant\n",
            "\n",
            "\n",
            "The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.\n",
            "\n",
            "(Refer to the full stack trace above for more information.)\n",
            "\n",
            "âœ… Model exported to: /content/output/en_IN-spicor-medium/en_IN-spicor-medium_en-US-epochs-3000.onnx\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“¦ **8. Export to ONNX** ğŸ“¦\n",
        "\n",
        "Export the trained model to ONNX format for production inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llnUug_pGQk1",
        "outputId": "9533be1a-9731-44cf-8058-629a5d2f54cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: /content/output/en_IN-spicor-medium/en_IN-spicor-medium.onnx not found, skipping.\n",
            "Copying /content/output/en_IN-spicor-medium/en_IN-spicor-medium.json -> /content/drive/MyDrive/Piper-POC-Training/output/en_IN-spicor-medium/en_IN-spicor-medium.onnx.json\n",
            "Copying /content/output/en_IN-spicor-medium/checkpoints/last.ckpt -> /content/drive/MyDrive/Piper-POC-Training/output/en_IN-spicor-medium/last.ckpt\n",
            "\n",
            "âœ… Model saved to Google Drive: /content/drive/MyDrive/Piper-POC-Training/output/en_IN-spicor-medium\n",
            "\n",
            "Files saved:\n",
            "  - lightning_logs (0.00 MB)\n",
            "  - last.ckpt (806.70 MB)\n",
            "  - en_IN-spicor-medium.json (0.00 MB)\n",
            "  - en_IN-spicor-medium.onnx.json (0.00 MB)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ONNX EXPORTER\n",
        "# =============================================================================\n",
        "# Export trained model to ONNX format.\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "\n",
        "class ONNXExporter:\n",
        "    \"\"\"Export Piper model to ONNX format for production inference.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: PiperConfig):\n",
        "        self.config = config\n",
        "    \n",
        "    def export(\n",
        "        self,\n",
        "        checkpoint_path: str,\n",
        "        output_path: str,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Export model checkpoint to ONNX format.\n",
        "        \n",
        "        Args:\n",
        "            checkpoint_path: Path to .ckpt checkpoint file\n",
        "            output_path: Path for output .onnx file\n",
        "            \n",
        "        Returns:\n",
        "            str: Path to exported ONNX file\n",
        "            \n",
        "        Raises:\n",
        "            FileNotFoundError: If checkpoint doesn't exist\n",
        "            RuntimeError: If export fails\n",
        "        \"\"\"\n",
        "        if not os.path.exists(checkpoint_path):\n",
        "            raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
        "        \n",
        "        logger.info(f\"Exporting ONNX from {checkpoint_path}...\")\n",
        "        logger.info(f\"Output path: {output_path}\")\n",
        "        \n",
        "        # Ensure output directory exists\n",
        "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Build export command\n",
        "        cmd = [\n",
        "            sys.executable, \"-m\", \"piper.train.export_onnx\",\n",
        "            \"--checkpoint\", checkpoint_path,\n",
        "            \"--output-file\", output_path\n",
        "        ]\n",
        "        \n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                cmd,\n",
        "                cwd=self.config.piper_dir,\n",
        "                check=True,\n",
        "                capture_output=True,\n",
        "                text=True\n",
        "            )\n",
        "            logger.info(result.stdout)\n",
        "            logger.info(f\"ONNX export successful: {output_path}\")\n",
        "            return output_path\n",
        "            \n",
        "        except subprocess.CalledProcessError as e:\n",
        "            error_msg = f\"ONNX export failed (exit code {e.returncode}):\\n{e.stderr}\"\n",
        "            raise RuntimeError(error_msg)\n",
        "    \n",
        "    def copy_config(self, source_config: str, output_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Copy and rename config file to match ONNX model naming convention.\n",
        "        \n",
        "        Args:\n",
        "            source_config: Path to source config JSON\n",
        "            output_path: Path for output config (should end in .onnx.json)\n",
        "            \n",
        "        Returns:\n",
        "            str: Path to copied config file\n",
        "        \"\"\"\n",
        "        if not os.path.exists(source_config):\n",
        "            raise FileNotFoundError(f\"Source config not found: {source_config}\")\n",
        "        \n",
        "        shutil.copy(source_config, output_path)\n",
        "        logger.info(f\"Config copied to: {output_path}\")\n",
        "        return output_path\n",
        "\n",
        "\n",
        "# Initialize ONNX exporter\n",
        "onnx_exporter = ONNXExporter(config)\n",
        "print(\"âœ… ONNX exporter initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test-header"
      },
      "source": [
        "# =============================================================================\n",
        "# EXPORT MODEL TO ONNX\n",
        "# =============================================================================\n",
        "# Run the ONNX export process.\n",
        "\n",
        "# Determine paths\n",
        "if config.colab_mode:\n",
        "    gdrive_model_dir = os.path.join(config.gdrive_output_path, config.model_name)\n",
        "    export_checkpoint_path = f\"{gdrive_model_dir}/last.ckpt\"\n",
        "    export_config_path = f\"{gdrive_model_dir}/{config.model_name}.json\"\n",
        "    onnx_output_path = f\"{gdrive_model_dir}/{config.model_name}.onnx\"\n",
        "    onnx_config_path = f\"{gdrive_model_dir}/{config.model_name}.onnx.json\"\n",
        "else:\n",
        "    export_checkpoint_path = f\"{config.local_output_dir}/checkpoints/last.ckpt\"\n",
        "    export_config_path = f\"{config.local_output_dir}/{config.model_name}.json\"\n",
        "    onnx_output_path = f\"{config.local_output_dir}/{config.model_name}.onnx\"\n",
        "    onnx_config_path = f\"{config.local_output_dir}/{config.model_name}.onnx.json\"\n",
        "\n",
        "try:\n",
        "    # Export to ONNX\n",
        "    onnx_exporter.export(export_checkpoint_path, onnx_output_path)\n",
        "    \n",
        "    # Copy config file\n",
        "    onnx_exporter.copy_config(export_config_path, onnx_config_path)\n",
        "    \n",
        "    # Display results\n",
        "    if os.path.exists(onnx_output_path):\n",
        "        onnx_size_mb = os.path.getsize(onnx_output_path) / (1024 * 1024)\n",
        "        print(f\"\\nâœ… ONNX export complete!\")\n",
        "        print(f\"   Model: {onnx_output_path} ({onnx_size_mb:.2f} MB)\")\n",
        "        print(f\"   Config: {onnx_config_path}\")\n",
        "    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"âš ï¸ Cannot export ONNX: {e}\")\n",
        "    print(\"Ensure training has completed and checkpoint exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "test-model",
        "outputId": "d036f01d-2cb5-4c78-8885-6910de27f57b"
      },
      "outputs": [
        {
          "ename": "RuntimeException",
          "evalue": "[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Where node. Name:'node_index_put' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/math/element_wise_ops.h:640 onnxruntime::Broadcaster::Broadcaster(gsl::span<const long int>, gsl::span<const long int>) largest <= 1 was false. Can broadcast 0 by 0 or 1. 67 is invalid.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3377341387.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m }\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeException\u001b[0m: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Where node. Name:'node_index_put' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/math/element_wise_ops.h:640 onnxruntime::Broadcaster::Broadcaster(gsl::span<const long int>, gsl::span<const long int>) largest <= 1 was false. Can broadcast 0 by 0 or 1. 67 is invalid.\n"
          ]
        }
      ],
      "source": [
        "# ğŸ§ **9. Test ONNX Model** ğŸ§\n",
        "\n",
        "Test the exported ONNX model by generating speech using ONNX Runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary-header"
      },
      "source": [
        "# =============================================================================\n",
        "# TEST ONNX MODEL\n",
        "# =============================================================================\n",
        "# Test the exported ONNX model using ONNX Runtime.\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "try:\n",
        "    import onnxruntime as ort\n",
        "except ImportError:\n",
        "    print(\"Installing onnxruntime...\")\n",
        "    !pip install -q onnxruntime\n",
        "    import onnxruntime as ort\n",
        "\n",
        "\n",
        "class ONNXModelTester:\n",
        "    \"\"\"Test ONNX model inference.\"\"\"\n",
        "    \n",
        "    def __init__(self, onnx_path: str, config_path: str):\n",
        "        \"\"\"\n",
        "        Initialize ONNX model tester.\n",
        "        \n",
        "        Args:\n",
        "            onnx_path: Path to .onnx model file\n",
        "            config_path: Path to .onnx.json config file\n",
        "        \"\"\"\n",
        "        if not os.path.exists(onnx_path):\n",
        "            raise FileNotFoundError(f\"ONNX model not found: {onnx_path}\")\n",
        "        if not os.path.exists(config_path):\n",
        "            raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
        "        \n",
        "        # Load config\n",
        "        with open(config_path, 'r') as f:\n",
        "            self.config = json.load(f)\n",
        "        \n",
        "        self.phoneme_id_map = self.config.get('phoneme_id_map', {})\n",
        "        self.sample_rate = self.config.get('audio', {}).get('sample_rate', 22050)\n",
        "        \n",
        "        # Create ONNX session\n",
        "        self.session = ort.InferenceSession(onnx_path)\n",
        "        \n",
        "        print(f\"ONNX model loaded: {onnx_path}\")\n",
        "    \n",
        "    def text_to_phoneme_ids(self, text: str) -> list:\n",
        "        \"\"\"\n",
        "        Simple text to phoneme ID conversion for testing.\n",
        "        \n",
        "        Note: For production, use espeak-ng for proper phonemization.\n",
        "        \n",
        "        Args:\n",
        "            text: Input text\n",
        "            \n",
        "        Returns:\n",
        "            list: List of phoneme IDs\n",
        "        \"\"\"\n",
        "        ids = []\n",
        "        for char in text.lower():\n",
        "            if char in self.phoneme_id_map:\n",
        "                value = self.phoneme_id_map[char]\n",
        "                if isinstance(value, list):\n",
        "                    ids.extend(value)\n",
        "                else:\n",
        "                    ids.append(value)\n",
        "            elif char == ' ' and ' ' in self.phoneme_id_map:\n",
        "                value = self.phoneme_id_map[' ']\n",
        "                if isinstance(value, list):\n",
        "                    ids.extend(value)\n",
        "                else:\n",
        "                    ids.append(value)\n",
        "        \n",
        "        if not ids:\n",
        "            raise ValueError(\"No valid phoneme IDs generated from text. Check phoneme_id_map.\")\n",
        "        \n",
        "        return ids\n",
        "    \n",
        "    def synthesize(\n",
        "        self,\n",
        "        text: str,\n",
        "        noise_scale: float = 0.667,\n",
        "        length_scale: float = 1.0,\n",
        "        noise_scale_w: float = 0.8,\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Synthesize speech from text using ONNX model.\n",
        "        \n",
        "        Args:\n",
        "            text: Text to synthesize\n",
        "            noise_scale: Noise scale\n",
        "            length_scale: Length scale (1.0 = normal speed)\n",
        "            noise_scale_w: Noise scale for duration predictor\n",
        "            \n",
        "        Returns:\n",
        "            np.ndarray: Audio data\n",
        "        \"\"\"\n",
        "        phoneme_ids = self.text_to_phoneme_ids(text)\n",
        "        \n",
        "        # Prepare inputs\n",
        "        input_array = np.array([phoneme_ids], dtype=np.int64)\n",
        "        input_lengths = np.array([len(phoneme_ids)], dtype=np.int64)\n",
        "        scales = np.array([noise_scale, length_scale, noise_scale_w], dtype=np.float32)\n",
        "        \n",
        "        inputs = {\n",
        "            'input': input_array,\n",
        "            'input_lengths': input_lengths,\n",
        "            'scales': scales,\n",
        "        }\n",
        "        \n",
        "        # Run inference\n",
        "        output = self.session.run(None, inputs)\n",
        "        audio = output[0].squeeze()\n",
        "        \n",
        "        return audio\n",
        "\n",
        "\n",
        "# Test ONNX model\n",
        "TEST_TEXT_ONNX = \"Hello, this is a test of the exported ONNX model.\"\n",
        "\n",
        "try:\n",
        "    onnx_tester = ONNXModelTester(onnx_output_path, onnx_config_path)\n",
        "    \n",
        "    print(f\"\\nSynthesizing with ONNX: \\\"{TEST_TEXT_ONNX}\\\"\")\n",
        "    audio_data = onnx_tester.synthesize(TEST_TEXT_ONNX)\n",
        "    \n",
        "    print(f\"Generated audio shape: {audio_data.shape}\")\n",
        "    print(f\"Audio duration: {len(audio_data) / onnx_tester.sample_rate:.2f} seconds\")\n",
        "    \n",
        "    # Display audio player\n",
        "    display(Audio(audio_data, rate=onnx_tester.sample_rate))\n",
        "    \n",
        "    print(\"\\nâœ… ONNX model test successful!\")\n",
        "    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"âš ï¸ Cannot test ONNX model: {e}\")\n",
        "    print(\"This is expected if ONNX export has not completed yet.\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ ONNX test failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ”§ **10. Troubleshooting** ğŸ”§\n",
        "\n",
        "This section contains utilities and code snippets for debugging common issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.1 Checkpoint Upgrade Utility\n",
        "\n",
        "Utility for upgrading checkpoints for CPU compatibility (PyTorch Lightning version changes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CHECKPOINT UPGRADE UTILITY (TROUBLESHOOTING)\n",
        "# =============================================================================\n",
        "# Use this if you encounter checkpoint compatibility issues.\n",
        "\n",
        "# import torch\n",
        "# import pathlib\n",
        "# from pathlib import Path\n",
        "# from argparse import Namespace\n",
        "# from lightning.pytorch.utilities.upgrade_checkpoint import _upgrade\n",
        "\n",
        "# def upgrade_checkpoint_cpu_safe(checkpoint_path: str) -> None:\n",
        "#     \"\"\"\n",
        "#     Upgrade checkpoint for CPU compatibility.\n",
        "#     \n",
        "#     Args:\n",
        "#         checkpoint_path: Path to checkpoint file\n",
        "#     \"\"\"\n",
        "#     path = Path(checkpoint_path)\n",
        "#     \n",
        "#     if not path.exists():\n",
        "#         raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
        "#     \n",
        "#     # Load checkpoint with CPU mapping\n",
        "#     checkpoint = torch.load(\n",
        "#         str(path),\n",
        "#         map_location=torch.device(\"cpu\"),\n",
        "#         weights_only=False  # Safe for Lightning checkpoints\n",
        "#     )\n",
        "#     \n",
        "#     # Save upgraded CPU-compatible version\n",
        "#     torch.save(checkpoint, str(path))\n",
        "#     \n",
        "#     # Run Lightning upgrade\n",
        "#     args = Namespace(path=str(path), extension=\".ckpt\")\n",
        "#     _upgrade(args)\n",
        "#     \n",
        "#     print(f\"Upgraded and saved CPU-compatible checkpoint: {path}\")\n",
        "\n",
        "# # Usage:\n",
        "# # upgrade_checkpoint_cpu_safe(\"/content/pretrained.ckpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.2 Library Patching for ONNX Export\n",
        "\n",
        "Patches for known issues when exporting to ONNX format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LIBRARY PATCHING FOR ONNX EXPORT (TROUBLESHOOTING)\n",
        "# =============================================================================\n",
        "# Use this if you encounter ONNX export issues.\n",
        "\n",
        "# def patch_piper_library_for_onnx(base_dir: str) -> None:\n",
        "#     \"\"\"\n",
        "#     Apply patches to Piper library files for ONNX export compatibility.\n",
        "#     \n",
        "#     Patches:\n",
        "#     1. transforms.py: Remove assertion that fails during export\n",
        "#     2. modules.py: Fix dynamic convolution shape issues\n",
        "#     3. lightning.py: Add sample_bytes parameter for legacy checkpoints\n",
        "#     4. export_onnx.py: Add PyTorch 2.6 security globals\n",
        "#     \n",
        "#     Args:\n",
        "#         base_dir: Path to piper repository root\n",
        "#     \"\"\"\n",
        "#     import os\n",
        "#     \n",
        "#     transforms_file = os.path.join(base_dir, \"src/piper/train/vits/transforms.py\")\n",
        "#     modules_file = os.path.join(base_dir, \"src/piper/train/vits/modules.py\")\n",
        "#     lightning_file = os.path.join(base_dir, \"src/piper/train/vits/lightning.py\")\n",
        "#     export_file = os.path.join(base_dir, \"src/piper/train/export_onnx.py\")\n",
        "#     \n",
        "#     print(\"ğŸ”§ Starting library patching for ONNX export...\")\n",
        "#     \n",
        "#     # --- PATCH 1: transforms.py (Remove Assertion) ---\n",
        "#     if os.path.exists(transforms_file):\n",
        "#         with open(transforms_file, \"r\") as f:\n",
        "#             lines = f.readlines()\n",
        "#         \n",
        "#         new_lines = []\n",
        "#         patched = False\n",
        "#         for line in lines:\n",
        "#             if \"assert (discriminant >= 0).all()\" in line and not line.strip().startswith(\"#\"):\n",
        "#                 new_lines.append(f\"# {line.strip()} # PATCHED\\n\")\n",
        "#                 patched = True\n",
        "#             else:\n",
        "#                 new_lines.append(line)\n",
        "#         \n",
        "#         if patched:\n",
        "#             with open(transforms_file, \"w\") as f:\n",
        "#                 f.writelines(new_lines)\n",
        "#             print(\"  âœ… [transforms.py] Assertion disabled.\")\n",
        "#         else:\n",
        "#             print(\"  â­ï¸ [transforms.py] Already patched or not needed.\")\n",
        "#     \n",
        "#     # --- PATCH 2: modules.py (Fix Dynamic Convolution Shape) ---\n",
        "#     if os.path.exists(modules_file):\n",
        "#         with open(modules_file, \"r\") as f:\n",
        "#             lines = f.readlines()\n",
        "#         \n",
        "#         # Remove any old patches\n",
        "#         clean_lines = [l for l in lines if \"torch._check\" not in l or \"x0.shape\" not in l]\n",
        "#         \n",
        "#         final_lines = []\n",
        "#         patched = False\n",
        "#         for line in clean_lines:\n",
        "#             if \"h = self.pre(x0) * x_mask\" in line:\n",
        "#                 indent = line.split(\"h = self.pre(x0)\")[0]\n",
        "#                 final_lines.append(f\"{indent}torch._check(x0.size(2) > 1) # ONNX PATCH\\n\")\n",
        "#                 final_lines.append(line)\n",
        "#                 patched = True\n",
        "#             else:\n",
        "#                 final_lines.append(line)\n",
        "#         \n",
        "#         if patched:\n",
        "#             with open(modules_file, \"w\") as f:\n",
        "#                 f.writelines(final_lines)\n",
        "#             print(\"  âœ… [modules.py] Shape guard inserted.\")\n",
        "#     \n",
        "#     # --- PATCH 3: lightning.py (Legacy Checkpoint) ---\n",
        "#     if os.path.exists(lightning_file):\n",
        "#         with open(lightning_file, \"r\") as f:\n",
        "#             lines = f.readlines()\n",
        "#         \n",
        "#         if not any(\"sample_bytes\" in line for line in lines):\n",
        "#             new_lines = []\n",
        "#             for line in lines:\n",
        "#                 new_lines.append(line)\n",
        "#                 if \"sample_rate: int = 22050,\" in line:\n",
        "#                     new_lines.append(\"        sample_bytes: int = 2, # PATCHED\\n\")\n",
        "#             with open(lightning_file, \"w\") as f:\n",
        "#                 f.writelines(new_lines)\n",
        "#             print(\"  âœ… [lightning.py] sample_bytes parameter added.\")\n",
        "#         else:\n",
        "#             print(\"  â­ï¸ [lightning.py] Already patched.\")\n",
        "#     \n",
        "#     # --- PATCH 4: export_onnx.py (PyTorch 2.6 Security) ---\n",
        "#     if os.path.exists(export_file):\n",
        "#         with open(export_file, \"r\") as f:\n",
        "#             content = f.read()\n",
        "#         \n",
        "#         patch_str = \"torch.serialization.add_safe_globals([pathlib.PosixPath])\"\n",
        "#         if patch_str not in content:\n",
        "#             new_content = content.replace(\n",
        "#                 \"import torch\",\n",
        "#                 \"import torch\\nimport pathlib\\ntry:\\n    torch.serialization.add_safe_globals([pathlib.PosixPath])\\nexcept AttributeError:\\n    pass\"\n",
        "#             )\n",
        "#             with open(export_file, \"w\") as f:\n",
        "#                 f.write(new_content)\n",
        "#             print(\"  âœ… [export_onnx.py] Security globals added.\")\n",
        "#         else:\n",
        "#             print(\"  â­ï¸ [export_onnx.py] Already patched.\")\n",
        "#     \n",
        "#     print(\"ğŸ”§ Patching complete.\\n\")\n",
        "\n",
        "# # Usage:\n",
        "# # patch_piper_library_for_onnx(\"/content/piper1-gpl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.3 Clear Dataset Directory\n",
        "\n",
        "Utility to clear local dataset directory for fresh download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CLEAR DATASET DIRECTORY (TROUBLESHOOTING)\n",
        "# =============================================================================\n",
        "# Use this to clear local dataset directory for a fresh download.\n",
        "\n",
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# def clear_dataset_directory(config: PiperConfig) -> None:\n",
        "#     \"\"\"\n",
        "#     Clear local dataset directory for fresh download.\n",
        "#     \n",
        "#     WARNING: This will delete all local copies of audio files!\n",
        "#     \n",
        "#     Args:\n",
        "#         config: PiperConfig instance\n",
        "#     \"\"\"\n",
        "#     if os.path.exists(config.local_wavs_dir):\n",
        "#         print(f\"Removing: {config.local_wavs_dir}\")\n",
        "#         shutil.rmtree(config.local_wavs_dir)\n",
        "#         os.makedirs(config.local_wavs_dir, exist_ok=True)\n",
        "#         print(\"âœ… Dataset directory cleared.\")\n",
        "#     else:\n",
        "#         print(\"Dataset directory does not exist.\")\n",
        "\n",
        "# # Usage:\n",
        "# # clear_dataset_directory(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“‹ **Summary**\n",
        "\n",
        "This notebook provides a complete pipeline for fine-tuning Piper TTS with support for both \n",
        "Google Colab (POC) and AWS (Production) environments.\n",
        "\n",
        "## Pipeline Overview\n",
        "\n",
        "| Step | Description |\n",
        "|------|-------------|\n",
        "| 1. Configuration | Set `COLAB` mode and configure paths/parameters |\n",
        "| 2. Environment Setup | GPU check, Google Drive mount (Colab) or S3 setup (AWS) |\n",
        "| 3. Install Dependencies | System packages, Piper TTS, and native extensions |\n",
        "| 4. Data ETL | Download/copy dataset, process transcripts |\n",
        "| 5. Training | Fine-tune model with checkpoint saving |\n",
        "| 6. Save Outputs | Copy checkpoints to remote storage |\n",
        "| 7. Test Model | Test from checkpoint |\n",
        "| 8. Export ONNX | Export to ONNX format |\n",
        "| 9. Test ONNX | Test exported ONNX model |\n",
        "| 10. Troubleshooting | Utilities for common issues |\n",
        "\n",
        "## Output Files\n",
        "\n",
        "After training, you'll find these files in your output folder:\n",
        "- `{model_name}.onnx` - The trained model in ONNX format\n",
        "- `{model_name}.onnx.json` - Model configuration file\n",
        "- `last.ckpt` - Latest training checkpoint (for resuming training)\n",
        "- `lightning_logs/` - Training logs and intermediate checkpoints\n",
        "\n",
        "## Using the Trained Model\n",
        "\n",
        "To use the trained model with Piper CLI:\n",
        "```bash\n",
        "echo \"Hello world\" | piper --model {model_name}.onnx --output_file output.wav\n",
        "```\n",
        "\n",
        "## Mode Comparison\n",
        "\n",
        "| Feature | COLAB Mode | AWS Mode |\n",
        "|---------|------------|----------|\n",
        "| Data Storage | Google Drive | AWS S3 |\n",
        "| Use Case | Proof of Concept | Production |\n",
        "| Data Loader | GoogleDriveDataLoader | S3DataLoader |\n",
        "| Credentials | Google Account | AWS IAM/Keys |\n",
        "\n",
        "## Key Classes\n",
        "\n",
        "- **PiperConfig**: Centralized configuration dataclass\n",
        "- **DataLoaderBase**: Abstract base for data loading (GoogleDriveDataLoader, S3DataLoader)\n",
        "- **TranscriptProcessor**: Text normalization and transcript processing\n",
        "- **CheckpointManager**: Manage pretrained/training checkpoints\n",
        "- **ModelTester**: Test model from checkpoint\n",
        "- **ONNXExporter**: Export model to ONNX format\n",
        "- **ONNXModelTester**: Test exported ONNX model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c9a7e1d091041bf8f00e6a00e77c066": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f919d5117704008886f31c8bc4e381c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b1ca79243bc4c2d834bfe6107407fc9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9280e01c53864f3283ba04f9d02f5f19",
            "value": "config.json:â€‡"
          }
        },
        "2be4a1bd326048079f6d6f07cc61fdee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a33fa8f5b7d4ef5accfe5948c735260": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e8549946201493dae02f115c0d1bb52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a544bd709214476e95b164e124684dbe",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a3c0bdb45774a7799b1f319f1a74378",
            "value": 1
          }
        },
        "66252483676a45ceba797b73d49850e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b1ca79243bc4c2d834bfe6107407fc9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d4b31e6470a4719a7a4b1e91b9f3a41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f1c8f47337d44129edd8b236d31f816": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7148e7a1e4f54aaa8bccad9bbe512ecb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a3c0bdb45774a7799b1f319f1a74378": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fd4a35190f94317a55614f5deadd200": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9280e01c53864f3283ba04f9d02f5f19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a30bbf28c28347d5bf860559e625dec0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a544bd709214476e95b164e124684dbe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b37cbe17bce44398931fa0922927bf02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a30bbf28c28347d5bf860559e625dec0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8fd4a35190f94317a55614f5deadd200",
            "value": "â€‡998M/998Mâ€‡[00:14&lt;00:00,â€‡76.0MB/s]"
          }
        },
        "b46ae31ffd85473598069efe0432d2d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b596459cae3e4d4ba2bdd11e7c6f20e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f919d5117704008886f31c8bc4e381c",
              "IPY_MODEL_5e8549946201493dae02f115c0d1bb52",
              "IPY_MODEL_fafa6ccf3e294292adb35a874b5a9ac2"
            ],
            "layout": "IPY_MODEL_b46ae31ffd85473598069efe0432d2d2"
          }
        },
        "c0a5664906ae4da9bfb8fb0243bb709b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7148e7a1e4f54aaa8bccad9bbe512ecb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1c9a7e1d091041bf8f00e6a00e77c066",
            "value": "en/en_US/ljspeech/high/ljspeech-2000.ckp(â€¦):â€‡100%"
          }
        },
        "d76c9fdd465444a7a49dfbadec82deec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0a5664906ae4da9bfb8fb0243bb709b",
              "IPY_MODEL_ecd1eb41504a4dbe8e370cd639cc8841",
              "IPY_MODEL_b37cbe17bce44398931fa0922927bf02"
            ],
            "layout": "IPY_MODEL_6f1c8f47337d44129edd8b236d31f816"
          }
        },
        "ecd1eb41504a4dbe8e370cd639cc8841": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a33fa8f5b7d4ef5accfe5948c735260",
            "max": 998167044,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66252483676a45ceba797b73d49850e5",
            "value": 998167044
          }
        },
        "fafa6ccf3e294292adb35a874b5a9ac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d4b31e6470a4719a7a4b1e91b9f3a41",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2be4a1bd326048079f6d6f07cc61fdee",
            "value": "â€‡7.09k/?â€‡[00:00&lt;00:00,â€‡338kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
