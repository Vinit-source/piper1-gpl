{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Vinit-source/piper1-gpl/blob/copilot/create-finetuning-notebook/notebooks/piper_finetuning_indian_english_from_gdrive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title-header"
   },
   "source": [
    "# Piper TTS Fine-Tuning: Indian English Voice (Google Drive Edition)\n",
    "\n",
    "This notebook provides a complete pipeline for fine-tuning the Piper TTS model\n",
    "using a dataset stored in Google Drive.\n",
    "\n",
    "**Compatible with:** Google Colab\n",
    "\n",
    "## Overview\n",
    "1. Colab Anti-Disconnect\n",
    "2. Check GPU Type\n",
    "3. Mount Google Drive\n",
    "4. Install Software\n",
    "5. Download Dataset from Drive (audio + transcript)\n",
    "6. Training\n",
    "7. Save Model and Config to Drive\n",
    "\n",
    "---\n",
    "\n",
    "**Repository:** https://github.com/Vinit-source/piper1-gpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "first-steps-header"
   },
   "source": [
    "# ðŸ”§ **1. First Steps** ðŸ”§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anti-disconnect",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@markdown ## **Google Colab Anti-Disconnect.** ðŸ”Œ\n",
    "#@markdown ---\n",
    "#@markdown #### Avoid automatic disconnection. Still, it will disconnect after **6 to 12 hours**.\n",
    "\n",
    "import IPython\n",
    "js_code = '''\n",
    "function ClickConnect(){\n",
    "console.log(\"Working\");\n",
    "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
    "}\n",
    "setInterval(ClickConnect,60000)\n",
    "'''\n",
    "display(IPython.display.Javascript(js_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@markdown ## **Check GPU Type.** ðŸ‘ï¸\n",
    "#@markdown ---\n",
    "#@markdown #### A higher capable GPU can lead to faster training speeds. By default, you will have a **Tesla T4**.\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-gdrive",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@markdown # **Mount Google Drive.** ðŸ“‚\n",
    "#@markdown ---\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-header"
   },
   "source": [
    "# ðŸ“¦ **2. Install Software** ðŸ“¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-software"
   },
   "outputs": [],
   "source": [
    "#@markdown # **Install Piper and Dependencies.** ðŸ“¦\n",
    "#@markdown ---\n",
    "#@markdown This cell installs Piper TTS and all necessary dependencies for training.\n",
    "\n",
    "import os\n",
    "\n",
    "# Install system dependencies\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq espeak-ng build-essential cmake ninja-build\n",
    "\n",
    "# Clone the piper1-gpl repository with updated dependencies\n",
    "PIPER_REPO_URL = \"https://github.com/Vinit-source/piper1-gpl.git\"\n",
    "PIPER_BRANCH = \"copilot/fine-tune-text-to-speech-model\"\n",
    "PIPER_DIR = \"/content/piper1-gpl\"\n",
    "\n",
    "if not os.path.exists(PIPER_DIR):\n",
    "    !git clone -b {PIPER_BRANCH} {PIPER_REPO_URL} {PIPER_DIR}\n",
    "else:\n",
    "    print(f\"Repository already exists at {PIPER_DIR}\")\n",
    "\n",
    "# Install piper with training dependencies\n",
    "%cd {PIPER_DIR}\n",
    "!pip install -e .[train] -q\n",
    "\n",
    "# Build monotonic alignment module (required for VITS)\n",
    "!bash build_monotonic_align.sh\n",
    "\n",
    "print(\"\\nâœ… Piper TTS installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset-header"
   },
   "source": [
    "# ðŸ“¥ **3. Download Dataset from Drive** ðŸ“¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset-config",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@markdown # **Configure Dataset Path.** ðŸ“\n",
    "#@markdown ---\n",
    "#@markdown ### Set the path to your dataset in Google Drive.\n",
    "#@markdown\n",
    "#@markdown **Expected folder structure:**\n",
    "#@markdown ```\n",
    "#@markdown your_dataset_folder/\n",
    "#@markdown â”œâ”€â”€ wavs/           # Audio files (WAV format, 22050Hz recommended)\n",
    "#@markdown â”‚   â”œâ”€â”€ 1.wav\n",
    "#@markdown â”‚   â”œâ”€â”€ 2.wav\n",
    "#@markdown â”‚   â””â”€â”€ ...\n",
    "#@markdown â””â”€â”€ metadata.csv    # Transcript file (format: wavs/filename.wav|text)\n",
    "#@markdown ```\n",
    "#@markdown\n",
    "#@markdown **Transcript format (single speaker):**\n",
    "#@markdown ```\n",
    "#@markdown wavs/1.wav|This is the text for audio 1.\n",
    "#@markdown wavs/2.wav|This is the text for audio 2.\n",
    "#@markdown ```\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ### Path to your dataset folder in Google Drive:\n",
    "gdrive_dataset_path = \"/content/drive/MyDrive/Piper-Training/dataset\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Model name (used for output files):\n",
    "model_name = \"en_IN-spicor-medium\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Output path in Google Drive (for saving checkpoints and models):\n",
    "gdrive_output_path = \"/content/drive/MyDrive/Piper-Training/output\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Validate paths\n",
    "gdrive_dataset_path = gdrive_dataset_path.strip()\n",
    "gdrive_output_path = gdrive_output_path.strip()\n",
    "\n",
    "# Create local working directories\n",
    "LOCAL_DATASET_DIR = \"/content/dataset\"\n",
    "LOCAL_WAVS_DIR = f\"{LOCAL_DATASET_DIR}/wavs\"\n",
    "LOCAL_CACHE_DIR = \"/content/audio_cache\"\n",
    "LOCAL_OUTPUT_DIR = f\"/content/output/{model_name}\"\n",
    "\n",
    "os.makedirs(LOCAL_DATASET_DIR, exist_ok=True)\n",
    "os.makedirs(LOCAL_WAVS_DIR, exist_ok=True)\n",
    "os.makedirs(LOCAL_CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(LOCAL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Create output directory in Google Drive\n",
    "os.makedirs(gdrive_output_path, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset path: {gdrive_dataset_path}\")\n",
    "print(f\"Output path: {gdrive_output_path}\")\n",
    "print(f\"Local dataset dir: {LOCAL_DATASET_DIR}\")\n",
    "print(f\"Local output dir: {LOCAL_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extract-dataset"
   },
   "outputs": [],
   "source": [
    "#@markdown # **Extract Dataset.** ðŸ“¥\n",
    "#@markdown ---\n",
    "#@markdown This cell copies your dataset from Google Drive to the local environment.\n",
    "#@markdown\n",
    "#@markdown **Important:** Audio files must be in WAV format (22050Hz, 16-bit, mono recommended).\n",
    "#@markdown ---\n",
    "\n",
    "import os\n",
    "import wave\n",
    "import zipfile\n",
    "import datetime\n",
    "import shutil\n",
    "\n",
    "def get_dataset_duration(wav_path):\n",
    "    \"\"\"Calculate total duration of all WAV files in a directory.\"\"\"\n",
    "    totalduration = 0\n",
    "    totalcount = 0\n",
    "    for file_name in os.listdir(wav_path):\n",
    "        if not file_name.endswith(\".wav\"):\n",
    "            continue\n",
    "        full_path = os.path.join(wav_path, file_name)\n",
    "        try:\n",
    "            with wave.open(full_path, \"rb\") as wave_file:\n",
    "                frames = wave_file.getnframes()\n",
    "                rate = wave_file.getframerate()\n",
    "                duration = frames / float(rate)\n",
    "                totalduration += duration\n",
    "                totalcount += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping bad file {file_name}: {e}\")\n",
    "            continue\n",
    "    duration_str = str(datetime.timedelta(seconds=round(totalduration, 0)))\n",
    "    return totalcount, duration_str\n",
    "\n",
    "def cleanup_ghost_files(dataset_path):\n",
    "    \"\"\"Delete macOS ghost files (._*) from the dataset.\"\"\"\n",
    "    count = 0\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.startswith(\"._\"):\n",
    "            file_path = os.path.join(dataset_path, filename)\n",
    "            os.remove(file_path)\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        print(f\"Cleaned up {count} macOS artifact files.\")\n",
    "\n",
    "# Check if dataset exists in Google Drive\n",
    "if not os.path.exists(gdrive_dataset_path):\n",
    "    raise Exception(f\"Dataset folder not found: {gdrive_dataset_path}\")\n",
    "\n",
    "# Check for wavs folder or zip file\n",
    "gdrive_wavs_path = os.path.join(gdrive_dataset_path, \"wavs\")\n",
    "gdrive_wavs_zip = os.path.join(gdrive_dataset_path, \"wavs.zip\")\n",
    "\n",
    "if os.path.exists(gdrive_wavs_zip):\n",
    "    print(\"Found wavs.zip, extracting...\")\n",
    "    with zipfile.ZipFile(gdrive_wavs_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(LOCAL_DATASET_DIR)\n",
    "    # Handle nested wavs folder if present\n",
    "    if os.path.exists(f\"{LOCAL_DATASET_DIR}/wavs/wavs\"):\n",
    "        for f in os.listdir(f\"{LOCAL_DATASET_DIR}/wavs/wavs\"):\n",
    "            shutil.move(f\"{LOCAL_DATASET_DIR}/wavs/wavs/{f}\", f\"{LOCAL_WAVS_DIR}/{f}\")\n",
    "elif os.path.exists(gdrive_wavs_path):\n",
    "    print(\"Found wavs folder, copying...\")\n",
    "    !cp -r \"{gdrive_wavs_path}\"/* \"{LOCAL_WAVS_DIR}/\"\n",
    "else:\n",
    "    raise Exception(f\"No 'wavs' folder or 'wavs.zip' found in {gdrive_dataset_path}\")\n",
    "\n",
    "# Clean up macOS ghost files\n",
    "cleanup_ghost_files(LOCAL_WAVS_DIR)\n",
    "\n",
    "# Copy transcript file\n",
    "transcript_files = [\"metadata.csv\", \"transcripts.txt\", \"transcript.txt\", \"metadata.txt\"]\n",
    "transcript_found = False\n",
    "\n",
    "for tf in transcript_files:\n",
    "    gdrive_transcript = os.path.join(gdrive_dataset_path, tf)\n",
    "    if os.path.exists(gdrive_transcript):\n",
    "        print(f\"Found transcript file: {tf}\")\n",
    "        shutil.copy(gdrive_transcript, f\"{LOCAL_DATASET_DIR}/metadata.csv\")\n",
    "        transcript_found = True\n",
    "        break\n",
    "\n",
    "if not transcript_found:\n",
    "    raise Exception(f\"No transcript file found in {gdrive_dataset_path}. Expected one of: {transcript_files}\")\n",
    "\n",
    "# Get dataset statistics\n",
    "audio_count, dataset_dur = get_dataset_duration(LOCAL_WAVS_DIR)\n",
    "print(f\"\\nâœ… Dataset loaded: {audio_count} audio files, total duration: {dataset_dur}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-header"
   },
   "source": [
    "# ðŸ¤– **4. Training** ðŸ¤–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-config",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@markdown # **Training Configuration.** âš™ï¸\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ### Language of the dataset:\n",
    "language = \"en\" #@param [\"ar\", \"ca\", \"cs\", \"da\", \"de\", \"el\", \"en\", \"en-us\", \"es\", \"es-419\", \"fi\", \"fr\", \"hu\", \"is\", \"it\", \"ka\", \"kk\", \"lb\", \"ne\", \"nl\", \"nb\", \"pl\", \"pt-br\", \"pt-pt\", \"ro\", \"ru\", \"sr\", \"sv\", \"sw\", \"tr\", \"uk\", \"vi\", \"zh\"]\n",
    "\n",
    "#@markdown ### Sample rate (22050 recommended for medium/high quality):\n",
    "sample_rate = 22050 #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ### Is this a single speaker dataset?\n",
    "single_speaker = True #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### Training Parameters:\n",
    "\n",
    "#@markdown ### Batch size (reduce if out of memory):\n",
    "batch_size = 16 #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ### Maximum epochs:\n",
    "max_epochs = 1000 #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ### Validation split (0.0 to disable validation):\n",
    "validation_split = 0.05 #@param {type:\"number\"}\n",
    "\n",
    "#@markdown ### Number of test examples for audio generation:\n",
    "num_test_examples = 3 #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### Fine-tuning Options:\n",
    "\n",
    "#@markdown ### Download and use a pretrained checkpoint for fine-tuning?\n",
    "use_pretrained = True #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ### Resume from existing checkpoint in output folder?\n",
    "resume_training = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "# Calculate num_speakers\n",
    "num_speakers = 1 if single_speaker else 0  # 0 will be calculated from metadata\n",
    "\n",
    "print(f\"Language: {language}\")\n",
    "print(f\"Sample rate: {sample_rate}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Max epochs: {max_epochs}\")\n",
    "print(f\"Validation split: {validation_split}\")\n",
    "print(f\"Use pretrained: {use_pretrained}\")\n",
    "print(f\"Resume training: {resume_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-pretrained"
   },
   "outputs": [],
   "source": [
    "#@markdown # **Download Pretrained Checkpoint.** ðŸ“¥\n",
    "#@markdown ---\n",
    "#@markdown Downloads a pretrained checkpoint from Hugging Face for fine-tuning.\n",
    "\n",
    "import os\n",
    "\n",
    "PRETRAINED_CKPT_PATH = \"/content/pretrained.ckpt\"\n",
    "\n",
    "if use_pretrained and not resume_training:\n",
    "    if not os.path.exists(PRETRAINED_CKPT_PATH):\n",
    "        print(\"Downloading pretrained checkpoint from Hugging Face...\")\n",
    "\n",
    "        # Install huggingface_hub if needed\n",
    "        !pip install -q huggingface_hub\n",
    "\n",
    "        from huggingface_hub import hf_hub_download\n",
    "\n",
    "        # Download the en_US hfc_female medium checkpoint\n",
    "        downloaded_path = hf_hub_download(\n",
    "            repo_id=\"rhasspy/piper-checkpoints\",\n",
    "            filename=\"en/en_US/hfc_female/medium/epoch=2868-step=1575188.ckpt\",\n",
    "            repo_type=\"dataset\",\n",
    "            local_dir=\"/content/checkpoints\",\n",
    "        )\n",
    "\n",
    "        # Also download the config for reference\n",
    "        config_path = hf_hub_download(\n",
    "            repo_id=\"rhasspy/piper-checkpoints\",\n",
    "            filename=\"en/en_US/hfc_female/medium/config.json\",\n",
    "            repo_type=\"dataset\",\n",
    "            local_dir=\"/content/checkpoints\",\n",
    "        )\n",
    "\n",
    "        # Copy to expected location\n",
    "        !cp \"{downloaded_path}\" \"{PRETRAINED_CKPT_PATH}\"\n",
    "\n",
    "        print(f\"\\nâœ… Pretrained checkpoint downloaded to: {PRETRAINED_CKPT_PATH}\")\n",
    "    else:\n",
    "        print(f\"Pretrained checkpoint already exists: {PRETRAINED_CKPT_PATH}\")\n",
    "elif resume_training:\n",
    "    print(\"Resume training mode - will look for existing checkpoint in output folder.\")\n",
    "else:\n",
    "    print(\"Training from scratch (no pretrained checkpoint).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tensorboard"
   },
   "outputs": [],
   "source": [
    "#@markdown # **Launch TensorBoard.** ðŸ“Š\n",
    "#@markdown ---\n",
    "#@markdown TensorBoard allows you to monitor training progress in real-time.\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {LOCAL_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-model"
   },
   "outputs": [],
   "source": [
    "#@markdown # **Start Training.** ðŸ‹ï¸â€â™‚ï¸\n",
    "#@markdown ---\n",
    "#@markdown This cell runs the Piper training process.\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Change to piper directory\n",
    "%cd /content/piper1-gpl\n",
    "\n",
    "# Build the training command\n",
    "csv_path = f\"{LOCAL_DATASET_DIR}/metadata.csv\"\n",
    "config_path = f\"{LOCAL_OUTPUT_DIR}/{model_name}.json\"\n",
    "cache_dir = LOCAL_CACHE_DIR\n",
    "audio_dir = LOCAL_WAVS_DIR\n",
    "\n",
    "# Determine checkpoint path\n",
    "ckpt_path_arg = \"\"\n",
    "\n",
    "if resume_training:\n",
    "    # Look for existing checkpoint\n",
    "    checkpoints = glob.glob(f\"{LOCAL_OUTPUT_DIR}/lightning_logs/**/checkpoints/last.ckpt\", recursive=True)\n",
    "    if checkpoints:\n",
    "        # Sort by version number to get the latest\n",
    "        latest_ckpt = sorted(checkpoints, key=lambda x: int(re.findall(r'version_(\\d+)', x)[0]) if re.findall(r'version_(\\d+)', x) else 0)[-1]\n",
    "        print(f\"Resuming from checkpoint: {latest_ckpt}\")\n",
    "        ckpt_path_arg = f\"--ckpt_path \\\"{latest_ckpt}\\\"\"\n",
    "    else:\n",
    "        print(\"No existing checkpoint found. Starting fresh.\")\n",
    "        if use_pretrained and os.path.exists(PRETRAINED_CKPT_PATH):\n",
    "            ckpt_path_arg = f\"--ckpt_path \\\"{PRETRAINED_CKPT_PATH}\\\"\"\n",
    "elif use_pretrained and os.path.exists(PRETRAINED_CKPT_PATH):\n",
    "    print(f\"Fine-tuning from pretrained checkpoint: {PRETRAINED_CKPT_PATH}\")\n",
    "    ckpt_path_arg = f\"--ckpt_path \\\"{PRETRAINED_CKPT_PATH}\\\"\"\n",
    "\n",
    "# Build the training command using the piper.train module\n",
    "train_cmd = f\"\"\"\n",
    "python -m piper.train \\\n",
    "    --data.csv_path \"{csv_path}\" \\\n",
    "    --data.cache_dir \"{cache_dir}\" \\\n",
    "    --data.audio_dir \"{audio_dir}\" \\\n",
    "    --data.espeak_voice \"{language}\" \\\n",
    "    --data.config_path \"{config_path}\" \\\n",
    "    --data.voice_name \"{model_name}\" \\\n",
    "    --data.sample_rate {sample_rate} \\\n",
    "    --data.batch_size {batch_size} \\\n",
    "    --data.validation_split {validation_split} \\\n",
    "    --data.num_test_examples {num_test_examples} \\\n",
    "    --model.sample_rate {sample_rate} \\\n",
    "    --model.num_speakers {num_speakers} \\\n",
    "    --trainer.max_epochs {max_epochs} \\\n",
    "    --trainer.accelerator gpu \\\n",
    "    --trainer.devices 1 \\\n",
    "    --trainer.precision 32 \\\n",
    "    --trainer.default_root_dir \"{LOCAL_OUTPUT_DIR}\" \\\n",
    "    --trainer.callbacks+=ModelCheckpoint \\\n",
    "    --trainer.callbacks.dirpath \"{LOCAL_OUTPUT_DIR}/checkpoints\" \\\n",
    "    --trainer.callbacks.filename \"piper-{{epoch:04d}}-{{step:08d}}\" \\\n",
    "    --trainer.callbacks.save_top_k 3 \\\n",
    "    --trainer.callbacks.save_last true \\\n",
    "    --trainer.callbacks.every_n_epochs 1 \\\n",
    "    {ckpt_path_arg}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training command:\\n{train_cmd}\")\n",
    "!{train_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-header"
   },
   "source": [
    "# ðŸ’¾ **5. Save Model to Drive** ðŸ’¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export-onnx"
   },
   "outputs": [],
   "source": [
    "#@markdown # **Export Model to ONNX.** ðŸ“¦\n",
    "#@markdown ---\n",
    "#@markdown This cell exports the trained model to ONNX format for inference.\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Find the best checkpoint\n",
    "checkpoint_dir = f\"{LOCAL_OUTPUT_DIR}/checkpoints\"\n",
    "last_ckpt = f\"{checkpoint_dir}/last.ckpt\"\n",
    "\n",
    "if os.path.exists(last_ckpt):\n",
    "    best_checkpoint = last_ckpt\n",
    "    print(f\"Using last checkpoint: {best_checkpoint}\")\n",
    "else:\n",
    "    # Find any checkpoint\n",
    "    all_checkpoints = glob.glob(f\"{LOCAL_OUTPUT_DIR}/**/checkpoints/*.ckpt\", recursive=True)\n",
    "    if all_checkpoints:\n",
    "        best_checkpoint = sorted(all_checkpoints)[-1]\n",
    "        print(f\"Using checkpoint: {best_checkpoint}\")\n",
    "    else:\n",
    "        raise Exception(f\"No checkpoints found in {LOCAL_OUTPUT_DIR}\")\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_output_path = f\"{LOCAL_OUTPUT_DIR}/{model_name}.onnx\"\n",
    "\n",
    "%cd /content/piper1-gpl\n",
    "!python -m piper.train.export_onnx \\\n",
    "    --checkpoint \"{best_checkpoint}\" \\\n",
    "    --output-file \"{onnx_output_path}\"\n",
    "\n",
    "print(f\"\\nâœ… Model exported to: {onnx_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-to-drive"
   },
   "outputs": [],
   "source": [
    "#@markdown # **Save Model and Config to Google Drive.** ðŸ’¾\n",
    "#@markdown ---\n",
    "#@markdown This cell copies the trained model and config files to Google Drive.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create model-specific output directory in Google Drive\n",
    "gdrive_model_dir = os.path.join(gdrive_output_path, model_name)\n",
    "os.makedirs(gdrive_model_dir, exist_ok=True)\n",
    "\n",
    "# Files to copy\n",
    "files_to_copy = [\n",
    "    (f\"{LOCAL_OUTPUT_DIR}/{model_name}.onnx\", f\"{gdrive_model_dir}/{model_name}.onnx\"),\n",
    "    (f\"{LOCAL_OUTPUT_DIR}/{model_name}.json\", f\"{gdrive_model_dir}/{model_name}.onnx.json\"),\n",
    "]\n",
    "\n",
    "# Also copy the last checkpoint\n",
    "last_ckpt = f\"{LOCAL_OUTPUT_DIR}/checkpoints/last.ckpt\"\n",
    "if os.path.exists(last_ckpt):\n",
    "    files_to_copy.append((last_ckpt, f\"{gdrive_model_dir}/last.ckpt\"))\n",
    "\n",
    "# Copy files\n",
    "for src, dst in files_to_copy:\n",
    "    if os.path.exists(src):\n",
    "        print(f\"Copying {src} -> {dst}\")\n",
    "        shutil.copy(src, dst)\n",
    "    else:\n",
    "        print(f\"Warning: {src} not found, skipping.\")\n",
    "\n",
    "print(f\"\\nâœ… Model saved to Google Drive: {gdrive_model_dir}\")\n",
    "print(\"\\nFiles saved:\")\n",
    "for f in os.listdir(gdrive_model_dir):\n",
    "    fpath = os.path.join(gdrive_model_dir, f)\n",
    "    size_mb = os.path.getsize(fpath) / (1024 * 1024)\n",
    "    print(f\"  - {f} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-header"
   },
   "source": [
    "# ðŸŽ§ **6. Test the Model (Optional)** ðŸŽ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-model"
   },
   "outputs": [],
   "source": [
    "#@markdown # **Test the Exported Model.** ðŸŽ§\n",
    "#@markdown ---\n",
    "#@markdown This cell tests the exported ONNX model by generating speech.\n",
    "\n",
    "#@markdown ### Text to synthesize:\n",
    "test_text = \"Hello, this is a test of the fine-tuned Piper text to speech model.\" #@param {type:\"string\"}\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "except ImportError:\n",
    "    !pip install -q onnxruntime\n",
    "    import onnxruntime as ort\n",
    "\n",
    "# Load model and config\n",
    "onnx_path = f\"{LOCAL_OUTPUT_DIR}/{model_name}.onnx\"\n",
    "config_json_path = f\"{LOCAL_OUTPUT_DIR}/{model_name}.json\"\n",
    "\n",
    "if not os.path.exists(onnx_path):\n",
    "    raise Exception(f\"ONNX model not found: {onnx_path}\")\n",
    "\n",
    "if not os.path.exists(config_json_path):\n",
    "    raise Exception(f\"Config file not found: {config_json_path}\")\n",
    "\n",
    "# Load config\n",
    "with open(config_json_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "phoneme_id_map = config.get('phoneme_id_map', {})\n",
    "model_sample_rate = config.get('audio', {}).get('sample_rate', 22050)\n",
    "\n",
    "# Create ONNX session\n",
    "session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "# Simple phoneme-based synthesis (for testing)\n",
    "# In production, use espeak-ng for proper phonemization\n",
    "def text_to_phoneme_ids(text, phoneme_id_map):\n",
    "    \"\"\"Simple text to phoneme ID conversion for testing.\"\"\"\n",
    "    ids = []\n",
    "    for char in text.lower():\n",
    "        if char in phoneme_id_map:\n",
    "            ids.extend(phoneme_id_map[char])\n",
    "        elif char == ' ':\n",
    "            if ' ' in phoneme_id_map:\n",
    "                ids.extend(phoneme_id_map[' '])\n",
    "    return ids if ids else [1, 2, 3, 4, 5]  # Fallback\n",
    "\n",
    "# Get phoneme IDs\n",
    "phoneme_ids = text_to_phoneme_ids(test_text, phoneme_id_map)\n",
    "\n",
    "# Prepare inputs\n",
    "input_array = np.array([phoneme_ids], dtype=np.int64)\n",
    "input_lengths = np.array([len(phoneme_ids)], dtype=np.int64)\n",
    "scales = np.array([0.667, 1.0, 0.8], dtype=np.float32)  # noise_scale, length_scale, noise_scale_w\n",
    "\n",
    "# Run inference\n",
    "inputs = {\n",
    "    'input': input_array,\n",
    "    'input_lengths': input_lengths,\n",
    "    'scales': scales,\n",
    "}\n",
    "\n",
    "output = session.run(None, inputs)\n",
    "audio = output[0].squeeze()\n",
    "\n",
    "print(f\"Generated audio shape: {audio.shape}\")\n",
    "print(f\"Audio duration: {len(audio) / model_sample_rate:.2f} seconds\")\n",
    "\n",
    "# Play audio\n",
    "display(Audio(audio, rate=model_sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-header"
   },
   "source": [
    "# ðŸ“‹ **Summary**\n",
    "\n",
    "This notebook provides a complete pipeline for fine-tuning Piper TTS with data from Google Drive:\n",
    "\n",
    "1. **Anti-Disconnect** - Keep Colab session alive\n",
    "2. **GPU Check** - Verify GPU availability\n",
    "3. **Mount Drive** - Access your Google Drive\n",
    "4. **Install Software** - Install Piper and dependencies from the piper1-gpl repository\n",
    "5. **Dataset Loading** - Copy dataset from Drive to local storage\n",
    "6. **Training** - Fine-tune the model with configurable parameters\n",
    "7. **Export & Save** - Export to ONNX and save to Google Drive\n",
    "\n",
    "### Output Files\n",
    "\n",
    "After training, you'll find these files in your Google Drive output folder:\n",
    "- `{model_name}.onnx` - The trained model in ONNX format\n",
    "- `{model_name}.onnx.json` - Model configuration file\n",
    "- `last.ckpt` - Latest training checkpoint (for resuming training)\n",
    "\n",
    "### Using the Model\n",
    "\n",
    "To use the trained model with Piper:\n",
    "```bash\n",
    "echo \"Hello world\" | piper --model {model_name}.onnx --output_file output.wav\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
