{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piper TTS Fine-Tuning: Indian English Voice\n",
    "\n",
    "This notebook provides a complete pipeline for fine-tuning the Piper TTS `en_US/hfc_female/medium` model\n",
    "using the IISc SPICOR English dataset to create an Indian English voice.\n",
    "\n",
    "**Compatible with:** Google Colab, AWS SageMaker\n",
    "\n",
    "## Overview\n",
    "1. Environment Setup\n",
    "2. Configuration\n",
    "3. Data ETL (Extract, Transform, Load from S3)\n",
    "4. Model Checkpoint Download\n",
    "5. Data Preprocessing\n",
    "6. Fine-Tuning with Checkpointing\n",
    "7. Resume Training from Checkpoint\n",
    "8. Export to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect runtime environment\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"Detect if running on Colab, SageMaker, or local.\"\"\"\n",
    "    if 'google.colab' in sys.modules:\n",
    "        return 'colab'\n",
    "    elif os.environ.get('SM_CURRENT_HOST'):\n",
    "        return 'sagemaker'\n",
    "    return 'local'\n",
    "\n",
    "RUNTIME_ENV = detect_environment()\n",
    "print(f\"Detected environment: {RUNTIME_ENV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install system dependencies\n",
    "if RUNTIME_ENV in ['colab', 'sagemaker']:\n",
    "    !apt-get update -qq\n",
    "    !apt-get install -y -qq build-essential cmake ninja-build espeak-ng\n",
    "    print(\"System dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python dependencies\n",
    "!pip install -q torch>=2.0 lightning>=2.0 tensorboard tensorboardX\n",
    "!pip install -q jsonargparse[signatures]>=4.27.7 pathvalidate>=3 onnx>=1\n",
    "!pip install -q pysilero-vad>=2.1 cython>=3 librosa boto3 huggingface_hub\n",
    "!pip install -q soundfile tqdm pandas\n",
    "print(\"Python dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and install Piper\n",
    "PIPER_REPO_URL = \"https://github.com/OHF-voice/piper1-gpl.git\"\n",
    "PIPER_DIR = \"/content/piper1-gpl\" if RUNTIME_ENV == 'colab' else \"./piper1-gpl\"\n",
    "\n",
    "if not os.path.exists(PIPER_DIR):\n",
    "    !git clone {PIPER_REPO_URL} {PIPER_DIR}\n",
    "\n",
    "os.chdir(PIPER_DIR)\n",
    "!pip install -e .[train] -q\n",
    "!./build_monotonic_align.sh\n",
    "!python setup.py build_ext --inplace\n",
    "print(\"Piper installed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Configure all paths and parameters. Replace placeholders with your actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for Piper fine-tuning pipeline.\"\"\"\n",
    "    \n",
    "    # ==================== PLACEHOLDERS - UPDATE THESE ====================\n",
    "    # S3 Configuration\n",
    "    s3_bucket: str = \"<YOUR_S3_BUCKET_NAME>\"  # e.g., \"my-tts-training-bucket\"\n",
    "    s3_dataset_prefix: str = \"<S3_DATASET_PATH>\"  # e.g., \"datasets/spicor\"\n",
    "    s3_checkpoint_prefix: str = \"<S3_CHECKPOINT_PATH>\"  # e.g., \"checkpoints/piper\"\n",
    "    aws_region: str = \"<AWS_REGION>\"  # e.g., \"us-east-1\"\n",
    "    \n",
    "    # AWS Credentials (leave empty to use IAM role or env vars)\n",
    "    aws_access_key_id: Optional[str] = None\n",
    "    aws_secret_access_key: Optional[str] = None\n",
    "    # =====================================================================\n",
    "    \n",
    "    # Local paths\n",
    "    base_dir: str = \"./piper_training\"\n",
    "    data_dir: str = field(default=\"\")\n",
    "    audio_dir: str = field(default=\"\")\n",
    "    cache_dir: str = field(default=\"\")\n",
    "    checkpoint_dir: str = field(default=\"\")\n",
    "    output_dir: str = field(default=\"\")\n",
    "    \n",
    "    # Model configuration\n",
    "    voice_name: str = \"en_IN-spicor-medium\"\n",
    "    espeak_voice: str = \"en-us\"  # espeak voice for phonemization\n",
    "    sample_rate: int = 22050\n",
    "    \n",
    "    # Training configuration\n",
    "    batch_size: int = 16  # Reduce if OOM\n",
    "    num_workers: int = 4\n",
    "    max_epochs: int = 1000\n",
    "    val_check_interval: int = 1000  # Validate every N steps\n",
    "    save_every_n_steps: int = 500  # Checkpoint frequency\n",
    "    \n",
    "    # Hugging Face checkpoint\n",
    "    hf_checkpoint_repo: str = \"datasets/rhasspy/piper-checkpoints\"\n",
    "    hf_checkpoint_path: str = \"en/en_US/hfc_female/medium/epoch=2868-step=1575188.ckpt\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.data_dir = f\"{self.base_dir}/data\"\n",
    "        self.audio_dir = f\"{self.base_dir}/data/audio\"\n",
    "        self.cache_dir = f\"{self.base_dir}/cache\"\n",
    "        self.checkpoint_dir = f\"{self.base_dir}/checkpoints\"\n",
    "        self.output_dir = f\"{self.base_dir}/output\"\n",
    "\n",
    "config = Config()\n",
    "print(f\"Configuration initialized for voice: {config.voice_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "def create_directories(config: Config):\n",
    "    \"\"\"Create all required directories.\"\"\"\n",
    "    dirs = [\n",
    "        config.data_dir,\n",
    "        config.audio_dir,\n",
    "        config.cache_dir,\n",
    "        config.checkpoint_dir,\n",
    "        config.output_dir,\n",
    "    ]\n",
    "    for d in dirs:\n",
    "        Path(d).mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Created: {d}\")\n",
    "\n",
    "create_directories(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data ETL (Extract, Transform, Load)\n",
    "\n",
    "Download and process the SPICOR dataset from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class S3DataLoader:\n",
    "    \"\"\"Handle S3 data operations for dataset and checkpoints.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.s3_client = self._create_s3_client()\n",
    "    \n",
    "    def _create_s3_client(self):\n",
    "        \"\"\"Create S3 client with optional credentials.\"\"\"\n",
    "        kwargs = {'region_name': self.config.aws_region}\n",
    "        if self.config.aws_access_key_id and self.config.aws_secret_access_key:\n",
    "            kwargs['aws_access_key_id'] = self.config.aws_access_key_id\n",
    "            kwargs['aws_secret_access_key'] = self.config.aws_secret_access_key\n",
    "        return boto3.client('s3', **kwargs)\n",
    "    \n",
    "    def list_objects(self, prefix: str) -> list:\n",
    "        \"\"\"List all objects under a prefix.\"\"\"\n",
    "        objects = []\n",
    "        paginator = self.s3_client.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=self.config.s3_bucket, Prefix=prefix):\n",
    "            if 'Contents' in page:\n",
    "                objects.extend([obj['Key'] for obj in page['Contents']])\n",
    "        return objects\n",
    "    \n",
    "    def download_file(self, s3_key: str, local_path: str) -> bool:\n",
    "        \"\"\"Download a single file from S3.\"\"\"\n",
    "        try:\n",
    "            Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "            self.s3_client.download_file(self.config.s3_bucket, s3_key, local_path)\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            logger.error(f\"Failed to download {s3_key}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def download_dataset(self, local_dir: str) -> int:\n",
    "        \"\"\"Download entire dataset from S3.\"\"\"\n",
    "        prefix = self.config.s3_dataset_prefix\n",
    "        objects = self.list_objects(prefix)\n",
    "        \n",
    "        downloaded = 0\n",
    "        for s3_key in tqdm(objects, desc=\"Downloading dataset\"):\n",
    "            relative_path = s3_key[len(prefix):].lstrip('/')\n",
    "            local_path = os.path.join(local_dir, relative_path)\n",
    "            if self.download_file(s3_key, local_path):\n",
    "                downloaded += 1\n",
    "        \n",
    "        logger.info(f\"Downloaded {downloaded} files from S3\")\n",
    "        return downloaded\n",
    "    \n",
    "    def upload_checkpoint(self, local_path: str, s3_key: str) -> bool:\n",
    "        \"\"\"Upload checkpoint to S3 for backup.\"\"\"\n",
    "        try:\n",
    "            self.s3_client.upload_file(local_path, self.config.s3_bucket, s3_key)\n",
    "            logger.info(f\"Uploaded checkpoint to s3://{self.config.s3_bucket}/{s3_key}\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            logger.error(f\"Failed to upload {local_path}: {e}\")\n",
    "            return False\n",
    "\n",
    "# Initialize S3 loader\n",
    "s3_loader = S3DataLoader(config)\n",
    "print(\"S3 Data Loader initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from S3\n",
    "# Uncomment and run when ready to download\n",
    "# downloaded_count = s3_loader.download_dataset(config.data_dir)\n",
    "# print(f\"Downloaded {downloaded_count} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download Pre-trained Checkpoint from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "\n",
    "def download_pretrained_checkpoint(config: Config) -> str:\n",
    "    \"\"\"Download pre-trained checkpoint from Hugging Face.\"\"\"\n",
    "    checkpoint_path = os.path.join(config.checkpoint_dir, \"pretrained.ckpt\")\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Checkpoint already exists: {checkpoint_path}\")\n",
    "        return checkpoint_path\n",
    "    \n",
    "    print(\"Downloading pre-trained checkpoint from Hugging Face...\")\n",
    "    downloaded_path = hf_hub_download(\n",
    "        repo_id=\"rhasspy/piper-checkpoints\",\n",
    "        filename=\"en/en_US/hfc_female/medium/epoch=2868-step=1575188.ckpt\",\n",
    "        repo_type=\"dataset\",\n",
    "        local_dir=config.checkpoint_dir,\n",
    "    )\n",
    "    \n",
    "    # Also download config.json\n",
    "    hf_hub_download(\n",
    "        repo_id=\"rhasspy/piper-checkpoints\",\n",
    "        filename=\"en/en_US/hfc_female/medium/config.json\",\n",
    "        repo_type=\"dataset\",\n",
    "        local_dir=config.checkpoint_dir,\n",
    "    )\n",
    "    \n",
    "    print(f\"Checkpoint downloaded to: {downloaded_path}\")\n",
    "    return downloaded_path\n",
    "\n",
    "pretrained_ckpt_path = download_pretrained_checkpoint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n",
    "\n",
    "Process SPICOR dataset and create metadata CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "class SPICORDataProcessor:\n",
    "    \"\"\"Process SPICOR dataset for Piper training.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.audio_extensions = ['.wav', '.flac', '.mp3', '.ogg']\n",
    "    \n",
    "    def find_audio_files(self, directory: str) -> List[Path]:\n",
    "        \"\"\"Find all audio files in directory.\"\"\"\n",
    "        audio_files = []\n",
    "        for ext in self.audio_extensions:\n",
    "            audio_files.extend(Path(directory).rglob(f\"*{ext}\"))\n",
    "        return sorted(audio_files)\n",
    "    \n",
    "    def get_audio_duration(self, audio_path: Path) -> float:\n",
    "        \"\"\"Get audio duration in seconds.\"\"\"\n",
    "        try:\n",
    "            info = sf.info(str(audio_path))\n",
    "            return info.duration\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    \n",
    "    def resample_audio(self, audio_path: Path, output_path: Path) -> bool:\n",
    "        \"\"\"Resample audio to target sample rate.\"\"\"\n",
    "        try:\n",
    "            audio, sr = librosa.load(str(audio_path), sr=self.config.sample_rate, mono=True)\n",
    "            sf.write(str(output_path), audio, self.config.sample_rate)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to resample {audio_path}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def parse_transcript(self, transcript_path: Path) -> dict:\n",
    "        \"\"\"Parse transcript file (adapt based on SPICOR format).\"\"\"\n",
    "        transcripts = {}\n",
    "        try:\n",
    "            with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if '|' in line:\n",
    "                        parts = line.split('|')\n",
    "                        if len(parts) >= 2:\n",
    "                            transcripts[parts[0]] = parts[1]\n",
    "                    elif '\\t' in line:\n",
    "                        parts = line.split('\\t')\n",
    "                        if len(parts) >= 2:\n",
    "                            transcripts[parts[0]] = parts[1]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to parse transcript: {e}\")\n",
    "        return transcripts\n",
    "    \n",
    "    def process_dataset(\n",
    "        self,\n",
    "        source_dir: str,\n",
    "        transcript_file: Optional[str] = None,\n",
    "        min_duration: float = 0.5,\n",
    "        max_duration: float = 15.0,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Process dataset and create metadata DataFrame.\"\"\"\n",
    "        audio_files = self.find_audio_files(source_dir)\n",
    "        logger.info(f\"Found {len(audio_files)} audio files\")\n",
    "        \n",
    "        # Load transcripts if provided\n",
    "        transcripts = {}\n",
    "        if transcript_file and os.path.exists(transcript_file):\n",
    "            transcripts = self.parse_transcript(Path(transcript_file))\n",
    "        \n",
    "        metadata = []\n",
    "        processed_dir = Path(self.config.audio_dir)\n",
    "        \n",
    "        for audio_path in tqdm(audio_files, desc=\"Processing audio files\"):\n",
    "            duration = self.get_audio_duration(audio_path)\n",
    "            \n",
    "            # Filter by duration\n",
    "            if duration < min_duration or duration > max_duration:\n",
    "                continue\n",
    "            \n",
    "            # Get transcript\n",
    "            audio_id = audio_path.stem\n",
    "            transcript = transcripts.get(audio_id, transcripts.get(audio_path.name, \"\"))\n",
    "            \n",
    "            if not transcript:\n",
    "                # Try to find transcript in .txt file with same name\n",
    "                txt_path = audio_path.with_suffix('.txt')\n",
    "                if txt_path.exists():\n",
    "                    transcript = txt_path.read_text(encoding='utf-8').strip()\n",
    "            \n",
    "            if not transcript:\n",
    "                continue\n",
    "            \n",
    "            # Resample and copy audio\n",
    "            output_path = processed_dir / f\"{audio_id}.wav\"\n",
    "            if not output_path.exists():\n",
    "                if not self.resample_audio(audio_path, output_path):\n",
    "                    continue\n",
    "            \n",
    "            metadata.append({\n",
    "                'audio_file': f\"{audio_id}.wav\",\n",
    "                'text': transcript,\n",
    "                'duration': duration,\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(metadata)\n",
    "        logger.info(f\"Processed {len(df)} valid utterances\")\n",
    "        return df\n",
    "    \n",
    "    def create_metadata_csv(self, df: pd.DataFrame, output_path: str) -> str:\n",
    "        \"\"\"Create metadata CSV in Piper format.\"\"\"\n",
    "        # Piper format: audio_file|text\n",
    "        csv_path = Path(output_path)\n",
    "        with open(csv_path, 'w', encoding='utf-8') as f:\n",
    "            for _, row in df.iterrows():\n",
    "                f.write(f\"{row['audio_file']}|{row['text']}\\n\")\n",
    "        \n",
    "        logger.info(f\"Created metadata CSV: {csv_path}\")\n",
    "        return str(csv_path)\n",
    "\n",
    "# Initialize processor\n",
    "data_processor = SPICORDataProcessor(config)\n",
    "print(\"Data processor initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process SPICOR dataset\n",
    "# Uncomment and adjust paths when dataset is downloaded\n",
    "\n",
    "# SPICOR_SOURCE_DIR = f\"{config.data_dir}/spicor_raw\"  # Adjust to your dataset structure\n",
    "# TRANSCRIPT_FILE = f\"{config.data_dir}/transcripts.txt\"  # Adjust path\n",
    "\n",
    "# metadata_df = data_processor.process_dataset(\n",
    "#     source_dir=SPICOR_SOURCE_DIR,\n",
    "#     transcript_file=TRANSCRIPT_FILE,\n",
    "#     min_duration=0.5,\n",
    "#     max_duration=15.0,\n",
    "# )\n",
    "\n",
    "# metadata_csv_path = data_processor.create_metadata_csv(\n",
    "#     metadata_df,\n",
    "#     f\"{config.data_dir}/metadata.csv\"\n",
    "# )\n",
    "\n",
    "# print(f\"Total utterances: {len(metadata_df)}\")\n",
    "# print(f\"Total audio duration: {metadata_df['duration'].sum() / 3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine-Tuning with Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from datetime import datetime\n",
    "\n",
    "class PiperTrainer:\n",
    "    \"\"\"Wrapper class for Piper model training with checkpointing.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self._setup_torch()\n",
    "    \n",
    "    def _setup_torch(self):\n",
    "        \"\"\"Configure PyTorch settings.\"\"\"\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.manual_seed(42)\n",
    "    \n",
    "    def get_callbacks(self) -> list:\n",
    "        \"\"\"Create training callbacks.\"\"\"\n",
    "        callbacks = [\n",
    "            # Save checkpoints frequently\n",
    "            ModelCheckpoint(\n",
    "                dirpath=self.config.checkpoint_dir,\n",
    "                filename='piper-{epoch:04d}-{step:08d}-{val_loss:.4f}',\n",
    "                save_top_k=3,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                every_n_train_steps=self.config.save_every_n_steps,\n",
    "                save_last=True,\n",
    "            ),\n",
    "            # Additional checkpoint for step-based saving\n",
    "            ModelCheckpoint(\n",
    "                dirpath=self.config.checkpoint_dir,\n",
    "                filename='piper-step-{step:08d}',\n",
    "                every_n_train_steps=self.config.save_every_n_steps,\n",
    "                save_top_k=-1,  # Keep all\n",
    "            ),\n",
    "            # Learning rate monitoring\n",
    "            LearningRateMonitor(logging_interval='step'),\n",
    "            # Early stopping (optional)\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=50,\n",
    "                mode='min',\n",
    "                verbose=True,\n",
    "            ),\n",
    "        ]\n",
    "        return callbacks\n",
    "    \n",
    "    def get_logger(self) -> TensorBoardLogger:\n",
    "        \"\"\"Create TensorBoard logger.\"\"\"\n",
    "        return TensorBoardLogger(\n",
    "            save_dir=self.config.output_dir,\n",
    "            name='piper_training',\n",
    "            version=datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "        )\n",
    "    \n",
    "    def find_latest_checkpoint(self) -> Optional[str]:\n",
    "        \"\"\"Find the latest checkpoint for resuming training.\"\"\"\n",
    "        ckpt_dir = Path(self.config.checkpoint_dir)\n",
    "        \n",
    "        # Look for 'last.ckpt' first\n",
    "        last_ckpt = ckpt_dir / 'last.ckpt'\n",
    "        if last_ckpt.exists():\n",
    "            return str(last_ckpt)\n",
    "        \n",
    "        # Find most recent checkpoint by modification time\n",
    "        checkpoints = list(ckpt_dir.glob('*.ckpt'))\n",
    "        if checkpoints:\n",
    "            latest = max(checkpoints, key=lambda p: p.stat().st_mtime)\n",
    "            return str(latest)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        metadata_csv: str,\n",
    "        pretrained_ckpt: Optional[str] = None,\n",
    "        resume_from_checkpoint: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"Run training with checkpointing.\"\"\"\n",
    "        from piper.train.vits.dataset import VitsDataModule\n",
    "        from piper.train.vits.lightning import VitsModel\n",
    "        \n",
    "        # Determine checkpoint path\n",
    "        ckpt_path = resume_from_checkpoint\n",
    "        if ckpt_path is None and pretrained_ckpt:\n",
    "            ckpt_path = pretrained_ckpt\n",
    "        \n",
    "        # Create data module\n",
    "        data_module = VitsDataModule(\n",
    "            csv_path=metadata_csv,\n",
    "            cache_dir=self.config.cache_dir,\n",
    "            espeak_voice=self.config.espeak_voice,\n",
    "            config_path=f\"{self.config.output_dir}/{self.config.voice_name}.json\",\n",
    "            voice_name=self.config.voice_name,\n",
    "            sample_rate=self.config.sample_rate,\n",
    "            audio_dir=self.config.audio_dir,\n",
    "            batch_size=self.config.batch_size,\n",
    "            num_workers=self.config.num_workers,\n",
    "        )\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = L.Trainer(\n",
    "            max_epochs=self.config.max_epochs,\n",
    "            accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "            devices=1,\n",
    "            callbacks=self.get_callbacks(),\n",
    "            logger=self.get_logger(),\n",
    "            val_check_interval=self.config.val_check_interval,\n",
    "            log_every_n_steps=50,\n",
    "            precision='16-mixed' if torch.cuda.is_available() else 32,\n",
    "            gradient_clip_val=1.0,\n",
    "        )\n",
    "        \n",
    "        # Load or create model\n",
    "        if ckpt_path and Path(ckpt_path).exists():\n",
    "            print(f\"Loading model from checkpoint: {ckpt_path}\")\n",
    "            model = VitsModel.load_from_checkpoint(ckpt_path)\n",
    "        else:\n",
    "            print(\"Creating new model\")\n",
    "            model = VitsModel(\n",
    "                batch_size=self.config.batch_size,\n",
    "                sample_rate=self.config.sample_rate,\n",
    "            )\n",
    "        \n",
    "        # Start training\n",
    "        print(f\"Starting training for {self.config.max_epochs} epochs...\")\n",
    "        trainer.fit(\n",
    "            model,\n",
    "            data_module,\n",
    "            ckpt_path=resume_from_checkpoint,  # Only for resuming optimizer state\n",
    "        )\n",
    "        \n",
    "        return trainer, model\n",
    "\n",
    "# Initialize trainer\n",
    "piper_trainer = PiperTrainer(config)\n",
    "print(\"Piper Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "# Uncomment when ready to train\n",
    "\n",
    "# METADATA_CSV = f\"{config.data_dir}/metadata.csv\"\n",
    "\n",
    "# trainer, model = piper_trainer.train(\n",
    "#     metadata_csv=METADATA_CSV,\n",
    "#     pretrained_ckpt=pretrained_ckpt_path,\n",
    "#     resume_from_checkpoint=None,  # Set to checkpoint path to resume\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Resume Training from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_training(config: Config, metadata_csv: str) -> tuple:\n",
    "    \"\"\"Resume training from the latest checkpoint.\"\"\"\n",
    "    trainer_instance = PiperTrainer(config)\n",
    "    \n",
    "    # Find latest checkpoint\n",
    "    latest_ckpt = trainer_instance.find_latest_checkpoint()\n",
    "    \n",
    "    if latest_ckpt:\n",
    "        print(f\"Resuming from checkpoint: {latest_ckpt}\")\n",
    "        return trainer_instance.train(\n",
    "            metadata_csv=metadata_csv,\n",
    "            resume_from_checkpoint=latest_ckpt,\n",
    "        )\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting fresh training.\")\n",
    "        return trainer_instance.train(\n",
    "            metadata_csv=metadata_csv,\n",
    "            pretrained_ckpt=download_pretrained_checkpoint(config),\n",
    "        )\n",
    "\n",
    "# Resume training\n",
    "# Uncomment when ready\n",
    "\n",
    "# METADATA_CSV = f\"{config.data_dir}/metadata.csv\"\n",
    "# trainer, model = resume_training(config, METADATA_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export to ONNX\n",
    "\n",
    "Export the fine-tuned model to ONNX format for web deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "from pathlib import Path\n",
    "\n",
    "OPSET_VERSION = 15\n",
    "\n",
    "class ONNXExporter:\n",
    "    \"\"\"Export Piper model to ONNX format.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "    \n",
    "    def export(\n",
    "        self,\n",
    "        checkpoint_path: str,\n",
    "        output_path: Optional[str] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Export model checkpoint to ONNX.\"\"\"\n",
    "        from piper.train.vits.lightning import VitsModel\n",
    "        \n",
    "        torch.manual_seed(1234)\n",
    "        \n",
    "        if output_path is None:\n",
    "            output_path = f\"{self.config.output_dir}/{self.config.voice_name}.onnx\"\n",
    "        \n",
    "        output_path = Path(output_path)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Load model\n",
    "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "        model = VitsModel.load_from_checkpoint(checkpoint_path, map_location='cpu')\n",
    "        model_g = model.model_g\n",
    "        \n",
    "        # Set to eval mode\n",
    "        model_g.eval()\n",
    "        \n",
    "        # Remove weight normalization\n",
    "        with torch.no_grad():\n",
    "            model_g.dec.remove_weight_norm()\n",
    "        \n",
    "        # Create inference forward function\n",
    "        def infer_forward(text, text_lengths, scales, sid=None):\n",
    "            noise_scale = scales[0]\n",
    "            length_scale = scales[1]\n",
    "            noise_scale_w = scales[2]\n",
    "            audio = model_g.infer(\n",
    "                text,\n",
    "                text_lengths,\n",
    "                noise_scale=noise_scale,\n",
    "                length_scale=length_scale,\n",
    "                noise_scale_w=noise_scale_w,\n",
    "                sid=sid,\n",
    "            )[0].unsqueeze(1)\n",
    "            return audio\n",
    "        \n",
    "        model_g.forward = infer_forward\n",
    "        \n",
    "        # Prepare dummy input\n",
    "        num_symbols = model_g.n_vocab\n",
    "        num_speakers = model_g.n_speakers\n",
    "        \n",
    "        dummy_input_length = 50\n",
    "        sequences = torch.randint(\n",
    "            low=0, high=num_symbols, size=(1, dummy_input_length), dtype=torch.long\n",
    "        )\n",
    "        sequence_lengths = torch.LongTensor([sequences.size(1)])\n",
    "        \n",
    "        sid = None\n",
    "        if num_speakers > 1:\n",
    "            sid = torch.LongTensor([0])\n",
    "        \n",
    "        scales = torch.FloatTensor([0.667, 1.0, 0.8])\n",
    "        dummy_input = (sequences, sequence_lengths, scales, sid)\n",
    "        \n",
    "        # Export to ONNX\n",
    "        print(f\"Exporting to ONNX: {output_path}\")\n",
    "        torch.onnx.export(\n",
    "            model=model_g,\n",
    "            args=dummy_input,\n",
    "            f=str(output_path),\n",
    "            verbose=False,\n",
    "            opset_version=OPSET_VERSION,\n",
    "            input_names=['input', 'input_lengths', 'scales', 'sid'],\n",
    "            output_names=['output'],\n",
    "            dynamic_axes={\n",
    "                'input': {0: 'batch_size', 1: 'phonemes'},\n",
    "                'input_lengths': {0: 'batch_size'},\n",
    "                'output': {0: 'batch_size', 2: 'time'},\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        # Verify the exported model\n",
    "        print(\"Verifying ONNX model...\")\n",
    "        onnx_model = onnx.load(str(output_path))\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        \n",
    "        print(f\"Model exported successfully to: {output_path}\")\n",
    "        print(f\"Model size: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        return str(output_path)\n",
    "    \n",
    "    def copy_config(self, source_config: str, output_path: Optional[str] = None):\n",
    "        \"\"\"Copy and rename config file to match ONNX model.\"\"\"\n",
    "        import shutil\n",
    "        \n",
    "        if output_path is None:\n",
    "            output_path = f\"{self.config.output_dir}/{self.config.voice_name}.onnx.json\"\n",
    "        \n",
    "        shutil.copy(source_config, output_path)\n",
    "        print(f\"Config copied to: {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "# Initialize exporter\n",
    "onnx_exporter = ONNXExporter(config)\n",
    "print(\"ONNX Exporter initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model to ONNX\n",
    "# Uncomment when training is complete\n",
    "\n",
    "# Find best checkpoint\n",
    "# best_ckpt = piper_trainer.find_latest_checkpoint()\n",
    "# Or specify manually:\n",
    "# best_ckpt = f\"{config.checkpoint_dir}/your-checkpoint.ckpt\"\n",
    "\n",
    "# Export to ONNX\n",
    "# onnx_path = onnx_exporter.export(best_ckpt)\n",
    "\n",
    "# Copy config file\n",
    "# config_source = f\"{config.output_dir}/{config.voice_name}.json\"\n",
    "# onnx_exporter.copy_config(config_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Upload Final Model to S3 (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_final_model(config: Config, s3_loader: S3DataLoader):\n",
    "    \"\"\"Upload final ONNX model and config to S3.\"\"\"\n",
    "    onnx_path = f\"{config.output_dir}/{config.voice_name}.onnx\"\n",
    "    config_path = f\"{config.output_dir}/{config.voice_name}.onnx.json\"\n",
    "    \n",
    "    s3_prefix = f\"{config.s3_checkpoint_prefix}/final\"\n",
    "    \n",
    "    # Upload ONNX model\n",
    "    s3_loader.upload_checkpoint(\n",
    "        onnx_path,\n",
    "        f\"{s3_prefix}/{config.voice_name}.onnx\"\n",
    "    )\n",
    "    \n",
    "    # Upload config\n",
    "    s3_loader.upload_checkpoint(\n",
    "        config_path,\n",
    "        f\"{s3_prefix}/{config.voice_name}.onnx.json\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Model uploaded to s3://{config.s3_bucket}/{s3_prefix}/\")\n",
    "\n",
    "# Upload final model\n",
    "# Uncomment when ready\n",
    "# upload_final_model(config, s3_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test the Exported Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def test_onnx_model(onnx_path: str, config_path: str, text: str = \"Hello, this is a test.\"):\n",
    "    \"\"\"Test the exported ONNX model.\"\"\"\n",
    "    # Load config\n",
    "    with open(config_path, 'r') as f:\n",
    "        model_config = json.load(f)\n",
    "    \n",
    "    phoneme_id_map = model_config['phoneme_id_map']\n",
    "    \n",
    "    # Create ONNX session\n",
    "    session = ort.InferenceSession(onnx_path)\n",
    "    \n",
    "    # For testing, use simple phoneme IDs\n",
    "    # In production, use espeak-ng for proper phonemization\n",
    "    test_phonemes = [phoneme_id_map.get(c, [0])[0] for c in text.lower() if c in phoneme_id_map]\n",
    "    \n",
    "    if not test_phonemes:\n",
    "        test_phonemes = [1, 2, 3, 4, 5]  # Fallback\n",
    "    \n",
    "    # Prepare inputs\n",
    "    input_array = np.array([test_phonemes], dtype=np.int64)\n",
    "    input_lengths = np.array([len(test_phonemes)], dtype=np.int64)\n",
    "    scales = np.array([0.667, 1.0, 0.8], dtype=np.float32)\n",
    "    \n",
    "    # Run inference\n",
    "    inputs = {\n",
    "        'input': input_array,\n",
    "        'input_lengths': input_lengths,\n",
    "        'scales': scales,\n",
    "    }\n",
    "    \n",
    "    output = session.run(None, inputs)\n",
    "    audio = output[0]\n",
    "    \n",
    "    print(f\"Generated audio shape: {audio.shape}\")\n",
    "    print(f\"Audio duration: {audio.shape[-1] / model_config['audio']['sample_rate']:.2f} seconds\")\n",
    "    \n",
    "    return audio\n",
    "\n",
    "# Test the model\n",
    "# Uncomment when ONNX export is complete\n",
    "\n",
    "# onnx_path = f\"{config.output_dir}/{config.voice_name}.onnx\"\n",
    "# config_path = f\"{config.output_dir}/{config.voice_name}.onnx.json\"\n",
    "# audio = test_onnx_model(onnx_path, config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete pipeline for:\n",
    "\n",
    "1. **Environment Setup**: Install all dependencies for Colab/SageMaker\n",
    "2. **Configuration**: Centralized config with S3 and training parameters\n",
    "3. **Data ETL**: Download SPICOR dataset from S3 and process it\n",
    "4. **Checkpoint Download**: Get pre-trained model from Hugging Face\n",
    "5. **Data Preprocessing**: Create metadata CSV in Piper format\n",
    "6. **Fine-Tuning**: Train with frequent checkpointing (every 500 steps)\n",
    "7. **Resume Training**: Automatically find and resume from latest checkpoint\n",
    "8. **ONNX Export**: Export to lightweight ONNX format for web deployment\n",
    "9. **S3 Upload**: Backup final model to S3\n",
    "10. **Testing**: Verify the exported model works correctly\n",
    "\n",
    "### Key Features:\n",
    "- Modular design with separate classes for each component\n",
    "- Automatic checkpoint saving every 500 training steps\n",
    "- Support for resuming training from any checkpoint\n",
    "- S3 integration for data and checkpoint storage\n",
    "- ONNX export with model verification\n",
    "- Compatible with both Google Colab and AWS SageMaker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
