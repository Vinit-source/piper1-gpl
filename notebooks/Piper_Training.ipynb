{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vinit-source/piper1-gpl/blob/main/notebooks/Piper_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0) Colab runtime + GPU check"
      ],
      "metadata": {
        "id": "ulTkpdiGQjI4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y37QtlLmPuqT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb1415f-0e88-4477-d9f6-9004e85355e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12\n",
            "PyTorch: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "Thu Dec 18 12:03:54 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8             11W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU\n",
        "import torch, platform, sys\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) System packages (incl. eSpeak dev)"
      ],
      "metadata": {
        "id": "tRWDRJUpQrfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y build-essential cmake ninja-build espeak-ng espeak-ng-data libespeak-ng-dev pkg-config ffmpeg\n",
        "!pkg-config --modversion espeak-ng"
      ],
      "metadata": {
        "id": "aN5lcQRZQs3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd79a4df-c12f-4f16-8ff6-d08fc014bc53"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [1 InRelease 14.2 kB/129 k\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connected to cloud.r-proj\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \r                                                                               \rGet:3 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \r                                                                               \rHit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \r                                                                               \rHit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r                                                                               \r0% [Connected to r2u.stat.illinois.edu (192.17.190.167)] [Waiting for headers]\r                                                                              \rHit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,633 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,287 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\n",
            "Get:15 https://cli.github.com/packages stable/main amd64 Packages [345 B]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,205 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,551 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,598 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,965 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.3 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,411 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [114 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [40.3 kB]\n",
            "Get:24 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,851 kB]\n",
            "Fetched 38.3 MB in 5s (7,277 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libbz2-dev libpkgconf3 libreadline-dev\n",
            "Use 'sudo apt autoremove' to remove them.\n",
            "The following packages will be REMOVED:\n",
            "  pkgconf r-base-dev\n",
            "The following NEW packages will be installed:\n",
            "  espeak-ng espeak-ng-data libespeak-ng-dev libespeak-ng1 libpcaudio0\n",
            "  libsonic0 ninja-build pkg-config\n",
            "0 upgraded, 8 newly installed, 2 to remove and 50 not upgraded.\n",
            "Need to get 4,939 kB of archives.\n",
            "After this operation, 13.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 pkg-config amd64 0.29.2-1ubuntu3 [48.2 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpcaudio0 amd64 1.1-6build2 [8,956 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsonic0 amd64 0.2.0-11build1 [10.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 espeak-ng-data amd64 1.50+dfsg-10ubuntu0.1 [3,956 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libespeak-ng1 amd64 1.50+dfsg-10ubuntu0.1 [207 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 espeak-ng amd64 1.50+dfsg-10ubuntu0.1 [343 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libespeak-ng-dev amd64 1.50+dfsg-10ubuntu0.1 [254 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ninja-build amd64 1.10.1-1 [111 kB]\n",
            "Fetched 4,939 kB in 1s (5,354 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 8.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Removing r-base-dev (4.5.2-1.2204.0) ...\n",
            "dpkg: pkgconf: dependency problems, but removing anyway as you requested:\n",
            " libsndfile1-dev:amd64 depends on pkg-config; however:\n",
            "  Package pkg-config is not installed.\n",
            "  Package pkgconf which provides pkg-config is to be removed.\n",
            " libmkl-dev:amd64 depends on pkg-config; however:\n",
            "  Package pkg-config is not installed.\n",
            "  Package pkgconf which provides pkg-config is to be removed.\n",
            " libglib2.0-dev:amd64 depends on pkg-config; however:\n",
            "  Package pkg-config is not installed.\n",
            "  Package pkgconf which provides pkg-config is to be removed.\n",
            " libfontconfig-dev:amd64 depends on pkg-config; however:\n",
            "  Package pkg-config is not installed.\n",
            "  Package pkgconf which provides pkg-config is to be removed.\n",
            "\n",
            "Removing pkgconf (1.8.0-1) ...\n",
            "Removing 'diversion of /usr/bin/pkg-config to /usr/bin/pkg-config.real by pkgconf'\n",
            "Removing 'diversion of /usr/share/aclocal/pkg.m4 to /usr/share/aclocal/pkg.real.m4 by pkgconf'\n",
            "Removing 'diversion of /usr/share/man/man1/pkg-config.1.gz to /usr/share/man/man1/pkg-config.real.1.gz by pkgconf'\n",
            "Removing 'diversion of /usr/share/pkg-config-crosswrapper to /usr/share/pkg-config-crosswrapper.real by pkgconf'\n",
            "Selecting previously unselected package pkg-config.\n",
            "(Reading database ... 121666 files and directories currently installed.)\n",
            "Preparing to unpack .../0-pkg-config_0.29.2-1ubuntu3_amd64.deb ...\n",
            "Unpacking pkg-config (0.29.2-1ubuntu3) ...\n",
            "Selecting previously unselected package libpcaudio0:amd64.\n",
            "Preparing to unpack .../1-libpcaudio0_1.1-6build2_amd64.deb ...\n",
            "Unpacking libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../2-libsonic0_0.2.0-11build1_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Selecting previously unselected package espeak-ng-data:amd64.\n",
            "Preparing to unpack .../3-espeak-ng-data_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package libespeak-ng1:amd64.\n",
            "Preparing to unpack .../4-libespeak-ng1_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package espeak-ng.\n",
            "Preparing to unpack .../5-espeak-ng_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package libespeak-ng-dev:amd64.\n",
            "Preparing to unpack .../6-libespeak-ng-dev_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking libespeak-ng-dev:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package ninja-build.\n",
            "Preparing to unpack .../7-ninja-build_1.10.1-1_amd64.deb ...\n",
            "Unpacking ninja-build (1.10.1-1) ...\n",
            "Setting up ninja-build (1.10.1-1) ...\n",
            "Setting up libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Setting up pkg-config (0.29.2-1ubuntu3) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Setting up espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libespeak-ng-dev:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "1.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Clone repo fresh"
      ],
      "metadata": {
        "id": "J_MRPygaQ1cQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!rm -rf piper1-gpl\n",
        "!git clone https://github.com/OHF-voice/piper1-gpl.git\n",
        "%cd piper1-gpl\n",
        "!pwd"
      ],
      "metadata": {
        "id": "w-0x1IWPQ7UB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa1319d-d47d-4e21-bb95-2f5289455068"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'piper1-gpl'...\n",
            "remote: Enumerating objects: 817, done.\u001b[K\n",
            "remote: Counting objects: 100% (308/308), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 817 (delta 243), reused 216 (delta 216), pack-reused 509 (from 2)\u001b[K\n",
            "Receiving objects: 100% (817/817), 4.69 MiB | 16.56 MiB/s, done.\n",
            "Resolving deltas: 100% (459/459), done.\n",
            "/content/piper1-gpl\n",
            "/content/piper1-gpl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Python deps (editable install, no venv in Colab)"
      ],
      "metadata": {
        "id": "k_qVUXtoRArT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install --upgrade pip setuptools wheel\n",
        "!python3 -m pip install -e \".[train]\""
      ],
      "metadata": {
        "id": "L9ARM5C9RDZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32fa2447-c57b-44c0-f99f-e5bb5b9dcb1b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (75.2.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-25.3 setuptools-80.9.0\n",
            "Obtaining file:///content/piper1-gpl\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnxruntime<2,>=1 (from piper-tts==1.3.1)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: torch<3,>=2 in /usr/local/lib/python3.12/dist-packages (from piper-tts==1.3.1) (2.9.0+cu126)\n",
            "Collecting lightning<3,>=2 (from piper-tts==1.3.1)\n",
            "  Downloading lightning-2.6.0-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: tensorboard<3,>=2 in /usr/local/lib/python3.12/dist-packages (from piper-tts==1.3.1) (2.19.0)\n",
            "Collecting tensorboardX<3,>=2 (from piper-tts==1.3.1)\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting jsonargparse>=4.27.7 (from jsonargparse[signatures]>=4.27.7; extra == \"train\"->piper-tts==1.3.1)\n",
            "  Downloading jsonargparse-4.44.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pathvalidate<4,>=3 (from piper-tts==1.3.1)\n",
            "  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting onnx<2,>=1 (from piper-tts==1.3.1)\n",
            "  Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting pysilero-vad<3,>=2.1 (from piper-tts==1.3.1)\n",
            "  Downloading pysilero_vad-2.1.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: cython<4,>=3 in /usr/local/lib/python3.12/dist-packages (from piper-tts==1.3.1) (3.0.12)\n",
            "Requirement already satisfied: librosa<1 in /usr/local/lib/python3.12/dist-packages (from piper-tts==1.3.1) (0.11.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.1) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.1) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.1) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.1) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.1) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.1) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.1) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.1) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.1) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.1) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.1) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.1) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa<1->piper-tts==1.3.1) (1.1.2)\n",
            "Requirement already satisfied: PyYAML<8.0,>5.4 in /usr/local/lib/python3.12/dist-packages (from lightning<3,>=2->piper-tts==1.3.1) (6.0.3)\n",
            "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.1) (2025.3.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<3,>=2->piper-tts==1.3.1)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.12/dist-packages (from lightning<3,>=2->piper-tts==1.3.1) (25.0)\n",
            "Collecting torchmetrics<3.0,>0.7.0 (from lightning<3,>=2->piper-tts==1.3.1)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from lightning<3,>=2->piper-tts==1.3.1) (4.67.1)\n",
            "Collecting pytorch-lightning (from lightning<3,>=2->piper-tts==1.3.1)\n",
            "  Downloading pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.1) (3.13.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<3,>=2->piper-tts==1.3.1) (80.9.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx<2,>=1->piper-tts==1.3.1) (5.29.5)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx<2,>=1->piper-tts==1.3.1) (0.5.4)\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1->piper-tts==1.3.1)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1->piper-tts==1.3.1) (25.9.23)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1->piper-tts==1.3.1) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2->piper-tts==1.3.1) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2->piper-tts==1.3.1) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2->piper-tts==1.3.1) (3.10)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2->piper-tts==1.3.1) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2->piper-tts==1.3.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2->piper-tts==1.3.1) (3.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (3.20.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2->piper-tts==1.3.1) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.1) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.1) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.1) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.1) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.1) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning<3,>=2->piper-tts==1.3.1) (3.11)\n",
            "Requirement already satisfied: docstring-parser>=0.17 in /usr/local/lib/python3.12/dist-packages (from jsonargparse[signatures]>=4.27.7; extra == \"train\"->piper-tts==1.3.1) (0.17.0)\n",
            "Collecting typeshed-client>=2.8.2 (from jsonargparse[signatures]>=4.27.7; extra == \"train\"->piper-tts==1.3.1)\n",
            "  Downloading typeshed_client-2.8.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa<1->piper-tts==1.3.1) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa<1->piper-tts==1.3.1) (4.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa<1->piper-tts==1.3.1) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa<1->piper-tts==1.3.1) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa<1->piper-tts==1.3.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa<1->piper-tts==1.3.1) (2025.11.12)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa<1->piper-tts==1.3.1) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa<1->piper-tts==1.3.1) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa<1->piper-tts==1.3.1) (2.23)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime<2,>=1->piper-tts==1.3.1) (1.3.0)\n",
            "Requirement already satisfied: importlib_resources>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from typeshed-client>=2.8.2->jsonargparse[signatures]>=4.27.7; extra == \"train\"->piper-tts==1.3.1) (6.5.2)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard<3,>=2->piper-tts==1.3.1) (3.0.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1->piper-tts==1.3.1)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Downloading lightning-2.6.0-py3-none-any.whl (845 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m846.0/846.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n",
            "Downloading pysilero_vad-2.1.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonargparse-4.44.0-py3-none-any.whl (241 kB)\n",
            "Downloading typeshed_client-2.8.2-py3-none-any.whl (760 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.5/760.5 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Downloading pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: piper-tts\n",
            "  Building editable for piper-tts (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for piper-tts: filename=piper_tts-1.3.1-0.editable-py3-none-any.whl size=14403 sha256=5ae01e12b6c25ba416d1a2af97b4061fdceb6481f6f1cf57dfa089f7cafd180f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9z3em63b/wheels/22/38/4e/d9fd0ab8ba964eb8332fc89984f961285397f819ab17291146\n",
            "Successfully built piper-tts\n",
            "Installing collected packages: typeshed-client, tensorboardX, pathvalidate, lightning-utilities, jsonargparse, humanfriendly, onnx, coloredlogs, onnxruntime, torchmetrics, pysilero-vad, piper-tts, pytorch-lightning, lightning\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [lightning]\n",
            "\u001b[1A\u001b[2KSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 jsonargparse-4.44.0 lightning-2.6.0 lightning-utilities-0.15.2 onnx-1.20.0 onnxruntime-1.23.2 pathvalidate-3.3.1 piper-tts-1.3.1 pysilero-vad-2.1.1 pytorch-lightning-2.6.0 tensorboardX-2.6.4 torchmetrics-1.8.2 typeshed-client-2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Build the Cython extension used for alignment"
      ],
      "metadata": {
        "id": "OXkhMJRkRUGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/piper1-gpl\n",
        "!chmod +x ./build_monotonic_align.sh\n",
        "!./build_monotonic_align.sh"
      ],
      "metadata": {
        "id": "EMQF3quvRVe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e23c735-2408-46c0-bfe2-fdb755a777a9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/piper1-gpl\n",
            "Compiling /content/piper1-gpl/src/piper/train/vits/monotonic_align/core.pyx because it changed.\n",
            "[1/1] Cythonizing /content/piper1-gpl/src/piper/train/vits/monotonic_align/core.pyx\n",
            "/usr/local/lib/python3.12/dist-packages/Cython/Compiler/Main.py:381: FutureWarning: Cython directive 'language_level' not set, using '3str' for now (Py3). This has changed from earlier releases! File: /content/piper1-gpl/src/piper/train/vits/monotonic_align/core.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "performance hint: core.pyx:7:5: Exception check on 'maximum_path_each' will always require the GIL to be acquired.\n",
            "Possible solutions:\n",
            "\t1. Declare 'maximum_path_each' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n",
            "\t2. Use an 'int' return type on 'maximum_path_each' to allow an error code to be returned.\n",
            "performance hint: core.pyx:38:6: Exception check on 'maximum_path_c' will always require the GIL to be acquired.\n",
            "Possible solutions:\n",
            "\t1. Declare 'maximum_path_c' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n",
            "\t2. Use an 'int' return type on 'maximum_path_c' to allow an error code to be returned.\n",
            "performance hint: core.pyx:42:21: Exception check after calling 'maximum_path_each' will always require the GIL to be acquired.\n",
            "Possible solutions:\n",
            "\t1. Declare 'maximum_path_each' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n",
            "\t2. Use an 'int' return type on 'maximum_path_each' to allow an error code to be returned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Dev build (repo mode)"
      ],
      "metadata": {
        "id": "f8P9iQhjRa81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install --upgrade pip setuptools wheel scikit-build cmake ninja"
      ],
      "metadata": {
        "id": "ZIcRcqt6p3kM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee67500-b6fd-490c-9e8d-f2b845b5d926"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (80.9.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Collecting scikit-build\n",
            "  Using cached scikit_build-0.18.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.12/dist-packages (3.31.10)\n",
            "Collecting cmake\n",
            "  Using cached cmake-4.2.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting ninja\n",
            "  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: distro in /usr/local/lib/python3.12/dist-packages (from scikit-build) (1.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from scikit-build) (25.0)\n",
            "Using cached scikit_build-0.18.1-py3-none-any.whl (85 kB)\n",
            "Using cached cmake-4.2.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (28.9 MB)\n",
            "Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "Installing collected packages: scikit-build, ninja, cmake\n",
            "\u001b[2K  Attempting uninstall: cmake\n",
            "\u001b[2K    Found existing installation: cmake 3.31.10\n",
            "\u001b[2K    Uninstalling cmake-3.31.10:\n",
            "\u001b[2K      Successfully uninstalled cmake-3.31.10\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [cmake]\n",
            "\u001b[1A\u001b[2KSuccessfully installed cmake-4.2.0 ninja-1.13.0 scikit-build-0.18.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/piper1-gpl\n",
        "!python3 setup.py build_ext --inplace -v"
      ],
      "metadata": {
        "id": "bTNUkZKmRcJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7627f320-49b3-4f40-b94a-d52e3ab546c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/piper1-gpl\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "-- Trying 'Ninja' generator\n",
            "--------------------------------\n",
            "---------------------------\n",
            "----------------------\n",
            "-----------------\n",
            "------------\n",
            "-------\n",
            "--\n",
            "Not searching for unused variables given on the command line.\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Configuring done (0.6s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/piper1-gpl/_cmake_test_compile/build\n",
            "--\n",
            "-------\n",
            "------------\n",
            "-----------------\n",
            "----------------------\n",
            "---------------------------\n",
            "--------------------------------\n",
            "-- Trying 'Ninja' generator - success\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Configuring Project\n",
            "  Working directory:\n",
            "    /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build\n",
            "  Command:\n",
            "    /usr/local/lib/python3.12/dist-packages/cmake/data/bin/cmake /content/piper1-gpl -G Ninja -DCMAKE_MAKE_PROGRAM:FILEPATH=/usr/local/bin/ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-install/src/piper -DPYTHON_VERSION_STRING:STRING=3.12.12 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/usr/local/lib/python3.12/dist-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/usr/bin/python3 -DPYTHON_INCLUDE_DIR:PATH=/usr/include/python3.12 -DPYTHON_LIBRARY:PATH=/usr/lib/x86_64-linux-gnu/libpython3.12.so -DPython_EXECUTABLE:PATH=/usr/bin/python3 -DPython_ROOT_DIR:PATH=/usr -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/usr/include/python3.12 -DPython_NumPy_INCLUDE_DIRS:PATH=/usr/local/lib/python3.12/dist-packages/numpy/_core/include -DPython3_EXECUTABLE:PATH=/usr/bin/python3 -DPython3_ROOT_DIR:PATH=/usr -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/usr/include/python3.12 -DPython3_NumPy_INCLUDE_DIRS:PATH=/usr/local/lib/python3.12/dist-packages/numpy/_core/include -DCMAKE_MAKE_PROGRAM:FILEPATH=/usr/local/bin/ninja -DCMAKE_BUILD_TYPE:STRING=Release\n",
            "\n",
            "Not searching for unused variables given on the command line.\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Python: /usr/include/python3.12 (found version \"3.12.12\") found components: Development.Module Development.SABIModule\n",
            "-- Configuring done (0.5s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build\n",
            "[2/12] Performing download step (git clone) for 'espeak_ng_external'\u001b[K\n",
            "Cloning into 'espeak_ng_external'...\n",
            "HEAD is now at 212928b3 Added some words in the ru_listx file. (#2150)\n",
            "[3/12] Performing disconnected update step for 'espeak_ng_external'\u001b[K\n",
            "-- Already at requested ref: 212928b394a96e8fd2096616bfd54e17845c48f6\n",
            "[5/12] Performing configure step for 'espeak_ng_external'\u001b[K\n",
            "CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Looking for mkstemp\n",
            "-- Looking for mkstemp - found\n",
            "-- Configuration:\n",
            "--   shared: OFF\n",
            "--   mbrola: OFF (MBROLA_BIN-NOTFOUND)\n",
            "--   libsonic: OFF (sonic /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/_deps/sonic-git-src)\n",
            "--   libpcaudio: OFF (PCAUDIO_LIB-NOTFOUND PCAUDIO_INC-NOTFOUND)\n",
            "--   klatt: OFF\n",
            "--   speech-player: OFF\n",
            "--   async: OFF\n",
            "-- Configuring done (2.0s)\n",
            "-- Generating done (0.1s)\n",
            "-- Build files have been written to: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build\n",
            "[6/12] Performing build step for 'espeak_ng_external'\u001b[K\n",
            "[1/167] Building C object src/ucd-tools/CMakeFiles/ucd.dir/src/case.c.o\n",
            "[2/167] Building C object src/ucd-tools/CMakeFiles/ucd.dir/src/ctype.c.o\n",
            "[3/167] Building C object src/ucd-tools/CMakeFiles/ucd.dir/src/categories.c.o\n",
            "[4/167] Building C object src/ucd-tools/CMakeFiles/ucd.dir/src/tostring.c.o\n",
            "[5/167] Building C object src/ucd-tools/CMakeFiles/ucd.dir/src/scripts.c.o\n",
            "[6/167] Building C object src/ucd-tools/CMakeFiles/ucd.dir/src/proplist.c.o\n",
            "[7/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/common.c.o\n",
            "[8/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/mnemonics.c.o\n",
            "[9/167] Linking C static library src/ucd-tools/libucd.a\n",
            "[10/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/ieee80.c.o\n",
            "[11/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/error.c.o\n",
            "[12/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/compiledict.c.o\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledict.c: In function ‘espeak_ng_CompileDictionary’:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledict.c:1558:30: warning: ‘rules.txt’ directive writing 9 bytes into a region of size between 6 and 205 [-Wformat-overflow=]\n",
            " 1558 |         sprintf(fname_in, \"%srules.txt\", path);\n",
            "      |                              ^~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledict.c:1558:9: note: ‘sprintf’ output between 10 and 209 bytes into a destination of size 205\n",
            " 1558 |         sprintf(fname_in, \"%srules.txt\", path);\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "[13/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/encoding.c.o\n",
            "[14/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/compiledata.c.o\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c: In function ‘LoadSpect’:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:924:47: warning: ‘%s’ directive output may be truncated writing up to 199 bytes into a region of size 180 [-Wformat-truncation=]\n",
            "  924 |         snprintf(filename, sizeof(filename), \"%s/%s\", ctx->phsrc, path);\n",
            "      |                                               ^~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:924:9: note: ‘snprintf’ output 2 or more bytes (assuming 201) into a destination of size 180\n",
            "  924 |         snprintf(filename, sizeof(filename), \"%s/%s\", ctx->phsrc, path);\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c: In function ‘CompilePhonemeFiles’:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2278:42: warning: ‘%s’ directive writing up to 255 bytes into a region of size between 80 and 279 [-Wformat-overflow=]\n",
            " 2278 |                         sprintf(buf, \"%s/%s\", ctx->phsrc, ctx->item_string);\n",
            "      |                                          ^~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2278:25: note: ‘sprintf’ output between 2 and 456 bytes into a destination of size 280\n",
            " 2278 |                         sprintf(buf, \"%s/%s\", ctx->phsrc, ctx->item_string);\n",
            "      |                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c: In function ‘espeak_ng_CompilePhonemeDataPath’:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2368:27: warning: ‘/phonemes’ directive writing 9 bytes into a region of size between 1 and 200 [-Wformat-overflow=]\n",
            " 2368 |         sprintf(fname, \"%s/phonemes\", ctx->phsrc);\n",
            "      |                           ^~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2368:9: note: ‘sprintf’ output between 10 and 209 bytes into a destination of size 200\n",
            " 2368 |         sprintf(fname, \"%s/phonemes\", ctx->phsrc);\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2376:28: warning: ‘%s’ directive writing 17 bytes into a region of size between 0 and 199 [-Wformat-overflow=]\n",
            " 2376 |         sprintf(fname, \"%s/%s\", phdst, \"phondata-manifest\");\n",
            "      |                            ^~          ~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2376:9: note: ‘sprintf’ output between 19 and 218 bytes into a destination of size 200\n",
            " 2376 |         sprintf(fname, \"%s/%s\", phdst, \"phondata-manifest\");\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2394:28: warning: ‘%s’ directive writing 8 bytes into a region of size between 0 and 199 [-Wformat-overflow=]\n",
            " 2394 |         sprintf(fname, \"%s/%s\", phdst, \"phondata\");\n",
            "      |                            ^~          ~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2394:9: note: ‘sprintf’ output between 10 and 209 bytes into a destination of size 200\n",
            " 2394 |         sprintf(fname, \"%s/%s\", phdst, \"phondata\");\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2404:28: warning: ‘%s’ directive writing 9 bytes into a region of size between 0 and 199 [-Wformat-overflow=]\n",
            " 2404 |         sprintf(fname, \"%s/%s\", phdst, \"phonindex\");\n",
            "      |                            ^~          ~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2404:9: note: ‘sprintf’ output between 11 and 210 bytes into a destination of size 200\n",
            " 2404 |         sprintf(fname, \"%s/%s\", phdst, \"phonindex\");\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2415:28: warning: ‘%s’ directive writing 7 bytes into a region of size between 0 and 199 [-Wformat-overflow=]\n",
            " 2415 |         sprintf(fname, \"%s/%s\", phdst, \"phontab\");\n",
            "      |                            ^~          ~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2415:9: note: ‘sprintf’ output between 9 and 208 bytes into a destination of size 200\n",
            " 2415 |         sprintf(fname, \"%s/%s\", phdst, \"phontab\");\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2427:27: warning: ‘/compile_prog_log’ directive writing 17 bytes into a region of size between 1 and 200 [-Wformat-overflow=]\n",
            " 2427 |         sprintf(fname, \"%s/compile_prog_log\", ctx->phsrc);\n",
            "      |                           ^~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/compiledata.c:2427:9: note: ‘sprintf’ output between 18 and 217 bytes into a destination of size 200\n",
            " 2427 |         sprintf(fname, \"%s/compile_prog_log\", ctx->phsrc);\n",
            "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "[15/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/dictionary.c.o\n",
            "[16/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/intonation.c.o\n",
            "[17/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/langopts.c.o\n",
            "[18/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/phoneme.c.o\n",
            "[19/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/phonemelist.c.o\n",
            "[20/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/readclause.c.o\n",
            "[21/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/numbers.c.o\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c: In function ‘LookupNum3’:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c:1331:60: warning: ‘%s’ directive writing up to 159 bytes into a region of size between 0 and 49 [-Wformat-overflow=]\n",
            " 1331 |                                 sprintf(ph_thousands, \"%s%c%s%c\", ph_digits, phonEND_WORD, ph_10T, phonEND_WORD);\n",
            "      |                                                            ^~                              ~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c:1331:33: note: ‘sprintf’ output between 3 and 211 bytes into a destination of size 50\n",
            " 1331 |                                 sprintf(ph_thousands, \"%s%c%s%c\", ph_digits, phonEND_WORD, ph_10T, phonEND_WORD);\n",
            "      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c:1329:56: warning: ‘%s’ directive writing up to 159 bytes into a region of size 50 [-Wformat-overflow=]\n",
            " 1329 |                                 sprintf(ph_thousands, \"%s%c%s%c\", ph_10T, phonEND_WORD, ph_digits, phonEND_WORD);\n",
            "      |                                                        ^~         ~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c:1329:33: note: ‘sprintf’ output between 3 and 211 bytes into a destination of size 50\n",
            " 1329 |                                 sprintf(ph_thousands, \"%s%c%s%c\", ph_10T, phonEND_WORD, ph_digits, phonEND_WORD);\n",
            "      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c:1395:36: warning: ‘%s’ directive writing up to 49 bytes into a region of size between 40 and 100 [-Wformat-overflow=]\n",
            " 1395 |                 sprintf(buf1, \"%s%s%s%s\", ph_thousands, ph_thousand_and, ph_digits, ph_100);\n",
            "      |                                    ^~                                    ~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c:1395:17: note: ‘sprintf’ output between 1 and 269 bytes into a destination of size 100\n",
            " 1395 |                 sprintf(buf1, \"%s%s%s%s\", ph_thousands, ph_thousand_and, ph_digits, ph_100);\n",
            "      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c: In function ‘TranslateNumber_1’:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c:1575:63: warning: ‘sprintf’ may write a terminating nul past the end of the destination [-Wformat-overflow=]\n",
            " 1575 |                                         sprintf(string, \"_x#%s\", suffix);\n",
            "      |                                                               ^\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/numbers.c:1575:41: note: ‘sprintf’ output between 4 and 33 bytes into a destination of size 32\n",
            " 1575 |                                         sprintf(string, \"_x#%s\", suffix);\n",
            "      |                                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "[22/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/setlengths.c.o\n",
            "[23/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/soundicon.c.o\n",
            "[24/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/spect.c.o\n",
            "[25/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/ssml.c.o\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/ssml.c: In function ‘ParseSsmlReference’:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/ssml.c:1015:50: warning: format ‘%x’ expects argument of type ‘unsigned int *’, but argument 3 has type ‘int *’ [-Wformat=]\n",
            " 1015 |                         return sscanf(&ref[2], \"%x\", c1);\n",
            "      |                                                 ~^   ~~\n",
            "      |                                                  |   |\n",
            "      |                                                  |   int *\n",
            "      |                                                  unsigned int *\n",
            "      |                                                 %x\n",
            "[26/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/synthdata.c.o\n",
            "[27/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/tr_languages.c.o\n",
            "[28/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/translateword.c.o\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/translateword.c: In function ‘TranslateLetter’:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/translateword.c:1020:39: warning: ‘%s’ directive writing up to 29 bytes into a region of size between 0 and 79 [-Wformat-overflow=]\n",
            " 1020 |                 sprintf(ph_buf2, \"%c%s%s%s\", 0xff, ph_alphabet, capital, ph_buf); // the 0xff marker will be removed or replaced in SetSpellingStress()\n",
            "      |                                       ^~                        ~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/translateword.c:1020:17: note: ‘sprintf’ output between 2 and 189 bytes into a destination of size 80\n",
            " 1020 |                 sprintf(ph_buf2, \"%c%s%s%s\", 0xff, ph_alphabet, capital, ph_buf); // the 0xff marker will be removed or replaced in SetSpellingStress()\n",
            "      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/translateword.c:1018:39: warning: ‘%s’ directive writing up to 79 bytes into a region of size between 0 and 79 [-Wformat-overflow=]\n",
            " 1018 |                 sprintf(ph_buf2, \"%c%s%s%s\", 0xff, ph_alphabet, ph_buf, capital);\n",
            "      |                                       ^~                        ~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/translateword.c:1018:17: note: ‘sprintf’ output between 2 and 189 bytes into a destination of size 80\n",
            " 1018 |                 sprintf(ph_buf2, \"%c%s%s%s\", 0xff, ph_alphabet, ph_buf, capital);\n",
            "      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "[29/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/synthesize.c.o\n",
            "[30/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/voices.c.o\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/voices.c: In function ‘LoadVoice’:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/voices.c:460:33: warning: ‘%s’ directive writing up to 39 bytes into a region of size between 19 and 190 [-Wformat-overflow=]\n",
            "  460 |                 sprintf(buf, \"%s%s\", path_voices, voicename); // look in the main voices directory\n",
            "      |                                 ^~                ~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/voices.c:460:17: note: ‘sprintf’ output between 1 and 211 bytes into a destination of size 190\n",
            "  460 |                 sprintf(buf, \"%s%s\", path_voices, voicename); // look in the main voices directory\n",
            "      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/voices.c:464:41: warning: ‘%s’ directive writing up to 39 bytes into a region of size between 19 and 190 [-Wformat-overflow=]\n",
            "  464 |                         sprintf(buf, \"%s%s\", path_voices, voicename); // look in the main languages directory\n",
            "      |                                         ^~                ~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/voices.c:464:25: note: ‘sprintf’ output between 1 and 211 bytes into a destination of size 190\n",
            "  464 |                         sprintf(buf, \"%s%s\", path_voices, voicename); // look in the main languages directory\n",
            "      |                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/voices.c: In function ‘SetVoiceScores’:\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/voices.c:977:41: warning: ‘%s’ directive writing up to 79 bytes into a region of size between 73 and 232 [-Wformat-overflow=]\n",
            "  977 |                 sprintf(buf, \"%s/voices/%s\", path_home, language);\n",
            "      |                                         ^~              ~~~~~~~~\n",
            "/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external/src/libespeak-ng/voices.c:977:17: note: ‘sprintf’ output between 9 and 247 bytes into a destination of size 240\n",
            "  977 |                 sprintf(buf, \"%s/voices/%s\", path_home, language);\n",
            "      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "[31/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/translate.c.o\n",
            "[32/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/espeak_api.c.o\n",
            "[33/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/wavegen.c.o\n",
            "[34/167] Building C object src/libespeak-ng/CMakeFiles/espeak-ng.dir/speech.c.o\n",
            "[35/167] Building C object src/CMakeFiles/espeak-ng-bin.dir/espeak-ng.c.o\n",
            "[36/167] Linking C static library src/libespeak-ng/libespeak-ng.a\n",
            "[37/167] Linking C executable src/espeak-ng\n",
            "[38/167] Compile intonations\n",
            "Compiled 34 intonation tunes: 0 errors.\n",
            "[39/167] Building CXX object src/speechPlayer/CMakeFiles/speechPlayer.dir/src/speechPlayer.cpp.o\n",
            "[40/167] Compile phonemes\n",
            "Compiling phoneme data: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/../phsource/phonemes\n",
            "\n",
            "Refs 4914,  Reused 3937\n",
            "Compiled phonemes: 0 errors.\n",
            "[41/167] Building CXX object src/speechPlayer/CMakeFiles/speechPlayer.dir/src/frame.cpp.o\n",
            "[42/167] Generating espeak-ng-data/af_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/af_dict'\n",
            "Using phonemetable: 'af'\n",
            "Compiling: 'af_list'\n",
            "\t1584 entries\n",
            "Compiling: 'af_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'af_rules'\n",
            "\t5202 rules, 60 groups (0)\n",
            "\n",
            "[43/167] Generating espeak-ng-data/am_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/am_dict'\n",
            "Using phonemetable: 'am'\n",
            "Compiling: 'am_list'\n",
            "\t31 entries\n",
            "Compiling: 'am_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'am_rules'\n",
            "\t345 rules, 7 groups (0)\n",
            "\n",
            "[44/167] Generating espeak-ng-data/an_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/an_dict'\n",
            "Using phonemetable: 'an'\n",
            "Compiling: 'an_list'\n",
            "\t484 entries\n",
            "Compiling: 'an_rules'\n",
            "\t184 rules, 29 groups (0)\n",
            "\n",
            "[45/167] Generating espeak-ng-data/as_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/as_dict'\n",
            "Using phonemetable: 'as'\n",
            "Compiling: 'as_list'\n",
            "\t209 entries\n",
            "Compiling: 'as_rules'\n",
            "\t146 rules, 66 groups (66)\n",
            "\n",
            "[46/167] Generating espeak-ng-data/az_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/az_dict'\n",
            "Using phonemetable: 'az'\n",
            "Compiling: 'az_list'\n",
            "\t84 entries\n",
            "Compiling: 'az_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'az_rules'\n",
            "\t58 rules, 34 groups (0)\n",
            "\n",
            "[47/167] Generating espeak-ng-data/ba_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ba_dict'\n",
            "Using phonemetable: 'ba'\n",
            "Compiling: 'ba_list'\n",
            "\t70 entries\n",
            "Compiling: 'ba_rules'\n",
            "\t52 rules, 44 groups (0)\n",
            "\n",
            "[48/167] Generating espeak-ng-data/be_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/be_dict'\n",
            "Using phonemetable: 'be'\n",
            "Compiling: 'be_list'\n",
            "\t77 entries\n",
            "Compiling: 'be_rules'\n",
            "\t83 rules, 33 groups (32)\n",
            "\n",
            "[49/167] Generating espeak-ng-data/bg_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/bg_dict'\n",
            "Using phonemetable: 'bg'\n",
            "Compiling: 'bg_listx'\n",
            "\t2790 entries\n",
            "Compiling: 'bg_list'\n",
            "\t231 entries\n",
            "Compiling: 'bg_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'bg_rules'\n",
            "\t118 rules, 31 groups (30)\n",
            "\n",
            "[50/167] Generating espeak-ng-data/bn_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/bn_dict'\n",
            "Using phonemetable: 'bn'\n",
            "Compiling: 'bn_list'\n",
            "\t380 entries\n",
            "Compiling: 'bn_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'bn_rules'\n",
            "\t168 rules, 68 groups (67)\n",
            "\n",
            "[51/167] Generating espeak-ng-data/bpy_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/bpy_dict'\n",
            "Using phonemetable: 'bpy'\n",
            "Compiling: 'bpy_list'\n",
            "\t179 entries\n",
            "Compiling: 'bpy_rules'\n",
            "\t212 rules, 63 groups (63)\n",
            "\n",
            "[52/167] Generating espeak-ng-data/bs_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/bs_dict'\n",
            "Using phonemetable: 'hr'\n",
            "Compiling: 'bs_list'\n",
            "\t613 entries\n",
            "Compiling: 'bs_emoji'\n",
            "\t1635 entries\n",
            "Compiling: 'bs_rules'\n",
            "\t112 rules, 34 groups (0)\n",
            "\n",
            "[53/167] Building CXX object src/speechPlayer/CMakeFiles/speechPlayer.dir/src/speechWaveGenerator.cpp.o\n",
            "[54/167] Generating espeak-ng-data/ca_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ca_dict'\n",
            "Using phonemetable: 'ca'\n",
            "Compiling: 'ca_list'\n",
            "\t18455 entries\n",
            "Compiling: 'ca_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ca_rules'\n",
            "\t965 rules, 32 groups (0)\n",
            "\n",
            "[55/167] Generating espeak-ng-data/chr_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/chr_dict'\n",
            "Using phonemetable: 'chr'\n",
            "Compiling: 'chr_list'\n",
            "\t0 entries\n",
            "Compiling: 'chr_rules'\n",
            "\t198 rules, 27 groups (0)\n",
            "\n",
            "[56/167] Generating espeak-ng-data/cs_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/cs_dict'\n",
            "Using phonemetable: 'cs'\n",
            "Compiling: 'cs_list'\n",
            "\t357 entries\n",
            "Compiling: 'cs_emoji'\n",
            "\t1689 entries\n",
            "Compiling: 'cs_rules'\n",
            "\t506 rules, 48 groups (0)\n",
            "\n",
            "[57/167] Generating espeak-ng-data/cv_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/cv_dict'\n",
            "Using phonemetable: 'cv'\n",
            "Compiling: 'cv_list'\n",
            "\t0 entries\n",
            "Compiling: 'cv_rules'\n",
            "\t39 rules, 37 groups (0)\n",
            "\n",
            "[58/167] Generating espeak-ng-data/cy_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/cy_dict'\n",
            "Using phonemetable: 'cy'\n",
            "Compiling: 'cy_list'\n",
            "\t166 entries\n",
            "Compiling: 'cy_emoji'\n",
            "\t1636 entries\n",
            "Compiling: 'cy_rules'\n",
            "\t210 rules, 27 groups (0)\n",
            "\n",
            "[59/167] Generating espeak-ng-data/cmn_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/cmn_dict'\n",
            "Using phonemetable: 'cmn'\n",
            "Compiling: 'cmn_list'\n",
            "\t3860 entries\n",
            "Compiling: 'cmn_listx'\n",
            "\t77860 entries\n",
            "Compiling: 'cmn_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'cmn_rules'\n",
            "\t136 rules, 28 groups (0)\n",
            "\n",
            "[60/167] Generating espeak-ng-data/de_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/de_dict'\n",
            "Using phonemetable: 'de'\n",
            "Compiling: 'de_list'\n",
            "\t998 entries\n",
            "Compiling: 'de_emoji'\n",
            "\t1688 entries\n",
            "Compiling: 'de_rules'\n",
            "\t1330 rules, 34 groups (0)\n",
            "\n",
            "[61/167] Generating espeak-ng-data/da_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/da_dict'\n",
            "Using phonemetable: 'da'\n",
            "Compiling: 'da_list'\n",
            "\t11152 entries\n",
            "Compiling: 'da_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'da_rules'\n",
            "\t9269 rules, 56 groups (0)\n",
            "\n",
            "[62/167] Generating espeak-ng-data/ar_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ar_dict'\n",
            "Using phonemetable: 'ar'\n",
            "Compiling: 'ar_listx'\n",
            "\t30089 entries\n",
            "Compiling: 'ar_list'\n",
            "\t252 entries\n",
            "Compiling: 'ar_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ar_rules'\n",
            "\t383 rules, 39 groups (37)\n",
            "\n",
            "[63/167] Generating espeak-ng-data/el_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/el_dict'\n",
            "Using phonemetable: 'el'\n",
            "Compiling: 'el_list'\n",
            "\t379 entries\n",
            "Compiling: 'el_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'el_rules'\n",
            "\t182 rules, 27 groups (26)\n",
            "\n",
            "[64/167] Generating espeak-ng-data/eo_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/eo_dict'\n",
            "Using phonemetable: 'eo'\n",
            "Compiling: 'eo_list'\n",
            "\t242 entries\n",
            "Compiling: 'eo_rules'\n",
            "\t130 rules, 27 groups (0)\n",
            "\n",
            "[65/167] Generating espeak-ng-data/es_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/es_dict'\n",
            "Using phonemetable: 'es'\n",
            "Compiling: 'es_list'\n",
            "\t371 entries\n",
            "Compiling: 'es_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'es_rules'\n",
            "\t192 rules, 29 groups (0)\n",
            "\n",
            "[66/167] Generating espeak-ng-data/et_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/et_dict'\n",
            "Using phonemetable: 'et'\n",
            "Compiling: 'et_list'\n",
            "\t303 entries\n",
            "Compiling: 'et_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'et_rules'\n",
            "\t221 rules, 31 groups (0)\n",
            "\n",
            "[67/167] Generating espeak-ng-data/eu_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/eu_dict'\n",
            "Using phonemetable: 'eu'\n",
            "Compiling: 'eu_list'\n",
            "\t194 entries\n",
            "Compiling: 'eu_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'eu_rules'\n",
            "\t159 rules, 27 groups (0)\n",
            "\n",
            "[68/167] Generating espeak-ng-data/fi_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/fi_dict'\n",
            "Using phonemetable: 'fi'\n",
            "Compiling: 'fi_list'\n",
            "\t332 entries\n",
            "Compiling: 'fi_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'fi_rules'\n",
            "\t144 rules, 29 groups (0)\n",
            "\n",
            "[69/167] Generating espeak-ng-data/en_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/en_dict'\n",
            "Using phonemetable: 'en'\n",
            "Compiling: 'en_list'\n",
            "\t5560 entries\n",
            "Compiling: 'en_emoji'\n",
            "\t1690 entries\n",
            "Compiling: 'en_rules'\n",
            "\t6793 rules, 104 groups (0)\n",
            "\n",
            "[70/167] Generating espeak-ng-data/ga_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ga_dict'\n",
            "Using phonemetable: 'ga'\n",
            "Compiling: 'ga_list'\n",
            "\t250 entries\n",
            "Compiling: 'ga_emoji'\n",
            "\t1636 entries\n",
            "Compiling: 'ga_rules'\n",
            "\t481 rules, 34 groups (0)\n",
            "\n",
            "[71/167] Generating espeak-ng-data/fr_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/fr_dict'\n",
            "Using phonemetable: 'fr'\n",
            "Compiling: 'fr_list'\n",
            "\t885 entries\n",
            "Compiling: 'fr_emoji'\n",
            "\t1638 entries\n",
            "Compiling: 'fr_rules'\n",
            "\t1207 rules, 33 groups (0)\n",
            "\n",
            "[72/167] Generating espeak-ng-data/gd_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/gd_dict'\n",
            "Using phonemetable: 'gd'\n",
            "Compiling: 'gd_list'\n",
            "\t104 entries\n",
            "Compiling: 'gd_emoji'\n",
            "\t1636 entries\n",
            "Compiling: 'gd_rules'\n",
            "\t252 rules, 35 groups (0)\n",
            "\n",
            "[73/167] Generating espeak-ng-data/gn_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/gn_dict'\n",
            "Using phonemetable: 'gn'\n",
            "Compiling: 'gn_list'\n",
            "\t150 entries\n",
            "Compiling: 'gn_rules'\n",
            "\t58 rules, 48 groups (0)\n",
            "\n",
            "[74/167] Generating espeak-ng-data/grc_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/grc_dict'\n",
            "Using phonemetable: 'grc'\n",
            "Compiling: 'grc_list'\n",
            "\t22 entries\n",
            "Compiling: 'grc_rules'\n",
            "\t136 rules, 55 groups (27)\n",
            "\n",
            "[75/167] Generating espeak-ng-data/gu_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/gu_dict'\n",
            "Using phonemetable: 'gu'\n",
            "Compiling: 'gu_list'\n",
            "\t174 entries\n",
            "Compiling: 'gu_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'gu_rules'\n",
            "\t269 rules, 79 groups (77)\n",
            "\n",
            "[76/167] Generating espeak-ng-data/hak_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/hak_dict'\n",
            "Using phonemetable: 'hak'\n",
            "Compiling: 'hak_list'\n",
            "\t22 entries\n",
            "Compiling: 'hak_rules'\n",
            "\t261 rules, 27 groups (0)\n",
            "\n",
            "[77/167] Generating espeak-ng-data/haw_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/haw_dict'\n",
            "Using phonemetable: 'haw'\n",
            "Compiling: 'haw_list'\n",
            "\t100 entries\n",
            "Compiling: 'haw_rules'\n",
            "\t40 rules, 30 groups (0)\n",
            "\n",
            "[78/167] Generating espeak-ng-data/he_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/he_dict'\n",
            "Using phonemetable: 'he'\n",
            "Compiling: 'he_listx'\n",
            "\t197 entries\n",
            "Compiling: 'he_list'\n",
            "\t93 entries\n",
            "Compiling: 'he_rules'\n",
            "\t723 rules, 65 groups (0)\n",
            "\n",
            "[79/167] Generating espeak-ng-data/fa_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/fa_dict'\n",
            "Using phonemetable: 'fa'\n",
            "Compiling: 'fa_list'\n",
            "\t13233 entries\n",
            "Compiling: 'fa_emoji'\n",
            "\t1649 entries\n",
            "Compiling: 'fa_rules'\n",
            "\t3922 rules, 93 groups (45)\n",
            "\n",
            "[80/167] Generating espeak-ng-data/hr_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/hr_dict'\n",
            "Using phonemetable: 'hr'\n",
            "Compiling: 'hr_list'\n",
            "\t613 entries\n",
            "Compiling: 'hr_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'hr_rules'\n",
            "\t112 rules, 34 groups (0)\n",
            "\n",
            "[81/167] Generating espeak-ng-data/hi_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/hi_dict'\n",
            "Using phonemetable: 'hi'\n",
            "Compiling: 'hi_list'\n",
            "\t316 entries\n",
            "Compiling: 'hi_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'hi_rules'\n",
            "\t301 rules, 87 groups (86)\n",
            "\n",
            "[82/167] Generating espeak-ng-data/ht_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ht_dict'\n",
            "Using phonemetable: 'ht'\n",
            "Compiling: 'ht_list'\n",
            "\t50 entries\n",
            "Compiling: 'ht_rules'\n",
            "\t36 rules, 32 groups (0)\n",
            "\n",
            "[83/167] Generating espeak-ng-data/hy_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/hy_dict'\n",
            "Using phonemetable: 'hy'\n",
            "Compiling: 'hy_list'\n",
            "\t170 entries\n",
            "Compiling: 'hy_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'hy_rules'\n",
            "\t96 rules, 38 groups (38)\n",
            "\n",
            "[84/167] Generating espeak-ng-data/id_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/id_dict'\n",
            "Using phonemetable: 'id'\n",
            "Compiling: 'id_list'\n",
            "\t132 entries\n",
            "Compiling: 'id_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'id_rules'\n",
            "\t78 rules, 27 groups (0)\n",
            "\n",
            "[85/167] Generating espeak-ng-data/hu_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/hu_dict'\n",
            "Using phonemetable: 'hu'\n",
            "Compiling: 'hu_list'\n",
            "\t5352 entries\n",
            "Compiling: 'hu_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'hu_rules'\n",
            "\t5309 rules, 48 groups (0)\n",
            "\n",
            "[86/167] Generating espeak-ng-data/io_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/io_dict'\n",
            "Using phonemetable: 'eo'\n",
            "Compiling: 'io_list'\n",
            "\t50 entries\n",
            "Compiling: 'io_rules'\n",
            "\t78 rules, 26 groups (0)\n",
            "\n",
            "[87/167] Generating espeak-ng-data/is_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/is_dict'\n",
            "Using phonemetable: 'is'\n",
            "Compiling: 'is_list'\n",
            "\t327 entries\n",
            "Compiling: 'is_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'is_rules'\n",
            "\t205 rules, 37 groups (0)\n",
            "\n",
            "[88/167] Generating espeak-ng-data/ja_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ja_dict'\n",
            "Using phonemetable: 'ja'\n",
            "Compiling: 'ja_list'\n",
            "\t23 entries\n",
            "Compiling: 'ja_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ja_rules'\n",
            "\t457 rules, 53 groups (0)\n",
            "\n",
            "[89/167] Generating espeak-ng-data/jbo_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/jbo_dict'\n",
            "Using phonemetable: 'jbo'\n",
            "Compiling: 'jbo_list'\n",
            "\t85 entries\n",
            "Compiling: 'jbo_rules'\n",
            "\t64 rules, 27 groups (0)\n",
            "\n",
            "[90/167] Generating espeak-ng-data/it_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/it_dict'\n",
            "Using phonemetable: 'it'\n",
            "Compiling: 'it_listx'\n",
            "\t4309 entries\n",
            "Compiling: 'it_list'\n",
            "\t5230 entries\n",
            "Compiling: 'it_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'it_rules'\n",
            "\t714 rules, 30 groups (0)\n",
            "\n",
            "[91/167] Generating espeak-ng-data/ka_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ka_dict'\n",
            "Using phonemetable: 'ka'\n",
            "Compiling: 'ka_list'\n",
            "\t148 entries\n",
            "Compiling: 'ka_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ka_rules'\n",
            "\t50 rules, 44 groups (43)\n",
            "\n",
            "[92/167] Generating espeak-ng-data/kk_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/kk_dict'\n",
            "Using phonemetable: 'kk'\n",
            "Compiling: 'kk_list'\n",
            "\t51 entries\n",
            "Compiling: 'kk_emoji'\n",
            "\t0 entries\n",
            "Compiling: 'kk_rules'\n",
            "\t42 rules, 42 groups (36)\n",
            "\n",
            "[93/167] Generating espeak-ng-data/kl_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/kl_dict'\n",
            "Using phonemetable: 'kl'\n",
            "Compiling: 'kl_list'\n",
            "\t102 entries\n",
            "Compiling: 'kl_rules'\n",
            "\t80 rules, 30 groups (0)\n",
            "\n",
            "[94/167] Generating espeak-ng-data/kn_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/kn_dict'\n",
            "Using phonemetable: 'kn'\n",
            "Compiling: 'kn_list'\n",
            "\t262 entries\n",
            "Compiling: 'kn_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'kn_rules'\n",
            "\t115 rules, 55 groups (50)\n",
            "\n",
            "[95/167] Generating espeak-ng-data/ko_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ko_dict'\n",
            "Using phonemetable: 'ko'\n",
            "Compiling: 'ko_list'\n",
            "\t133 entries\n",
            "Compiling: 'ko_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ko_rules'\n",
            "\t128 rules, 68 groups (40)\n",
            "\n",
            "[96/167] Generating espeak-ng-data/ia_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ia_dict'\n",
            "Using phonemetable: 'ia'\n",
            "Compiling: 'ia_listx'\n",
            "\t15620 entries\n",
            "Compiling: 'ia_list'\n",
            "\t90 entries\n",
            "Compiling: 'ia_rules'\n",
            "\t79 rules, 26 groups (0)\n",
            "\n",
            "[97/167] Generating espeak-ng-data/ku_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ku_dict'\n",
            "Using phonemetable: 'ku'\n",
            "Compiling: 'ku_list'\n",
            "\t106 entries\n",
            "Compiling: 'ku_rules'\n",
            "\t46 rules, 32 groups (0)\n",
            "\n",
            "[98/167] Generating espeak-ng-data/kok_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/kok_dict'\n",
            "Using phonemetable: 'kok'\n",
            "Compiling: 'kok_list'\n",
            "\t189 entries\n",
            "Compiling: 'kok_rules'\n",
            "\t337 rules, 88 groups (0)\n",
            "\n",
            "[99/167] Generating espeak-ng-data/ky_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ky_dict'\n",
            "Using phonemetable: 'ky'\n",
            "Compiling: 'ky_list'\n",
            "\t131 entries\n",
            "Compiling: 'ky_emoji'\n",
            "\t1636 entries\n",
            "Compiling: 'ky_rules'\n",
            "\t113 rules, 33 groups (0)\n",
            "\n",
            "[100/167] Generating espeak-ng-data/la_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/la_dict'\n",
            "Using phonemetable: 'la'\n",
            "Compiling: 'la_list'\n",
            "\t215 entries\n",
            "Compiling: 'la_rules'\n",
            "\t113 rules, 31 groups (0)\n",
            "\n",
            "[101/167] Generating espeak-ng-data/lfn_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/lfn_dict'\n",
            "Using phonemetable: 'base2'\n",
            "Compiling: 'lfn_list'\n",
            "\t127 entries\n",
            "Compiling: 'lfn_rules'\n",
            "\t76 rules, 27 groups (0)\n",
            "\n",
            "[102/167] Generating espeak-ng-data/lt_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/lt_dict'\n",
            "Using phonemetable: 'lt'\n",
            "Compiling: 'lt_list'\n",
            "\t167 entries\n",
            "Compiling: 'lt_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'lt_rules'\n",
            "\t210 rules, 38 groups (0)\n",
            "\n",
            "[103/167] Generating espeak-ng-data/mi_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/mi_dict'\n",
            "Using phonemetable: 'mi'\n",
            "Compiling: 'mi_list'\n",
            "\t14 entries\n",
            "Compiling: 'mi_rules'\n",
            "\t27 rules, 18 groups (0)\n",
            "\n",
            "[104/167] Generating espeak-ng-data/lv_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/lv_dict'\n",
            "Using phonemetable: 'lv'\n",
            "Compiling: 'lv_list'\n",
            "\t887 entries\n",
            "Compiling: 'lv_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'lv_rules'\n",
            "\t1523 rules, 62 groups (0)\n",
            "\n",
            "[105/167] Generating espeak-ng-data/mk_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/mk_dict'\n",
            "Using phonemetable: 'mk'\n",
            "Compiling: 'mk_list'\n",
            "\t188 entries\n",
            "Compiling: 'mk_emoji'\n",
            "\t1636 entries\n",
            "Compiling: 'mk_rules'\n",
            "\t96 rules, 34 groups (31)\n",
            "\n",
            "[106/167] Generating espeak-ng-data/ml_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ml_dict'\n",
            "Using phonemetable: 'ml'\n",
            "Compiling: 'ml_list'\n",
            "\t165 entries\n",
            "Compiling: 'ml_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ml_rules'\n",
            "\t141 rules, 45 groups (42)\n",
            "\n",
            "[107/167] Generating espeak-ng-data/mr_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/mr_dict'\n",
            "Using phonemetable: 'mr'\n",
            "Compiling: 'mr_list'\n",
            "\t235 entries\n",
            "Compiling: 'mr_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'mr_rules'\n",
            "\t298 rules, 87 groups (85)\n",
            "\n",
            "[108/167] Generating espeak-ng-data/mt_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/mt_dict'\n",
            "Using phonemetable: 'mt'\n",
            "Compiling: 'mt_list'\n",
            "\t232 entries\n",
            "Compiling: 'mt_rules'\n",
            "\t99 rules, 31 groups (0)\n",
            "\n",
            "[109/167] Generating espeak-ng-data/ms_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ms_dict'\n",
            "Using phonemetable: 'id'\n",
            "Compiling: 'ms_list'\n",
            "\t703 entries\n",
            "Compiling: 'ms_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ms_rules'\n",
            "\t125 rules, 27 groups (0)\n",
            "\n",
            "[110/167] Generating espeak-ng-data/mto_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/mto_dict'\n",
            "Using phonemetable: 'mto'\n",
            "Compiling: 'mto_list'\n",
            "\t185 entries\n",
            "Compiling: 'mto_rules'\n",
            "\t56 rules, 23 groups (0)\n",
            "\n",
            "[111/167] Generating espeak-ng-data/my_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/my_dict'\n",
            "Using phonemetable: 'my'\n",
            "Compiling: 'my_list'\n",
            "\t11 entries\n",
            "Compiling: 'my_emoji'\n",
            "\t1644 entries\n",
            "Compiling: 'my_rules'\n",
            "\t78 rules, 64 groups (0)\n",
            "\n",
            "[112/167] Generating espeak-ng-data/nci_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/nci_dict'\n",
            "Using phonemetable: 'nci'\n",
            "Compiling: 'nci_list'\n",
            "\t16 entries\n",
            "Compiling: 'nci_rules'\n",
            "\t41 rules, 21 groups (0)\n",
            "\n",
            "[113/167] Generating espeak-ng-data/ne_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ne_dict'\n",
            "Using phonemetable: 'ne'\n",
            "Compiling: 'ne_list'\n",
            "\t193 entries\n",
            "Compiling: 'ne_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ne_rules'\n",
            "\t520 rules, 100 groups (92)\n",
            "\n",
            "[114/167] Generating espeak-ng-data/nl_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/nl_dict'\n",
            "Using phonemetable: 'nl'\n",
            "Compiling: 'nl_list'\n",
            "\t1797 entries\n",
            "Compiling: 'nl_emoji'\n",
            "\t1690 entries\n",
            "Compiling: 'nl_rules'\n",
            "\t826 rules, 37 groups (0)\n",
            "\n",
            "[115/167] Generating espeak-ng-data/no_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/no_dict'\n",
            "Using phonemetable: 'no'\n",
            "Compiling: 'no_list'\n",
            "\t236 entries\n",
            "Compiling: 'no_rules'\n",
            "\t142 rules, 32 groups (0)\n",
            "\n",
            "[116/167] Generating espeak-ng-data/nog_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/nog_dict'\n",
            "Using phonemetable: 'nog'\n",
            "Compiling: 'nog_list'\n",
            "\t37 entries\n",
            "Compiling: 'nog_rules'\n",
            "\t180 rules, 33 groups (0)\n",
            "\n",
            "[117/167] Generating espeak-ng-data/om_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/om_dict'\n",
            "Using phonemetable: 'om'\n",
            "Compiling: 'om_list'\n",
            "\t87 entries\n",
            "Compiling: 'om_rules'\n",
            "\t53 rules, 33 groups (0)\n",
            "\n",
            "[118/167] Generating espeak-ng-data/or_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/or_dict'\n",
            "Using phonemetable: 'or'\n",
            "Compiling: 'or_list'\n",
            "\t198 entries\n",
            "Compiling: 'or_emoji'\n",
            "\t1634 entries\n",
            "Compiling: 'or_rules'\n",
            "\t197 rules, 67 groups (66)\n",
            "\n",
            "[119/167] Generating espeak-ng-data/pa_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/pa_dict'\n",
            "Using phonemetable: 'pa'\n",
            "Compiling: 'pa_list'\n",
            "\t196 entries\n",
            "Compiling: 'pa_emoji'\n",
            "\t1636 entries\n",
            "Compiling: 'pa_rules'\n",
            "\t283 rules, 68 groups (65)\n",
            "\n",
            "[120/167] Generating espeak-ng-data/pap_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/pap_dict'\n",
            "Using phonemetable: 'base2'\n",
            "Compiling: 'pap_list'\n",
            "\t86 entries\n",
            "Compiling: 'pap_rules'\n",
            "\t68 rules, 32 groups (0)\n",
            "\n",
            "[121/167] Generating espeak-ng-data/piqd_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/piqd_dict'\n",
            "Using phonemetable: 'piqd'\n",
            "Compiling: 'piqd_list'\n",
            "\t36 entries\n",
            "Compiling: 'piqd_rules'\n",
            "\t35 rules, 23 groups (0)\n",
            "\n",
            "[122/167] Generating espeak-ng-data/pl_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/pl_dict'\n",
            "Using phonemetable: 'pl'\n",
            "Compiling: 'pl_list'\n",
            "\t2956 entries\n",
            "Compiling: 'pl_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'pl_rules'\n",
            "\t869 rules, 46 groups (0)\n",
            "\n",
            "[123/167] Generating espeak-ng-data/pt_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/pt_dict'\n",
            "Using phonemetable: 'pt-pt'\n",
            "Compiling: 'pt_list'\n",
            "\t2006 entries\n",
            "Compiling: 'pt_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'pt_rules'\n",
            "\t1105 rules, 39 groups (0)\n",
            "\n",
            "[124/167] Generating espeak-ng-data/py_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/py_dict'\n",
            "Using phonemetable: 'py'\n",
            "Compiling: 'py_list'\n",
            "\t106 entries\n",
            "Compiling: 'py_rules'\n",
            "\t31 rules, 28 groups (0)\n",
            "\n",
            "[125/167] Generating espeak-ng-data/qdb_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/qdb_dict'\n",
            "Using phonemetable: 'qdb'\n",
            "Compiling: 'qdb_list'\n",
            "\t169 entries\n",
            "Compiling: 'qdb_rules'\n",
            "\t38 rules, 25 groups (0)\n",
            "\n",
            "[126/167] Generating espeak-ng-data/qu_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/qu_dict'\n",
            "Using phonemetable: 'qu'\n",
            "Compiling: 'qu_list'\n",
            "\t76 entries\n",
            "Compiling: 'qu_rules'\n",
            "\t39 rules, 28 groups (0)\n",
            "\n",
            "[127/167] Generating espeak-ng-data/quc_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/quc_dict'\n",
            "Using phonemetable: 'quc'\n",
            "Compiling: 'quc_list'\n",
            "\t10 entries\n",
            "Compiling: 'quc_emoji'\n",
            "\t0 entries\n",
            "Compiling: 'quc_rules'\n",
            "\t35 rules, 27 groups (0)\n",
            "\n",
            "[128/167] Generating espeak-ng-data/qya_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/qya_dict'\n",
            "Using phonemetable: 'qya'\n",
            "Compiling: 'qya_list'\n",
            "\t38 entries\n",
            "Compiling: 'qya_rules'\n",
            "\t70 rules, 38 groups (0)\n",
            "\n",
            "[129/167] Generating espeak-ng-data/ro_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ro_dict'\n",
            "Using phonemetable: 'ro'\n",
            "Compiling: 'ro_list'\n",
            "\t2167 entries\n",
            "Compiling: 'ro_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ro_rules'\n",
            "\t349 rules, 32 groups (0)\n",
            "\n",
            "[130/167] Generating espeak-ng-data/sd_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/sd_dict'\n",
            "Using phonemetable: 'sd'\n",
            "Compiling: 'sd_list'\n",
            "\t266 entries\n",
            "Compiling: 'sd_emoji'\n",
            "\t1565 entries\n",
            "Compiling: 'sd_rules'\n",
            "\t372 rules, 73 groups (47)\n",
            "\n",
            "[131/167] Generating espeak-ng-data/shn_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/shn_dict'\n",
            "Using phonemetable: 'shn'\n",
            "Compiling: 'shn_list'\n",
            "\t4175 entries\n",
            "Compiling: 'shn_rules'\n",
            "\t229 rules, 53 groups (0)\n",
            "\n",
            "[132/167] Generating espeak-ng-data/lb_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/lb_dict'\n",
            "Using phonemetable: 'lb'\n",
            "Compiling: 'lb_list'\n",
            "\t35646 entries\n",
            "Compiling: 'lb_emoji'\n",
            "\t25 entries\n",
            "Compiling: 'lb_rules'\n",
            "\t175 rules, 50 groups (0)\n",
            "\n",
            "[133/167] Generating espeak-ng-data/si_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/si_dict'\n",
            "Using phonemetable: 'si'\n",
            "Compiling: 'si_list'\n",
            "\t152 entries\n",
            "Compiling: 'si_emoji'\n",
            "\t1648 entries\n",
            "Compiling: 'si_rules'\n",
            "\t142 rules, 75 groups (73)\n",
            "\n",
            "[134/167] Generating espeak-ng-data/sjn_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/sjn_dict'\n",
            "Using phonemetable: 'sjn'\n",
            "Compiling: 'sjn_list'\n",
            "\t21 entries\n",
            "Compiling: 'sjn_rules'\n",
            "\t71 rules, 40 groups (0)\n",
            "\n",
            "[135/167] Generating espeak-ng-data/sk_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/sk_dict'\n",
            "Using phonemetable: 'sk'\n",
            "Compiling: 'sk_list'\n",
            "\t348 entries\n",
            "Compiling: 'sk_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'sk_rules'\n",
            "\t535 rules, 46 groups (0)\n",
            "\n",
            "[136/167] Generating espeak-ng-data/sl_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/sl_dict'\n",
            "Using phonemetable: 'sl'\n",
            "Compiling: 'sl_list'\n",
            "\t196 entries\n",
            "Compiling: 'sl_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'sl_rules'\n",
            "\t108 rules, 31 groups (0)\n",
            "\n",
            "[137/167] Generating espeak-ng-data/smj_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/smj_dict'\n",
            "Using phonemetable: 'smj'\n",
            "Compiling: 'smj_list'\n",
            "\t2816 entries\n",
            "Compiling: 'smj_rules'\n",
            "\t136 rules, 34 groups (0)\n",
            "\n",
            "[138/167] Generating espeak-ng-data/sq_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/sq_dict'\n",
            "Using phonemetable: 'sq'\n",
            "Compiling: 'sq_list'\n",
            "\t132 entries\n",
            "Compiling: 'sq_emoji'\n",
            "\t1636 entries\n",
            "Compiling: 'sq_rules'\n",
            "\t100 rules, 29 groups (0)\n",
            "\n",
            "[139/167] Generating espeak-ng-data/sr_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/sr_dict'\n",
            "Using phonemetable: 'sr'\n",
            "Compiling: 'sr_list'\n",
            "\t613 entries\n",
            "Compiling: 'sr_emoji'\n",
            "\t1568 entries\n",
            "Compiling: 'sr_rules'\n",
            "\t112 rules, 34 groups (0)\n",
            "\n",
            "[140/167] Generating espeak-ng-data/sv_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/sv_dict'\n",
            "Using phonemetable: 'sv'\n",
            "Compiling: 'sv_list'\n",
            "\t341 entries\n",
            "Compiling: 'sv_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'sv_rules'\n",
            "\t697 rules, 30 groups (0)\n",
            "\n",
            "[141/167] Generating espeak-ng-data/sw_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/sw_dict'\n",
            "Using phonemetable: 'sw'\n",
            "Compiling: 'sw_list'\n",
            "\t170 entries\n",
            "Compiling: 'sw_emoji'\n",
            "\t1638 entries\n",
            "Compiling: 'sw_rules'\n",
            "\t63 rules, 27 groups (0)\n",
            "\n",
            "[142/167] Generating espeak-ng-data/te_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/te_dict'\n",
            "Using phonemetable: 'te'\n",
            "Compiling: 'te_list'\n",
            "\t155 entries\n",
            "Compiling: 'te_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'te_rules'\n",
            "\t112 rules, 56 groups (53)\n",
            "\n",
            "[143/167] Generating espeak-ng-data/th_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/th_dict'\n",
            "Using phonemetable: 'th'\n",
            "Compiling: 'th_list'\n",
            "\t10 entries\n",
            "Compiling: 'th_rules'\n",
            "\t139 rules, 64 groups (0)\n",
            "\n",
            "[144/167] Generating espeak-ng-data/ti_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ti_dict'\n",
            "Using phonemetable: 'ti'\n",
            "Compiling: 'ti_list'\n",
            "\t31 entries\n",
            "Compiling: 'ti_emoji'\n",
            "\t1690 entries\n",
            "Compiling: 'ti_rules'\n",
            "\t361 rules, 7 groups (0)\n",
            "\n",
            "[145/167] Generating espeak-ng-data/ta_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ta_dict'\n",
            "Using phonemetable: 'ta'\n",
            "Compiling: 'ta_list'\n",
            "\t551 entries\n",
            "Compiling: 'ta_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ta_rules'\n",
            "\t5040 rules, 33 groups (28)\n",
            "\n",
            "[146/167] Generating espeak-ng-data/tk_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/tk_dict'\n",
            "Using phonemetable: 'tk'\n",
            "Compiling: 'tk_listx'\n",
            "\t868 entries\n",
            "Compiling: 'tk_list'\n",
            "\t62 entries\n",
            "Compiling: 'tk_rules'\n",
            "\t318 rules, 30 groups (0)\n",
            "\n",
            "[147/167] Generating espeak-ng-data/tn_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/tn_dict'\n",
            "Using phonemetable: 'tn'\n",
            "Compiling: 'tn_list'\n",
            "\t110 entries\n",
            "Compiling: 'tn_rules'\n",
            "\t82 rules, 27 groups (0)\n",
            "\n",
            "[148/167] Generating espeak-ng-data/tr_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/tr_dict'\n",
            "Using phonemetable: 'tr'\n",
            "Compiling: 'tr_listx'\n",
            "\t147 entries\n",
            "Compiling: 'tr_list'\n",
            "\t175 entries\n",
            "Compiling: 'tr_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'tr_rules'\n",
            "\t215 rules, 37 groups (0)\n",
            "\n",
            "[149/167] Generating espeak-ng-data/tt_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/tt_dict'\n",
            "Using phonemetable: 'tt'\n",
            "Compiling: 'tt_list'\n",
            "\t74 entries\n",
            "Compiling: 'tt_rules'\n",
            "\t49 rules, 40 groups (34)\n",
            "\n",
            "[150/167] Generating espeak-ng-data/ug_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ug_dict'\n",
            "Using phonemetable: 'ug'\n",
            "Compiling: 'ug_list'\n",
            "\t37 entries\n",
            "Compiling: 'ug_rules'\n",
            "\t81 rules, 61 groups (0)\n",
            "\n",
            "[151/167] Generating espeak-ng-data/uk_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/uk_dict'\n",
            "Using phonemetable: 'uk'\n",
            "Compiling: 'uk_list'\n",
            "\t167 entries\n",
            "Compiling: 'uk_rules'\n",
            "\t47 rules, 34 groups (33)\n",
            "\n",
            "[152/167] Generating espeak-ng-data/uz_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/uz_dict'\n",
            "Using phonemetable: 'uz'\n",
            "Compiling: 'uz_list'\n",
            "\t122 entries\n",
            "Compiling: 'uz_rules'\n",
            "\t35 rules, 26 groups (0)\n",
            "\n",
            "[153/167] Generating espeak-ng-data/vi_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/vi_dict'\n",
            "Using phonemetable: 'vi'\n",
            "Compiling: 'vi_list'\n",
            "\t135 entries\n",
            "Compiling: 'vi_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'vi_rules'\n",
            "\t592 rules, 97 groups (0)\n",
            "\n",
            "[154/167] Generating espeak-ng-data/ur_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ur_dict'\n",
            "Using phonemetable: 'ur'\n",
            "Compiling: 'ur_list'\n",
            "\t3028 entries\n",
            "Compiling: 'ur_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ur_rules'\n",
            "\t1488 rules, 62 groups (44)\n",
            "\n",
            "[155/167] Building C object CMakeFiles/sonic.dir/_deps/sonic-git-src/sonic.c.o\n",
            "[156/167] Link espeak-ng to compat names\n",
            "[157/167] Linking CXX static library src/speechPlayer/libspeechPlayer.a\n",
            "[158/167] Generating espeak-ng-data/yue_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/yue_dict'\n",
            "Using phonemetable: 'yue'\n",
            "Compiling: 'yue_list'\n",
            "\t3822 entries\n",
            "Compiling: 'yue_listx'\n",
            "\t33813 entries\n",
            "Compiling: 'yue_emoji'\n",
            "\t1635 entries\n",
            "Compiling: 'yue_rules'\n",
            "\t79 rules, 27 groups (0)\n",
            "\n",
            "[159/167] Generating espeak-ng-data/ru_dict\n",
            "Can't read dictionary file: '/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng/src/espeak_ng_external-build/espeak-ng-data/ru_dict'\n",
            "Using phonemetable: 'ru'\n",
            "Compiling: 'ru_listx'\n",
            "\t811332 entries\n",
            "Compiling: 'ru_list'\n",
            "\t287 entries\n",
            "Compiling: 'ru_emoji'\n",
            "\t1639 entries\n",
            "Compiling: 'ru_rules'\n",
            "\t437 rules, 34 groups (33)\n",
            "\n",
            "[160/167] Building C object tests/CMakeFiles/test_ieee80.dir/ieee80.c.o\n",
            "[161/167] Building C object tests/CMakeFiles/test_api.dir/api.c.o\n",
            "[162/167] Linking C executable tests/test_api\n",
            "[163/167] Building C object tests/CMakeFiles/test_readclause.dir/readclause.c.o\n",
            "[164/167] Linking C executable tests/test_ieee80\n",
            "[165/167] Linking C executable tests/test_readclause\n",
            "[166/167] Building C object tests/CMakeFiles/test_encoding.dir/encoding.c.o\n",
            "[167/167] Linking C executable tests/test_encoding\n",
            "[7/12] Performing install step for 'espeak_ng_external'\u001b[K\n",
            "[1/2] Link espeak-ng to compat names\n",
            "[1/2] Install the project...\n",
            "-- Install configuration: \"\"\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/qu_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/my_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/mr_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/grc_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/kok_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/el_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/nog_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/be_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/is_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ml_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/sd_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/bs_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/sl_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/la_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ko_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/sjn_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/eo_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/si_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/shn_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ku_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/bg_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/pap_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/th_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/mi_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/hak_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/eu_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/fi_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lt_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/es_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/phondata\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ti_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/hr_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/mt_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/john\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robosoft2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/m3\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/boris\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/m4\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Annie\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/max\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Alex\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Mike\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Lee\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/aunty\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/linda\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robert\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robosoft8\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/steph2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/paul\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/whisperf\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robosoft6\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/miguel\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/m2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/f2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/kaukovalta\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/mike2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Andrea\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/benjamin\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/f4\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Storm\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/marcelo\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/edward\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robosoft4\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Diogo\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/announcer\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/ed\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Gene2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/RicishayMax3\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/klatt6\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robosoft\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Tweaky\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/m5\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/rob\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/klatt\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/norbert\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/UniRobot\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/victor\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Hugo\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Demonic\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/whisper\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Alicia\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/antonio\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Denis\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/anikaRobot\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/croak\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/AnxiousAndy\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/iven\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/f5\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/RicishayMax2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/klatt5\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/sandro\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/m6\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/quincy\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/grandma\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/iven3\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/klatt2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/david\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robosoft3\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/gustave\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/grandpa\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Mario\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/m7\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/m8\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Marco\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/klatt3\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/pablo\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/zac\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/m1\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/klatt4\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/iven4\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/pedro\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/travis\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/steph3\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/f1\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/shelby\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Michael\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Gene\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/michel\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robosoft7\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/belinda\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Jacky\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/anika\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/robosoft5\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Mr serious\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/caleb\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/ian\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/f3\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Henrique\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/iven2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/steph\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/fast\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Andy\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Nguyen\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/Reed\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/RicishayMax\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/edward2\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/voices/!v/adam\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/phondata-manifest\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/haw_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/it_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/da_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/no_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/sv_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/uk_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lb_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/tr_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/quc_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/sr_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/qdb_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/fr_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ca_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ia_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/an_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lfn_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/piqd_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/gn_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ne_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/smj_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/id_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/de_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ms_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ta_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lv_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/kl_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/hu_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/tn_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/cv_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/am_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/io_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/en_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ka_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/intonations\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/et_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/or_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/az_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/sk_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/bn_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ky_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/tk_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/phonindex\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/myn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/myn/quc\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/eu\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ccs\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ccs/ka\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/eo\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/qya\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/io\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/lfn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/ia\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/sjn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/jbo\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/piqd\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/py\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/xex\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/art/qdb\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/poz\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/poz/id\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/poz/mi\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/poz/ms\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/miz\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/miz/mto\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmq\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmq/fo\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmq/is\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmq/nb\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmq/sv\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmq/da\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/iro\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/iro/chr\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/urj\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/urj/hu\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/urj/smj\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/urj/fi\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/urj/et\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/jpx\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/jpx/ja\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/dra\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/dra/kn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/dra/te\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/dra/ml\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/dra/ta\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/ht\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/fr-CH\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/it\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/fr-BE\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/fr\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/an\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/ca-nw\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/ro\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/pt\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/ca\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/es\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/ca-ba\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/es-419\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/ca-va\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/pap\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/roa/pt-BR\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/aav\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/aav/vi-VN-x-central\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/aav/vi\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/aav/vi-VN-x-south\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/qu\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/tt\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/nog\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/uz\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/az\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/kaa\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/kk\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/tk\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/tr\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/ug\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/cv\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/ky\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/trk/ba\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/map\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/map/haw\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sai\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sai/gn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sit\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sit/my\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sit/yue-Latn-jyutping\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sit/yue\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sit/hak\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sit/cmn-Latn-pinyin\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sit/cmn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zls\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zls/hr\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zls/mk\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zls/bs\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zls/sr\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zls/sl\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zls/bg\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/cus\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/cus/om\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/cel\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/cel/gd\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/cel/cy\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/cel/ga\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/bat\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/bat/lt\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/bat/lv\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/bat/ltg\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/hi\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/kok\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/bpy\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/as\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/ur\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/or\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/si\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/ne\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/mr\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/bn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/gu\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/sd\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/inc/pa\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/bnt\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/bnt/sw\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/bnt/tn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zle\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zle/ru-LV\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zle/ru\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zle/ru-cl\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zle/be\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zle/uk\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/tai\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/tai/shn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/tai/th\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/azc\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/azc/nci\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ko\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zlw\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zlw/pl\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zlw/sk\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/zlw/cs\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ine\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ine/hyw\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ine/sq\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ine/hy\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ira\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ira/fa\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ira/ku\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ira/ps\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/ira/fa-Latn\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/esx\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/esx/kl\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/itc\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/itc/la\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en-Shaw\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/de\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en-US-nyc\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en-GB-scotland\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en-GB-x-rp\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/lb\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en-GB-x-gbcwmd\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en-US\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/af\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/nl\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en-029\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/gmw/en-GB-x-gbclan\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/grk\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/grk/grc\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/grk/el\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sem\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sem/am\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sem/ti\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sem/he\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sem/mt\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/lang/sem/ar\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/bpy_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/tt_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/cmn_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/qya_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ja_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ba_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ga_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/uz_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/sw_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/phontab\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/te_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ar_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/mto_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/yue_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/gd_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/cy_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/kk_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ht_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/hy_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ru_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ug_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ro_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/nl_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/vi_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/kn_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/py_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/gu_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/pa_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/pl_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/om_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/jbo_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/nci_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/af_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/cs_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/ur_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/fa_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/sq_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/mk_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/pt_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/chr_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/as_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/he_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/espeak-ng-data/hi_dict\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/lib/pkgconfig/espeak-ng.pc\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/vim/vimfiles/ftdetect\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/vim/vimfiles/ftdetect/espeakfiletype.vim\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/vim/vimfiles/syntax\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/vim/vimfiles/syntax/espeaklist.vim\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/share/vim/vimfiles/syntax/espeakrules.vim\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/bin/espeak-ng\n",
            "-- Set non-toolchain portion of runtime path of \"/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/bin/espeak-ng\" to \"/content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/lib\"\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/include/espeak\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/include/espeak/speak_lib.h\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/include/espeak-ng\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/include/espeak-ng/espeak_ng.h\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/include/espeak-ng/speak_lib.h\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/include/espeak-ng/encoding.h\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-build/espeak_ng-install/lib/libespeak-ng.a\n",
            "[11/12] Install the project...\u001b[K\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /content/piper1-gpl/_skbuild/linux-x86_64-3.12/cmake-install/src/piper/./espeakbridge.so\n",
            "copying _skbuild/linux-x86_64-3.12/cmake-install/src/piper/espeakbridge.so -> src/piper/espeakbridge.so\n",
            "\n",
            "running build_ext\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) (Optional) Mount Google Drive for datasets and outputs"
      ],
      "metadata": {
        "id": "xsmEOdPpRjPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "S82oT9EaRlR0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed1f8ec7-e819-42fc-a098-6471d0003c1a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NEW CODE\n",
        "!pip install onnxscript"
      ],
      "metadata": {
        "id": "EUmCozQGcqtV",
        "outputId": "2b33ef2d-b27a-42b1-9ed2-44bcc49e2326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxscript\n",
            "  Downloading onnxscript-0.5.7-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from onnxscript) (2.0.2)\n",
            "Collecting onnx_ir<2,>=0.1.12 (from onnxscript)\n",
            "  Downloading onnx_ir-0.1.13-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: onnx>=1.16 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (1.20.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxscript) (25.0)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (4.15.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.16->onnxscript) (5.29.5)\n",
            "Downloading onnxscript-0.5.7-py3-none-any.whl (693 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.4/693.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx_ir-0.1.13-py3-none-any.whl (133 kB)\n",
            "Installing collected packages: onnx_ir, onnxscript\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [onnxscript]\n",
            "\u001b[1A\u001b[2KSuccessfully installed onnx_ir-0.1.13 onnxscript-0.5.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Set paths and training hyperparams"
      ],
      "metadata": {
        "id": "bRewWxitSmSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# ==== CHANGE THESE ====\n",
        "VOICE_NAME      = \"my_colab_voice\"\n",
        "ESPEAK_VOICE    = \"en-us\"  # run `!espeak-ng --voices` to see options\n",
        "SAMPLE_RATE_HZ  = 22050\n",
        "BATCH_SIZE      = 32       # drop to 8 or 4 if you OOM\n",
        "\n",
        "DATA_ROOT       = Path(\"/content/drive/MyDrive/Piper-POC-Training\")\n",
        "AUDIO_DIR       = DATA_ROOT / \"wavs\"\n",
        "CSV_PATH        = DATA_ROOT / \"metadata.csv\"\n",
        "\n",
        "CACHE_DIR       = Path(\"/content/piper_cache\")\n",
        "CONFIG_PATH     = DATA_ROOT / f\"{VOICE_NAME}.json\"\n",
        "\n",
        "# Optional: start from an existing checkpoint to speed up & stabilize training\n",
        "# Get a .ckpt from https://huggingface.co/datasets/rhasspy/piper-checkpoints (medium quality recommended)\n",
        "CKPT_PATH       = \"\"  # e.g., \"/content/drive/MyDrive/piper_ckpts/en_US-lessac-medium.ckpt\"\n",
        "\n",
        "# Make sure dirs exist\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"CSV exists:\", CSV_PATH.exists())\n",
        "print(\"Audio dir exists:\", AUDIO_DIR.exists())\n",
        "print(\"Cache dir:\", CACHE_DIR)\n",
        "print(\"Config will be written to:\", CONFIG_PATH)"
      ],
      "metadata": {
        "id": "nyXNxSWFSq2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "effae4f9-3bc0-4100-b2ad-925ce041e8ab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV exists: True\n",
            "Audio dir exists: False\n",
            "Cache dir: /content/piper_cache\n",
            "Config will be written to: /content/drive/MyDrive/Piper-POC-Training/my_colab_voice.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) Quick sanity checks"
      ],
      "metadata": {
        "id": "YZqVTzcDSwRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!espeak-ng --voices | head -n 20"
      ],
      "metadata": {
        "id": "PPTTUbbxSydH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77fb0913-f06a-42e1-f860-919ef29e6db4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pty Language       Age/Gender VoiceName          File                 Other Languages\n",
            " 5  af              --/M      Afrikaans          gmw/af               \n",
            " 5  am              --/M      Amharic            sem/am               \n",
            " 5  an              --/M      Aragonese          roa/an               \n",
            " 5  ar              --/M      Arabic             sem/ar               \n",
            " 5  as              --/M      Assamese           inc/as               \n",
            " 5  az              --/M      Azerbaijani        trk/az               \n",
            " 5  ba              --/M      Bashkir            trk/ba               \n",
            " 5  bg              --/M      Bulgarian          zls/bg               \n",
            " 5  bn              --/M      Bengali            inc/bn               \n",
            " 5  bpy             --/M      Bishnupriya_Manipuri inc/bpy              \n",
            " 5  bs              --/M      Bosnian            zls/bs               \n",
            " 5  ca              --/M      Catalan            roa/ca               \n",
            " 5  cmn             --/M      Chinese_(Mandarin) sit/cmn              (zh-cmn 5)(zh 5)\n",
            " 5  cs              --/M      Czech              zlw/cs               \n",
            " 5  cy              --/M      Welsh              cel/cy               \n",
            " 5  da              --/M      Danish             gmq/da               \n",
            " 5  de              --/M      German             gmw/de               \n",
            " 5  el              --/M      Greek              grk/el               \n",
            " 5  en-029          --/M      English_(Caribbean) gmw/en-029           (en 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, io, os, textwrap\n",
        "\n",
        "csv_path = str(CSV_PATH)\n",
        "if os.path.exists(csv_path):\n",
        "    # Read as pipe-delimited, two columns\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, sep=\"|\", header=None, names=[\"audio\",\"text\"])\n",
        "        print(df.head())\n",
        "        # Check a few audio files exist\n",
        "        missing = [a for a in df[\"audio\"].head(5) if not (AUDIO_DIR/str(a)).exists()]\n",
        "        print(\"Missing among first 5:\", missing)\n",
        "    except Exception as e:\n",
        "        print(\"CSV read error:\", e)\n",
        "else:\n",
        "    print(\"CSV not found at:\", csv_path)"
      ],
      "metadata": {
        "id": "JJImgsKzS1pk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca029493-83db-4015-e23c-931f31cb615c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV not found at: /content/drive/MyDrive/tts_data/myvoice/metadata.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) Kick off training"
      ],
      "metadata": {
        "id": "jiggGLHzTxGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m piper.train fit \\\n",
        "  --data.voice_name \"my_colab_voice\" \\\n",
        "  --data.csv_path \"/content/drive/MyDrive/Piper-POC-Training/wavs/metadata.csv\" \\\n",
        "  --data.audio_dir \"/content/drive/MyDrive/Piper-POC-Training/wavs\" \\\n",
        "  --model.sample_rate 22050 \\\n",
        "  --data.espeak_voice \"en-us\" \\\n",
        "  --data.cache_dir \"/content/piper_cache\" \\\n",
        "  --data.config_path \"/content/drive/MyDrive/Piper-POC-Training/wavs/my_colab_voice.json\" \\\n",
        "  --data.batch_size 8 \\\n",
        "  --trainer.max_epochs 2166 \\ # NEW CODE\n",
        "  --ckpt_path \"https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/en/en_US/lessac/medium/epoch%3D2164-step%3D1355540.ckpt\""
      ],
      "metadata": {
        "id": "PZigA3tIUSeh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66eddd6c-f4d7-427a-f096-e2317ec69989"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/utilities/seed.py:44: No seed found, seed set to 0\n",
            "Seed set to 0\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "INFO:piper.train.vits.dataset:Processed 10 utterance(s)\n",
            "2025-12-18 12:23:46.108937: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766060626.128023    7966 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766060626.134232    7966 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766060626.149706    7966 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766060626.149739    7966 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766060626.149744    7966 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766060626.149752    7966 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-18 12:23:46.154310: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Restoring states from the checkpoint path at https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/en/en_US/lessac/medium/epoch%3D2164-step%3D1355540.ckpt\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.9.0 to v2.6.0. To apply the upgrade to your files permanently, run `python -m lightning.pytorch.utilities.upgrade_checkpoint https:/huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/en/en_US/lessac/medium/epoch%3D2164-step%3D1355540.ckpt`\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:566: The dirpath has changed from '/home/hansenm/larynx2/local/en-us/lessac/medium/lightning_logs/version_2/checkpoints' to '/content/piper1-gpl/lightning_logs/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
            "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n",
            "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
            "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model_g │ SynthesizerTrn           │ 23.7 M │ train │     0 │\n",
            "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ model_d │ MultiPeriodDiscriminator │ 46.7 M │ train │     0 │\n",
            "└───┴─────────┴──────────────────────────┴────────┴───────┴───────┘\n",
            "\u001b[1mTrainable params\u001b[0m: 70.4 M                                                        \n",
            "\u001b[1mNon-trainable params\u001b[0m: 0                                                         \n",
            "\u001b[1mTotal params\u001b[0m: 70.4 M                                                            \n",
            "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 281                                     \n",
            "\u001b[1mModules in train mode\u001b[0m: 504                                                      \n",
            "\u001b[1mModules in eval mode\u001b[0m: 0                                                         \n",
            "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                  \n",
            "Restored all states from the checkpoint at https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/en/en_US/lessac/medium/epoch%3D2164-step%3D1355540.ckpt\n",
            "\u001b[2K/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:317:\n",
            "The number of training batches (1) is smaller than the logging interval \n",
            "Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you \n",
            "want to see logs for the training epoch.\n",
            "\u001b[2KEpoch 2165/2165 \u001b[35m━━━━━━━━━━━━━━━━━━━━\u001b[0m 1/1 \u001b[2m0:00:01 • 0:00:00\u001b[0m \u001b[2;4m0.00it/s\u001b[0m \u001b[3mv_num: 2.000\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KEpoch 2165/2165 \u001b[35m━━━━━━━━━━━━━━━━━━━━\u001b[0m 1/1 \u001b[2m0:00:01 • 0:00:00\u001b[0m \u001b[2;4m0.00it/s\u001b[0m \u001b[3mv_num: 2.000\u001b[0m`Trainer.fit` stopped: `max_epochs=2166` reached.\n",
            "\u001b[2KEpoch 2165/2165 \u001b[35m━━━━━━━━━━━━━━━━━━━━\u001b[0m 1/1 \u001b[2m0:00:01 • 0:00:00\u001b[0m \u001b[2;4m0.00it/s\u001b[0m \u001b[3mv_num: 2.000\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Export to onnx"
      ],
      "metadata": {
        "id": "bSUfANcq7jUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CHANGED FILE NAME\n",
        "!python3 -m piper.train.export_onnx \\\n",
        "  --checkpoint \"/content/piper1-gpl/lightning_logs/version_2/checkpoints/epoch=2165-step=2.ckpt\" \\\n",
        "  --output-file \"/content/model.onnx\""
      ],
      "metadata": {
        "id": "oX63f32Q7m98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cbc61d2-2a9e-4551-c2ac-ec13f4c8fbb3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "/content/piper1-gpl/src/piper/train/export_onnx.py:92: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
            "  torch.onnx.export(\n",
            "W1218 12:33:38.976000 10536 torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 15 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
            "W1218 12:33:46.373000 10536 torch/fx/experimental/symbolic_shapes.py:7942] Unable to find user code corresponding to {u2}\n",
            "\n",
            "\n",
            "\n",
            "def forward(self, arg0_1: \"f32[256, 192]\", arg1_1: \"f32[1, 9, 96]\", arg2_1: \"f32[1, 9, 96]\", arg3_1: \"f32[192, 192, 1]\", arg4_1: \"f32[192]\", arg5_1: \"f32[192, 192, 1]\", arg6_1: \"f32[192]\", arg7_1: \"f32[192, 192, 1]\", arg8_1: \"f32[192]\", arg9_1: \"f32[192, 192, 1]\", arg10_1: \"f32[192]\", arg11_1: \"f32[1, 9, 96]\", arg12_1: \"f32[1, 9, 96]\", arg13_1: \"f32[192, 192, 1]\", arg14_1: \"f32[192]\", arg15_1: \"f32[192, 192, 1]\", arg16_1: \"f32[192]\", arg17_1: \"f32[192, 192, 1]\", arg18_1: \"f32[192]\", arg19_1: \"f32[192, 192, 1]\", arg20_1: \"f32[192]\", arg21_1: \"f32[1, 9, 96]\", arg22_1: \"f32[1, 9, 96]\", arg23_1: \"f32[192, 192, 1]\", arg24_1: \"f32[192]\", arg25_1: \"f32[192, 192, 1]\", arg26_1: \"f32[192]\", arg27_1: \"f32[192, 192, 1]\", arg28_1: \"f32[192]\", arg29_1: \"f32[192, 192, 1]\", arg30_1: \"f32[192]\", arg31_1: \"f32[1, 9, 96]\", arg32_1: \"f32[1, 9, 96]\", arg33_1: \"f32[192, 192, 1]\", arg34_1: \"f32[192]\", arg35_1: \"f32[192, 192, 1]\", arg36_1: \"f32[192]\", arg37_1: \"f32[192, 192, 1]\", arg38_1: \"f32[192]\", arg39_1: \"f32[192, 192, 1]\", arg40_1: \"f32[192]\", arg41_1: \"f32[1, 9, 96]\", arg42_1: \"f32[1, 9, 96]\", arg43_1: \"f32[192, 192, 1]\", arg44_1: \"f32[192]\", arg45_1: \"f32[192, 192, 1]\", arg46_1: \"f32[192]\", arg47_1: \"f32[192, 192, 1]\", arg48_1: \"f32[192]\", arg49_1: \"f32[192, 192, 1]\", arg50_1: \"f32[192]\", arg51_1: \"f32[1, 9, 96]\", arg52_1: \"f32[1, 9, 96]\", arg53_1: \"f32[192, 192, 1]\", arg54_1: \"f32[192]\", arg55_1: \"f32[192, 192, 1]\", arg56_1: \"f32[192]\", arg57_1: \"f32[192, 192, 1]\", arg58_1: \"f32[192]\", arg59_1: \"f32[192, 192, 1]\", arg60_1: \"f32[192]\", arg61_1: \"f32[192]\", arg62_1: \"f32[192]\", arg63_1: \"f32[192]\", arg64_1: \"f32[192]\", arg65_1: \"f32[192]\", arg66_1: \"f32[192]\", arg67_1: \"f32[192]\", arg68_1: \"f32[192]\", arg69_1: \"f32[192]\", arg70_1: \"f32[192]\", arg71_1: \"f32[192]\", arg72_1: \"f32[192]\", arg73_1: \"f32[768, 192, 3]\", arg74_1: \"f32[768]\", arg75_1: \"f32[192, 768, 3]\", arg76_1: \"f32[192]\", arg77_1: \"f32[768, 192, 3]\", arg78_1: \"f32[768]\", arg79_1: \"f32[192, 768, 3]\", arg80_1: \"f32[192]\", arg81_1: \"f32[768, 192, 3]\", arg82_1: \"f32[768]\", arg83_1: \"f32[192, 768, 3]\", arg84_1: \"f32[192]\", arg85_1: \"f32[768, 192, 3]\", arg86_1: \"f32[768]\", arg87_1: \"f32[192, 768, 3]\", arg88_1: \"f32[192]\", arg89_1: \"f32[768, 192, 3]\", arg90_1: \"f32[768]\", arg91_1: \"f32[192, 768, 3]\", arg92_1: \"f32[192]\", arg93_1: \"f32[768, 192, 3]\", arg94_1: \"f32[768]\", arg95_1: \"f32[192, 768, 3]\", arg96_1: \"f32[192]\", arg97_1: \"f32[192]\", arg98_1: \"f32[192]\", arg99_1: \"f32[192]\", arg100_1: \"f32[192]\", arg101_1: \"f32[192]\", arg102_1: \"f32[192]\", arg103_1: \"f32[192]\", arg104_1: \"f32[192]\", arg105_1: \"f32[192]\", arg106_1: \"f32[192]\", arg107_1: \"f32[192]\", arg108_1: \"f32[192]\", arg109_1: \"f32[384, 192, 1]\", arg110_1: \"f32[384]\", arg111_1: \"f32[256, 192, 7]\", arg112_1: \"f32[256]\", arg113_1: \"f32[128]\", arg114_1: \"f32[256, 128, 16]\", arg115_1: \"f32[64]\", arg116_1: \"f32[128, 64, 16]\", arg117_1: \"f32[32]\", arg118_1: \"f32[64, 32, 8]\", arg119_1: \"f32[128]\", arg120_1: \"f32[128, 128, 3]\", arg121_1: \"f32[128]\", arg122_1: \"f32[128, 128, 3]\", arg123_1: \"f32[128]\", arg124_1: \"f32[128, 128, 5]\", arg125_1: \"f32[128]\", arg126_1: \"f32[128, 128, 5]\", arg127_1: \"f32[128]\", arg128_1: \"f32[128, 128, 7]\", arg129_1: \"f32[128]\", arg130_1: \"f32[128, 128, 7]\", arg131_1: \"f32[64]\", arg132_1: \"f32[64, 64, 3]\", arg133_1: \"f32[64]\", arg134_1: \"f32[64, 64, 3]\", arg135_1: \"f32[64]\", arg136_1: \"f32[64, 64, 5]\", arg137_1: \"f32[64]\", arg138_1: \"f32[64, 64, 5]\", arg139_1: \"f32[64]\", arg140_1: \"f32[64, 64, 7]\", arg141_1: \"f32[64]\", arg142_1: \"f32[64, 64, 7]\", arg143_1: \"f32[32]\", arg144_1: \"f32[32, 32, 3]\", arg145_1: \"f32[32]\", arg146_1: \"f32[32, 32, 3]\", arg147_1: \"f32[32]\", arg148_1: \"f32[32, 32, 5]\", arg149_1: \"f32[32]\", arg150_1: \"f32[32, 32, 5]\", arg151_1: \"f32[32]\", arg152_1: \"f32[32, 32, 7]\", arg153_1: \"f32[32]\", arg154_1: \"f32[32, 32, 7]\", arg155_1: \"f32[1, 32, 7]\", arg156_1: \"f32[192, 513, 1]\", arg157_1: \"f32[192]\", arg158_1: \"f32[384]\", arg159_1: \"f32[384, 1, 1]\", arg160_1: \"f32[384, 192, 5]\", arg161_1: \"f32[384]\", arg162_1: \"f32[384, 1, 1]\", arg163_1: \"f32[384, 192, 5]\", arg164_1: \"f32[384]\", arg165_1: \"f32[384, 1, 1]\", arg166_1: \"f32[384, 192, 5]\", arg167_1: \"f32[384]\", arg168_1: \"f32[384, 1, 1]\", arg169_1: \"f32[384, 192, 5]\", arg170_1: \"f32[384]\", arg171_1: \"f32[384, 1, 1]\", arg172_1: \"f32[384, 192, 5]\", arg173_1: \"f32[384]\", arg174_1: \"f32[384, 1, 1]\", arg175_1: \"f32[384, 192, 5]\", arg176_1: \"f32[384]\", arg177_1: \"f32[384, 1, 1]\", arg178_1: \"f32[384, 192, 5]\", arg179_1: \"f32[384]\", arg180_1: \"f32[384, 1, 1]\", arg181_1: \"f32[384, 192, 5]\", arg182_1: \"f32[384]\", arg183_1: \"f32[384, 1, 1]\", arg184_1: \"f32[384, 192, 5]\", arg185_1: \"f32[384]\", arg186_1: \"f32[384, 1, 1]\", arg187_1: \"f32[384, 192, 5]\", arg188_1: \"f32[384]\", arg189_1: \"f32[384, 1, 1]\", arg190_1: \"f32[384, 192, 5]\", arg191_1: \"f32[384]\", arg192_1: \"f32[384, 1, 1]\", arg193_1: \"f32[384, 192, 5]\", arg194_1: \"f32[384]\", arg195_1: \"f32[384, 1, 1]\", arg196_1: \"f32[384, 192, 5]\", arg197_1: \"f32[384]\", arg198_1: \"f32[384, 1, 1]\", arg199_1: \"f32[384, 192, 5]\", arg200_1: \"f32[384]\", arg201_1: \"f32[384, 1, 1]\", arg202_1: \"f32[384, 192, 5]\", arg203_1: \"f32[384]\", arg204_1: \"f32[384, 1, 1]\", arg205_1: \"f32[384, 192, 5]\", arg206_1: \"f32[384]\", arg207_1: \"f32[384, 1, 1]\", arg208_1: \"f32[384, 192, 1]\", arg209_1: \"f32[384]\", arg210_1: \"f32[384, 1, 1]\", arg211_1: \"f32[384, 192, 1]\", arg212_1: \"f32[384]\", arg213_1: \"f32[384, 1, 1]\", arg214_1: \"f32[384, 192, 1]\", arg215_1: \"f32[384]\", arg216_1: \"f32[384, 1, 1]\", arg217_1: \"f32[384, 192, 1]\", arg218_1: \"f32[384]\", arg219_1: \"f32[384, 1, 1]\", arg220_1: \"f32[384, 192, 1]\", arg221_1: \"f32[384]\", arg222_1: \"f32[384, 1, 1]\", arg223_1: \"f32[384, 192, 1]\", arg224_1: \"f32[384]\", arg225_1: \"f32[384, 1, 1]\", arg226_1: \"f32[384, 192, 1]\", arg227_1: \"f32[384]\", arg228_1: \"f32[384, 1, 1]\", arg229_1: \"f32[384, 192, 1]\", arg230_1: \"f32[384]\", arg231_1: \"f32[384, 1, 1]\", arg232_1: \"f32[384, 192, 1]\", arg233_1: \"f32[384]\", arg234_1: \"f32[384, 1, 1]\", arg235_1: \"f32[384, 192, 1]\", arg236_1: \"f32[384]\", arg237_1: \"f32[384, 1, 1]\", arg238_1: \"f32[384, 192, 1]\", arg239_1: \"f32[384]\", arg240_1: \"f32[384, 1, 1]\", arg241_1: \"f32[384, 192, 1]\", arg242_1: \"f32[384]\", arg243_1: \"f32[384, 1, 1]\", arg244_1: \"f32[384, 192, 1]\", arg245_1: \"f32[384]\", arg246_1: \"f32[384, 1, 1]\", arg247_1: \"f32[384, 192, 1]\", arg248_1: \"f32[384]\", arg249_1: \"f32[384, 1, 1]\", arg250_1: \"f32[384, 192, 1]\", arg251_1: \"f32[192]\", arg252_1: \"f32[192, 1, 1]\", arg253_1: \"f32[192, 192, 1]\", arg254_1: \"f32[384, 192, 1]\", arg255_1: \"f32[384]\", arg256_1: \"f32[192, 96, 1]\", arg257_1: \"f32[192]\", arg258_1: \"f32[384]\", arg259_1: \"f32[384, 1, 1]\", arg260_1: \"f32[384, 192, 5]\", arg261_1: \"f32[384]\", arg262_1: \"f32[384, 1, 1]\", arg263_1: \"f32[384, 192, 5]\", arg264_1: \"f32[384]\", arg265_1: \"f32[384, 1, 1]\", arg266_1: \"f32[384, 192, 5]\", arg267_1: \"f32[384]\", arg268_1: \"f32[384, 1, 1]\", arg269_1: \"f32[384, 192, 5]\", arg270_1: \"f32[384]\", arg271_1: \"f32[384, 1, 1]\", arg272_1: \"f32[384, 192, 1]\", arg273_1: \"f32[384]\", arg274_1: \"f32[384, 1, 1]\", arg275_1: \"f32[384, 192, 1]\", arg276_1: \"f32[384]\", arg277_1: \"f32[384, 1, 1]\", arg278_1: \"f32[384, 192, 1]\", arg279_1: \"f32[192]\", arg280_1: \"f32[192, 1, 1]\", arg281_1: \"f32[192, 192, 1]\", arg282_1: \"f32[96, 192, 1]\", arg283_1: \"f32[96]\", arg284_1: \"f32[192, 96, 1]\", arg285_1: \"f32[192]\", arg286_1: \"f32[384]\", arg287_1: \"f32[384, 1, 1]\", arg288_1: \"f32[384, 192, 5]\", arg289_1: \"f32[384]\", arg290_1: \"f32[384, 1, 1]\", arg291_1: \"f32[384, 192, 5]\", arg292_1: \"f32[384]\", arg293_1: \"f32[384, 1, 1]\", arg294_1: \"f32[384, 192, 5]\", arg295_1: \"f32[384]\", arg296_1: \"f32[384, 1, 1]\", arg297_1: \"f32[384, 192, 5]\", arg298_1: \"f32[384]\", arg299_1: \"f32[384, 1, 1]\", arg300_1: \"f32[384, 192, 1]\", arg301_1: \"f32[384]\", arg302_1: \"f32[384, 1, 1]\", arg303_1: \"f32[384, 192, 1]\", arg304_1: \"f32[384]\", arg305_1: \"f32[384, 1, 1]\", arg306_1: \"f32[384, 192, 1]\", arg307_1: \"f32[192]\", arg308_1: \"f32[192, 1, 1]\", arg309_1: \"f32[192, 192, 1]\", arg310_1: \"f32[96, 192, 1]\", arg311_1: \"f32[96]\", arg312_1: \"f32[192, 96, 1]\", arg313_1: \"f32[192]\", arg314_1: \"f32[384]\", arg315_1: \"f32[384, 1, 1]\", arg316_1: \"f32[384, 192, 5]\", arg317_1: \"f32[384]\", arg318_1: \"f32[384, 1, 1]\", arg319_1: \"f32[384, 192, 5]\", arg320_1: \"f32[384]\", arg321_1: \"f32[384, 1, 1]\", arg322_1: \"f32[384, 192, 5]\", arg323_1: \"f32[384]\", arg324_1: \"f32[384, 1, 1]\", arg325_1: \"f32[384, 192, 5]\", arg326_1: \"f32[384]\", arg327_1: \"f32[384, 1, 1]\", arg328_1: \"f32[384, 192, 1]\", arg329_1: \"f32[384]\", arg330_1: \"f32[384, 1, 1]\", arg331_1: \"f32[384, 192, 1]\", arg332_1: \"f32[384]\", arg333_1: \"f32[384, 1, 1]\", arg334_1: \"f32[384, 192, 1]\", arg335_1: \"f32[192]\", arg336_1: \"f32[192, 1, 1]\", arg337_1: \"f32[192, 192, 1]\", arg338_1: \"f32[96, 192, 1]\", arg339_1: \"f32[96]\", arg340_1: \"f32[192, 96, 1]\", arg341_1: \"f32[192]\", arg342_1: \"f32[384]\", arg343_1: \"f32[384, 1, 1]\", arg344_1: \"f32[384, 192, 5]\", arg345_1: \"f32[384]\", arg346_1: \"f32[384, 1, 1]\", arg347_1: \"f32[384, 192, 5]\", arg348_1: \"f32[384]\", arg349_1: \"f32[384, 1, 1]\", arg350_1: \"f32[384, 192, 5]\", arg351_1: \"f32[384]\", arg352_1: \"f32[384, 1, 1]\", arg353_1: \"f32[384, 192, 5]\", arg354_1: \"f32[384]\", arg355_1: \"f32[384, 1, 1]\", arg356_1: \"f32[384, 192, 1]\", arg357_1: \"f32[384]\", arg358_1: \"f32[384, 1, 1]\", arg359_1: \"f32[384, 192, 1]\", arg360_1: \"f32[384]\", arg361_1: \"f32[384, 1, 1]\", arg362_1: \"f32[384, 192, 1]\", arg363_1: \"f32[192]\", arg364_1: \"f32[192, 1, 1]\", arg365_1: \"f32[192, 192, 1]\", arg366_1: \"f32[96, 192, 1]\", arg367_1: \"f32[96]\", arg368_1: \"f32[2, 1]\", arg369_1: \"f32[2, 1]\", arg370_1: \"f32[192, 1, 1]\", arg371_1: \"f32[192]\", arg372_1: \"f32[192, 1, 3]\", arg373_1: \"f32[192]\", arg374_1: \"f32[192, 1, 3]\", arg375_1: \"f32[192]\", arg376_1: \"f32[192, 1, 3]\", arg377_1: \"f32[192]\", arg378_1: \"f32[192, 192, 1]\", arg379_1: \"f32[192]\", arg380_1: \"f32[192, 192, 1]\", arg381_1: \"f32[192]\", arg382_1: \"f32[192, 192, 1]\", arg383_1: \"f32[192]\", arg384_1: \"f32[192]\", arg385_1: \"f32[192]\", arg386_1: \"f32[192]\", arg387_1: \"f32[192]\", arg388_1: \"f32[192]\", arg389_1: \"f32[192]\", arg390_1: \"f32[192]\", arg391_1: \"f32[192]\", arg392_1: \"f32[192]\", arg393_1: \"f32[192]\", arg394_1: \"f32[192]\", arg395_1: \"f32[192]\", arg396_1: \"f32[29, 192, 1]\", arg397_1: \"f32[29]\", arg398_1: \"f32[192, 1, 1]\", arg399_1: \"f32[192]\", arg400_1: \"f32[192, 1, 3]\", arg401_1: \"f32[192]\", arg402_1: \"f32[192, 1, 3]\", arg403_1: \"f32[192]\", arg404_1: \"f32[192, 1, 3]\", arg405_1: \"f32[192]\", arg406_1: \"f32[192, 192, 1]\", arg407_1: \"f32[192]\", arg408_1: \"f32[192, 192, 1]\", arg409_1: \"f32[192]\", arg410_1: \"f32[192, 192, 1]\", arg411_1: \"f32[192]\", arg412_1: \"f32[192]\", arg413_1: \"f32[192]\", arg414_1: \"f32[192]\", arg415_1: \"f32[192]\", arg416_1: \"f32[192]\", arg417_1: \"f32[192]\", arg418_1: \"f32[192]\", arg419_1: \"f32[192]\", arg420_1: \"f32[192]\", arg421_1: \"f32[192]\", arg422_1: \"f32[192]\", arg423_1: \"f32[192]\", arg424_1: \"f32[29, 192, 1]\", arg425_1: \"f32[29]\", arg426_1: \"f32[192, 1, 1]\", arg427_1: \"f32[192]\", arg428_1: \"f32[192, 1, 3]\", arg429_1: \"f32[192]\", arg430_1: \"f32[192, 1, 3]\", arg431_1: \"f32[192]\", arg432_1: \"f32[192, 1, 3]\", arg433_1: \"f32[192]\", arg434_1: \"f32[192, 192, 1]\", arg435_1: \"f32[192]\", arg436_1: \"f32[192, 192, 1]\", arg437_1: \"f32[192]\", arg438_1: \"f32[192, 192, 1]\", arg439_1: \"f32[192]\", arg440_1: \"f32[192]\", arg441_1: \"f32[192]\", arg442_1: \"f32[192]\", arg443_1: \"f32[192]\", arg444_1: \"f32[192]\", arg445_1: \"f32[192]\", arg446_1: \"f32[192]\", arg447_1: \"f32[192]\", arg448_1: \"f32[192]\", arg449_1: \"f32[192]\", arg450_1: \"f32[192]\", arg451_1: \"f32[192]\", arg452_1: \"f32[29, 192, 1]\", arg453_1: \"f32[29]\", arg454_1: \"f32[192, 1, 1]\", arg455_1: \"f32[192]\", arg456_1: \"f32[192, 1, 3]\", arg457_1: \"f32[192]\", arg458_1: \"f32[192, 1, 3]\", arg459_1: \"f32[192]\", arg460_1: \"f32[192, 1, 3]\", arg461_1: \"f32[192]\", arg462_1: \"f32[192, 192, 1]\", arg463_1: \"f32[192]\", arg464_1: \"f32[192, 192, 1]\", arg465_1: \"f32[192]\", arg466_1: \"f32[192, 192, 1]\", arg467_1: \"f32[192]\", arg468_1: \"f32[192]\", arg469_1: \"f32[192]\", arg470_1: \"f32[192]\", arg471_1: \"f32[192]\", arg472_1: \"f32[192]\", arg473_1: \"f32[192]\", arg474_1: \"f32[192]\", arg475_1: \"f32[192]\", arg476_1: \"f32[192]\", arg477_1: \"f32[192]\", arg478_1: \"f32[192]\", arg479_1: \"f32[192]\", arg480_1: \"f32[29, 192, 1]\", arg481_1: \"f32[29]\", arg482_1: \"f32[192, 1, 1]\", arg483_1: \"f32[192]\", arg484_1: \"f32[192, 192, 1]\", arg485_1: \"f32[192]\", arg486_1: \"f32[192, 1, 3]\", arg487_1: \"f32[192]\", arg488_1: \"f32[192, 1, 3]\", arg489_1: \"f32[192]\", arg490_1: \"f32[192, 1, 3]\", arg491_1: \"f32[192]\", arg492_1: \"f32[192, 192, 1]\", arg493_1: \"f32[192]\", arg494_1: \"f32[192, 192, 1]\", arg495_1: \"f32[192]\", arg496_1: \"f32[192, 192, 1]\", arg497_1: \"f32[192]\", arg498_1: \"f32[192]\", arg499_1: \"f32[192]\", arg500_1: \"f32[192]\", arg501_1: \"f32[192]\", arg502_1: \"f32[192]\", arg503_1: \"f32[192]\", arg504_1: \"f32[192]\", arg505_1: \"f32[192]\", arg506_1: \"f32[192]\", arg507_1: \"f32[192]\", arg508_1: \"f32[192]\", arg509_1: \"f32[192]\", arg510_1: \"f32[2, 1]\", arg511_1: \"f32[2, 1]\", arg512_1: \"f32[192, 1, 1]\", arg513_1: \"f32[192]\", arg514_1: \"f32[192, 1, 3]\", arg515_1: \"f32[192]\", arg516_1: \"f32[192, 1, 3]\", arg517_1: \"f32[192]\", arg518_1: \"f32[192, 1, 3]\", arg519_1: \"f32[192]\", arg520_1: \"f32[192, 192, 1]\", arg521_1: \"f32[192]\", arg522_1: \"f32[192, 192, 1]\", arg523_1: \"f32[192]\", arg524_1: \"f32[192, 192, 1]\", arg525_1: \"f32[192]\", arg526_1: \"f32[192]\", arg527_1: \"f32[192]\", arg528_1: \"f32[192]\", arg529_1: \"f32[192]\", arg530_1: \"f32[192]\", arg531_1: \"f32[192]\", arg532_1: \"f32[192]\", arg533_1: \"f32[192]\", arg534_1: \"f32[192]\", arg535_1: \"f32[192]\", arg536_1: \"f32[192]\", arg537_1: \"f32[192]\", arg538_1: \"f32[29, 192, 1]\", arg539_1: \"f32[29]\", arg540_1: \"f32[192, 1, 1]\", arg541_1: \"f32[192]\", arg542_1: \"f32[192, 1, 3]\", arg543_1: \"f32[192]\", arg544_1: \"f32[192, 1, 3]\", arg545_1: \"f32[192]\", arg546_1: \"f32[192, 1, 3]\", arg547_1: \"f32[192]\", arg548_1: \"f32[192, 192, 1]\", arg549_1: \"f32[192]\", arg550_1: \"f32[192, 192, 1]\", arg551_1: \"f32[192]\", arg552_1: \"f32[192, 192, 1]\", arg553_1: \"f32[192]\", arg554_1: \"f32[192]\", arg555_1: \"f32[192]\", arg556_1: \"f32[192]\", arg557_1: \"f32[192]\", arg558_1: \"f32[192]\", arg559_1: \"f32[192]\", arg560_1: \"f32[192]\", arg561_1: \"f32[192]\", arg562_1: \"f32[192]\", arg563_1: \"f32[192]\", arg564_1: \"f32[192]\", arg565_1: \"f32[192]\", arg566_1: \"f32[29, 192, 1]\", arg567_1: \"f32[29]\", arg568_1: \"f32[192, 1, 1]\", arg569_1: \"f32[192]\", arg570_1: \"f32[192, 1, 3]\", arg571_1: \"f32[192]\", arg572_1: \"f32[192, 1, 3]\", arg573_1: \"f32[192]\", arg574_1: \"f32[192, 1, 3]\", arg575_1: \"f32[192]\", arg576_1: \"f32[192, 192, 1]\", arg577_1: \"f32[192]\", arg578_1: \"f32[192, 192, 1]\", arg579_1: \"f32[192]\", arg580_1: \"f32[192, 192, 1]\", arg581_1: \"f32[192]\", arg582_1: \"f32[192]\", arg583_1: \"f32[192]\", arg584_1: \"f32[192]\", arg585_1: \"f32[192]\", arg586_1: \"f32[192]\", arg587_1: \"f32[192]\", arg588_1: \"f32[192]\", arg589_1: \"f32[192]\", arg590_1: \"f32[192]\", arg591_1: \"f32[192]\", arg592_1: \"f32[192]\", arg593_1: \"f32[192]\", arg594_1: \"f32[29, 192, 1]\", arg595_1: \"f32[29]\", arg596_1: \"f32[192, 1, 1]\", arg597_1: \"f32[192]\", arg598_1: \"f32[192, 1, 3]\", arg599_1: \"f32[192]\", arg600_1: \"f32[192, 1, 3]\", arg601_1: \"f32[192]\", arg602_1: \"f32[192, 1, 3]\", arg603_1: \"f32[192]\", arg604_1: \"f32[192, 192, 1]\", arg605_1: \"f32[192]\", arg606_1: \"f32[192, 192, 1]\", arg607_1: \"f32[192]\", arg608_1: \"f32[192, 192, 1]\", arg609_1: \"f32[192]\", arg610_1: \"f32[192]\", arg611_1: \"f32[192]\", arg612_1: \"f32[192]\", arg613_1: \"f32[192]\", arg614_1: \"f32[192]\", arg615_1: \"f32[192]\", arg616_1: \"f32[192]\", arg617_1: \"f32[192]\", arg618_1: \"f32[192]\", arg619_1: \"f32[192]\", arg620_1: \"f32[192]\", arg621_1: \"f32[192]\", arg622_1: \"f32[29, 192, 1]\", arg623_1: \"f32[29]\", arg624_1: \"f32[192, 192, 1]\", arg625_1: \"f32[192]\", arg626_1: \"f32[192, 192, 1]\", arg627_1: \"f32[192]\", arg628_1: \"f32[192, 1, 3]\", arg629_1: \"f32[192]\", arg630_1: \"f32[192, 1, 3]\", arg631_1: \"f32[192]\", arg632_1: \"f32[192, 1, 3]\", arg633_1: \"f32[192]\", arg634_1: \"f32[192, 192, 1]\", arg635_1: \"f32[192]\", arg636_1: \"f32[192, 192, 1]\", arg637_1: \"f32[192]\", arg638_1: \"f32[192, 192, 1]\", arg639_1: \"f32[192]\", arg640_1: \"f32[192]\", arg641_1: \"f32[192]\", arg642_1: \"f32[192]\", arg643_1: \"f32[192]\", arg644_1: \"f32[192]\", arg645_1: \"f32[192]\", arg646_1: \"f32[192]\", arg647_1: \"f32[192]\", arg648_1: \"f32[192]\", arg649_1: \"f32[192]\", arg650_1: \"f32[192]\", arg651_1: \"f32[192]\", arg652_1: \"i64[s31, s12]\", arg653_1: \"i64[s31]\", arg654_1: \"f32[3]\", arg655_1):\n",
            "    # No stacktrace found for following nodes\n",
            "    select: \"f32[]\" = torch.ops.aten.select.int(arg654_1, 0, 0);  select = None\n",
            "    select_1: \"f32[]\" = torch.ops.aten.select.int(arg654_1, 0, 1);  select_1 = None\n",
            "    select_2: \"f32[]\" = torch.ops.aten.select.int(arg654_1, 0, 2);  arg654_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(\n",
            "    embedding: \"f32[s31, s12, 192]\" = torch.ops.aten.embedding.default(arg0_1, arg652_1);  arg0_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:199 in forward, code: x = self.emb(x) * math.sqrt(self.hidden_channels)  # [b, t, h]\n",
            "    mul: \"f32[s31, s12, 192]\" = torch.ops.aten.mul.Tensor(embedding, 13.856406460551018);  embedding = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:200 in forward, code: x = torch.transpose(x, 1, -1)  # [b, h, t]\n",
            "    transpose: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(mul, 1, -1);  mul = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:202 in forward, code: commons.sequence_mask(x_lengths, x.size(2)), 1\n",
            "    sym_size_int: \"Sym(s12)\" = torch.ops.aten.sym_size.int(arg652_1, 1)\n",
            "    arange: \"i64[s12]\" = torch.ops.aten.arange.default(sym_size_int, dtype = torch.int64, device = device(type='cpu'), pin_memory = False);  sym_size_int = None\n",
            "    unsqueeze: \"i64[1, s12]\" = torch.ops.aten.unsqueeze.default(arange, 0)\n",
            "    unsqueeze_1: \"i64[s31, 1]\" = torch.ops.aten.unsqueeze.default(arg653_1, 1)\n",
            "    lt: \"b8[s31, s12]\" = torch.ops.aten.lt.Tensor(unsqueeze, unsqueeze_1);  unsqueeze = unsqueeze_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:201 in forward, code: x_mask = torch.unsqueeze(\n",
            "    unsqueeze_2: \"b8[s31, 1, s12]\" = torch.ops.aten.unsqueeze.default(lt, 1);  lt = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:203 in forward, code: ).type_as(x)\n",
            "    type_as: \"f32[s31, 1, s12]\" = torch.ops.aten.type_as.default(unsqueeze_2, transpose);  unsqueeze_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:205 in forward, code: x = self.encoder(x * x_mask, x_mask)\n",
            "    mul_1: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose, type_as);  transpose = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:61 in forward, code: attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n",
            "    unsqueeze_3: \"f32[s31, 1, 1, s12]\" = torch.ops.aten.unsqueeze.default(type_as, 2)\n",
            "    unsqueeze_4: \"f32[s31, 1, s12, 1]\" = torch.ops.aten.unsqueeze.default(type_as, -1)\n",
            "    mul_2: \"f32[s31, 1, s12, s12]\" = torch.ops.aten.mul.Tensor(unsqueeze_3, unsqueeze_4);  unsqueeze_3 = unsqueeze_4 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:62 in forward, code: x = x * x_mask\n",
            "    mul_3: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(mul_1, type_as);  mul_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_3, arg3_1, arg4_1);  arg3_1 = arg4_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_1: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_3, arg5_1, arg6_1);  arg5_1 = arg6_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_2: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_3, arg7_1, arg8_1);  arg7_1 = arg8_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_1: \"Sym(s31)\" = torch.ops.aten.sym_size.int(arg652_1, 0);  arg652_1 = None\n",
            "    sym_size_int_2: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d, 2)\n",
            "    view: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d, [sym_size_int_1, 2, 96, sym_size_int_2]);  conv1d = None\n",
            "    transpose_1: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view, 2, 3);  view = None\n",
            "    sym_size_int_3: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_1, 2)\n",
            "    view_1: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_1, [sym_size_int_1, 2, 96, sym_size_int_3]);  conv1d_1 = None\n",
            "    transpose_2: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_1, 2, 3);  view_1 = None\n",
            "    view_2: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_2, [sym_size_int_1, 2, 96, sym_size_int_3]);  conv1d_2 = None\n",
            "    transpose_3: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_2, 2, 3);  view_2 = None\n",
            "    div: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_1, 9.797958971132712)\n",
            "    transpose_4: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_2, -2, -1);  transpose_2 = None\n",
            "    matmul: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div, transpose_4);  div = transpose_4 = None\n",
            "    eq: \"Sym(True)\" = sym_size_int_3 == sym_size_int_2;  eq = None\n",
            "    sub: \"Sym(s12 - 5)\" = sym_size_int_3 - 5\n",
            "    sym_max: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub, 0);  sub = None\n",
            "    sub_1: \"Sym(5 - s12)\" = 5 - sym_size_int_3\n",
            "    sym_max_1: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_1, 0);  sub_1 = None\n",
            "    mul_4: \"Sym(2*s12)\" = 2 * sym_size_int_3\n",
            "    add: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_1 + mul_4;  mul_4 = None\n",
            "    sub_2: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add - 1;  add = None\n",
            "    gt: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max > 0;  gt = None\n",
            "    pad: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg1_1, [0, 0, sym_max, sym_max, 0, 0]);  arg1_1 = sym_max = None\n",
            "    slice_1: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad, 1, sym_max_1, sub_2);  pad = sym_max_1 = sub_2 = None\n",
            "    div_1: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_1, 9.797958971132712);  transpose_1 = None\n",
            "    unsqueeze_5: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_1, 0);  slice_1 = None\n",
            "    transpose_5: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_5, -2, -1);  unsqueeze_5 = None\n",
            "    matmul_1: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_1, transpose_5);  div_1 = transpose_5 = None\n",
            "    pad_1: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_1, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_1 = None\n",
            "    mul_5: \"Sym(2*s12)\" = sym_size_int_2 * 2\n",
            "    mul_6: \"Sym(2*s12**2)\" = mul_5 * sym_size_int_2;  mul_5 = None\n",
            "    view_3: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_1, [sym_size_int_1, 2, mul_6]);  pad_1 = mul_6 = None\n",
            "    sub_3: \"Sym(s12 - 1)\" = sym_size_int_2 - 1\n",
            "    pad_2: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_3, [0, sub_3, 0, 0, 0, 0]);  view_3 = sub_3 = None\n",
            "    add_1: \"Sym(s12 + 1)\" = sym_size_int_2 + 1\n",
            "    mul_7: \"Sym(2*s12)\" = 2 * sym_size_int_2\n",
            "    sub_4: \"Sym(2*s12 - 1)\" = mul_7 - 1;  mul_7 = None\n",
            "    view_4: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_2, [sym_size_int_1, 2, add_1, sub_4]);  pad_2 = add_1 = sub_4 = None\n",
            "    sub_5: \"Sym(s12 - 1)\" = sym_size_int_2 - 1\n",
            "    slice_2: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_4, 2, None, sym_size_int_2);  view_4 = None\n",
            "    slice_3: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_2, 3, sub_5);  slice_2 = sub_5 = None\n",
            "    add_2: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul, slice_3);  matmul = slice_3 = None\n",
            "    eq_1: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_2, eq_1, -10000.0);  add_2 = eq_1 = None\n",
            "    softmax: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill, -1);  masked_fill = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax, 0.1, False);  softmax = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_2: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout, transpose_3);  transpose_3 = None\n",
            "    sym_size_int_4: \"Sym(s12)\" = torch.ops.aten.sym_size.int(arange, 0);  arange = None\n",
            "    sub_6: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_3: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout, [0, sub_6, 0, 0, 0, 0, 0, 0]);  dropout = sub_6 = None\n",
            "    mul_8: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_7: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_9: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_7;  sub_7 = None\n",
            "    add_3: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_8 + mul_9;  mul_8 = mul_9 = None\n",
            "    sym_size_int_5: \"Sym(s31)\" = torch.ops.aten.sym_size.int(arg653_1, 0);  arg653_1 = None\n",
            "    view_5: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_3, [sym_size_int_5, 2, add_3]);  pad_3 = add_3 = None\n",
            "    pad_4: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_5, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_5 = None\n",
            "    mul_10: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_6: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_4, [sym_size_int_5, 2, sym_size_int_4, mul_10]);  pad_4 = mul_10 = None\n",
            "    le: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le = None\n",
            "    slice_4: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_6, 0, 0, 9223372036854775807);  view_6 = None\n",
            "    le_1: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_1 = None\n",
            "    slice_5: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_4, 2, 0, 9223372036854775807);  slice_4 = None\n",
            "    slice_6: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_5, 3, 1, 9223372036854775807);  slice_5 = None\n",
            "    sub_8: \"Sym(s12 - 5)\" = sym_size_int_3 - 5\n",
            "    sym_max_2: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_8, 0);  sub_8 = None\n",
            "    sub_9: \"Sym(5 - s12)\" = 5 - sym_size_int_3\n",
            "    sym_max_3: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_9, 0);  sub_9 = None\n",
            "    mul_11: \"Sym(2*s12)\" = 2 * sym_size_int_3;  sym_size_int_3 = None\n",
            "    add_4: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_3 + mul_11;  mul_11 = None\n",
            "    sub_10: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_4 - 1;  add_4 = None\n",
            "    gt_1: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_2 > 0;  gt_1 = None\n",
            "    pad_5: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg2_1, [0, 0, sym_max_2, sym_max_2, 0, 0]);  arg2_1 = sym_max_2 = None\n",
            "    slice_7: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_5, 1, sym_max_3, sub_10);  pad_5 = sym_max_3 = sub_10 = None\n",
            "    unsqueeze_6: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_7, 0);  slice_7 = None\n",
            "    matmul_3: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_6, unsqueeze_6);  slice_6 = unsqueeze_6 = None\n",
            "    add_5: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_2, matmul_3);  matmul_2 = matmul_3 = None\n",
            "    transpose_6: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_5, 2, 3)\n",
            "    sym_numel_default: \"Sym(192*s12*s31)\" = torch.ops.aten.sym_numel.default(transpose_6)\n",
            "    eq_2: \"Sym(False)\" = sym_numel_default == 0;  eq_2 = None\n",
            "    eq_3: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1;  eq_3 = None\n",
            "    eq_4: \"Sym(False)\" = sym_numel_default == 0\n",
            "    eq_5: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1\n",
            "    or_: \"Sym(Eq(s12, 1))\" = eq_5 | False;  eq_5 = None\n",
            "    and_: \"Sym(Eq(s12, 1))\" = True & or_;  or_ = None\n",
            "    eq_6: \"Sym(Eq(s12, 1))\" = 1 == sym_size_int_4\n",
            "    or__1: \"Sym(Eq(s12, 1))\" = False | eq_6;  eq_6 = None\n",
            "    and__1: \"Sym(Eq(s12, 1))\" = and_ & or__1;  and_ = or__1 = None\n",
            "    mul_12: \"Sym(96*s12)\" = sym_size_int_4 * 96\n",
            "    sym_stride_int: \"Sym(96*s12)\" = torch.ops.aten.sym_stride.int(add_5, 1)\n",
            "    eq_7: \"Sym(True)\" = sym_stride_int == mul_12;  sym_stride_int = None\n",
            "    or__2: \"Sym(True)\" = False | eq_7;  eq_7 = None\n",
            "    and__2: \"Sym(Eq(s12, 1))\" = and__1 & or__2;  and__1 = or__2 = None\n",
            "    mul_13: \"Sym(192*s12)\" = mul_12 * 2;  mul_12 = None\n",
            "    eq_8: \"Sym(Eq(s31, 1))\" = sym_size_int_5 == 1\n",
            "    sym_stride_int_1: \"Sym(192*s12)\" = torch.ops.aten.sym_stride.int(add_5, 0);  add_5 = None\n",
            "    eq_9: \"Sym(True)\" = sym_stride_int_1 == mul_13;  sym_stride_int_1 = None\n",
            "    or__3: \"Sym(True)\" = eq_8 | eq_9;  eq_8 = eq_9 = None\n",
            "    and__3: \"Sym(Eq(s12, 1))\" = and__2 & or__3;  and__2 = or__3 = None\n",
            "    mul_14: \"Sym(192*s12*s31)\" = mul_13 * sym_size_int_5;  mul_13 = mul_14 = None\n",
            "    or__4: \"Sym(Eq(s12, 1))\" = and__3 | eq_4;  and__3 = eq_4 = or__4 = None\n",
            "    eq_10: \"Sym(False)\" = sym_numel_default == 0;  sym_numel_default = eq_10 = None\n",
            "    eq_11: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1;  eq_11 = None\n",
            "    contiguous: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_6);  transpose_6 = None\n",
            "    view_7: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous, [sym_size_int_1, 192, sym_size_int_2]);  contiguous = sym_size_int_2 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_3: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_7, arg9_1, arg10_1);  view_7 = arg9_1 = arg10_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_1: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_3, 0.1, False);  conv1d_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_6: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(mul_3, dropout_1);  mul_3 = dropout_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_7: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_6, 1, -1);  add_6 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_7, [192], arg61_1, arg62_1);  transpose_7 = arg61_1 = arg62_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_8: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm, 1, -1);  layer_norm = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_15: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_8, type_as)\n",
            "    pad_6: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_15, [1, 1, 0, 0, 0, 0]);  mul_15 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_4: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_6, arg73_1, arg74_1);  pad_6 = arg73_1 = arg74_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_4);  conv1d_4 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_2: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu, 0.1, False);  relu = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_16: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_2, type_as);  dropout_2 = None\n",
            "    pad_7: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_16, [1, 1, 0, 0, 0, 0]);  mul_16 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_5: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_7, arg75_1, arg76_1);  pad_7 = arg75_1 = arg76_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_17: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_5, type_as);  conv1d_5 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_3: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_17, 0.1, False);  mul_17 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_7: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_8, dropout_3);  transpose_8 = dropout_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_9: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_7, 1, -1);  add_7 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_1: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_9, [192], arg97_1, arg98_1);  transpose_9 = arg97_1 = arg98_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_10: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_1, 1, -1);  layer_norm_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_6: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_10, arg13_1, arg14_1);  arg13_1 = arg14_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_7: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_10, arg15_1, arg16_1);  arg15_1 = arg16_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_8: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_10, arg17_1, arg18_1);  arg17_1 = arg18_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_6: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_6, 2)\n",
            "    view_8: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_6, [sym_size_int_1, 2, 96, sym_size_int_6]);  conv1d_6 = None\n",
            "    transpose_11: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_8, 2, 3);  view_8 = None\n",
            "    sym_size_int_7: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_7, 2)\n",
            "    view_9: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_7, [sym_size_int_1, 2, 96, sym_size_int_7]);  conv1d_7 = None\n",
            "    transpose_12: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_9, 2, 3);  view_9 = None\n",
            "    view_10: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_8, [sym_size_int_1, 2, 96, sym_size_int_7]);  conv1d_8 = None\n",
            "    transpose_13: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_10, 2, 3);  view_10 = None\n",
            "    div_2: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_11, 9.797958971132712)\n",
            "    transpose_14: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_12, -2, -1);  transpose_12 = None\n",
            "    matmul_4: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_2, transpose_14);  div_2 = transpose_14 = None\n",
            "    eq_12: \"Sym(True)\" = sym_size_int_7 == sym_size_int_6;  eq_12 = None\n",
            "    sub_11: \"Sym(s12 - 5)\" = sym_size_int_7 - 5\n",
            "    sym_max_4: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_11, 0);  sub_11 = None\n",
            "    sub_12: \"Sym(5 - s12)\" = 5 - sym_size_int_7\n",
            "    sym_max_5: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_12, 0);  sub_12 = None\n",
            "    mul_18: \"Sym(2*s12)\" = 2 * sym_size_int_7\n",
            "    add_8: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_5 + mul_18;  mul_18 = None\n",
            "    sub_13: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_8 - 1;  add_8 = None\n",
            "    gt_2: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_4 > 0;  gt_2 = None\n",
            "    pad_8: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg11_1, [0, 0, sym_max_4, sym_max_4, 0, 0]);  arg11_1 = sym_max_4 = None\n",
            "    slice_8: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_8, 1, sym_max_5, sub_13);  pad_8 = sym_max_5 = sub_13 = None\n",
            "    div_3: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_11, 9.797958971132712);  transpose_11 = None\n",
            "    unsqueeze_7: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_8, 0);  slice_8 = None\n",
            "    transpose_15: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_7, -2, -1);  unsqueeze_7 = None\n",
            "    matmul_5: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_3, transpose_15);  div_3 = transpose_15 = None\n",
            "    pad_9: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_5, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_5 = None\n",
            "    mul_19: \"Sym(2*s12)\" = sym_size_int_6 * 2\n",
            "    mul_20: \"Sym(2*s12**2)\" = mul_19 * sym_size_int_6;  mul_19 = None\n",
            "    view_11: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_9, [sym_size_int_1, 2, mul_20]);  pad_9 = mul_20 = None\n",
            "    sub_14: \"Sym(s12 - 1)\" = sym_size_int_6 - 1\n",
            "    pad_10: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_11, [0, sub_14, 0, 0, 0, 0]);  view_11 = sub_14 = None\n",
            "    add_9: \"Sym(s12 + 1)\" = sym_size_int_6 + 1\n",
            "    mul_21: \"Sym(2*s12)\" = 2 * sym_size_int_6\n",
            "    sub_15: \"Sym(2*s12 - 1)\" = mul_21 - 1;  mul_21 = None\n",
            "    view_12: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_10, [sym_size_int_1, 2, add_9, sub_15]);  pad_10 = add_9 = sub_15 = None\n",
            "    sub_16: \"Sym(s12 - 1)\" = sym_size_int_6 - 1\n",
            "    slice_9: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_12, 2, None, sym_size_int_6);  view_12 = None\n",
            "    slice_10: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_9, 3, sub_16);  slice_9 = sub_16 = None\n",
            "    add_10: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_4, slice_10);  matmul_4 = slice_10 = None\n",
            "    eq_13: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill_1: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_10, eq_13, -10000.0);  add_10 = eq_13 = None\n",
            "    softmax_1: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_1, -1);  masked_fill_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_4: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_1, 0.1, False);  softmax_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_6: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_4, transpose_13);  transpose_13 = None\n",
            "    sub_17: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_11: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_4, [0, sub_17, 0, 0, 0, 0, 0, 0]);  dropout_4 = sub_17 = None\n",
            "    mul_22: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_18: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_23: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_18;  sub_18 = None\n",
            "    add_11: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_22 + mul_23;  mul_22 = mul_23 = None\n",
            "    view_13: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_11, [sym_size_int_5, 2, add_11]);  pad_11 = add_11 = None\n",
            "    pad_12: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_13, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_13 = None\n",
            "    mul_24: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_14: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_12, [sym_size_int_5, 2, sym_size_int_4, mul_24]);  pad_12 = mul_24 = None\n",
            "    le_2: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le_2 = None\n",
            "    slice_11: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_14, 0, 0, 9223372036854775807);  view_14 = None\n",
            "    le_3: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_3 = None\n",
            "    slice_12: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_11, 2, 0, 9223372036854775807);  slice_11 = None\n",
            "    slice_13: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_12, 3, 1, 9223372036854775807);  slice_12 = None\n",
            "    sub_19: \"Sym(s12 - 5)\" = sym_size_int_7 - 5\n",
            "    sym_max_6: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_19, 0);  sub_19 = None\n",
            "    sub_20: \"Sym(5 - s12)\" = 5 - sym_size_int_7\n",
            "    sym_max_7: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_20, 0);  sub_20 = None\n",
            "    mul_25: \"Sym(2*s12)\" = 2 * sym_size_int_7;  sym_size_int_7 = None\n",
            "    add_12: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_7 + mul_25;  mul_25 = None\n",
            "    sub_21: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_12 - 1;  add_12 = None\n",
            "    gt_3: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_6 > 0;  gt_3 = None\n",
            "    pad_13: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg12_1, [0, 0, sym_max_6, sym_max_6, 0, 0]);  arg12_1 = sym_max_6 = None\n",
            "    slice_14: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_13, 1, sym_max_7, sub_21);  pad_13 = sym_max_7 = sub_21 = None\n",
            "    unsqueeze_8: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_14, 0);  slice_14 = None\n",
            "    matmul_7: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_13, unsqueeze_8);  slice_13 = unsqueeze_8 = None\n",
            "    add_13: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_6, matmul_7);  matmul_6 = matmul_7 = None\n",
            "    transpose_16: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_13, 2, 3);  add_13 = None\n",
            "    contiguous_1: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_16);  transpose_16 = None\n",
            "    view_15: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_1, [sym_size_int_1, 192, sym_size_int_6]);  contiguous_1 = sym_size_int_6 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_9: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_15, arg19_1, arg20_1);  view_15 = arg19_1 = arg20_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_5: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_9, 0.1, False);  conv1d_9 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_14: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_10, dropout_5);  transpose_10 = dropout_5 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_17: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_14, 1, -1);  add_14 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_2: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_17, [192], arg63_1, arg64_1);  transpose_17 = arg63_1 = arg64_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_18: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_2, 1, -1);  layer_norm_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_26: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_18, type_as)\n",
            "    pad_14: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_26, [1, 1, 0, 0, 0, 0]);  mul_26 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_10: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_14, arg77_1, arg78_1);  pad_14 = arg77_1 = arg78_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_1: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_10);  conv1d_10 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_6: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_1, 0.1, False);  relu_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_27: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_6, type_as);  dropout_6 = None\n",
            "    pad_15: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_27, [1, 1, 0, 0, 0, 0]);  mul_27 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_11: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_15, arg79_1, arg80_1);  pad_15 = arg79_1 = arg80_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_28: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_11, type_as);  conv1d_11 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_7: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_28, 0.1, False);  mul_28 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_15: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_18, dropout_7);  transpose_18 = dropout_7 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_19: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_15, 1, -1);  add_15 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_3: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_19, [192], arg99_1, arg100_1);  transpose_19 = arg99_1 = arg100_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_20: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_3, 1, -1);  layer_norm_3 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_12: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_20, arg23_1, arg24_1);  arg23_1 = arg24_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_13: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_20, arg25_1, arg26_1);  arg25_1 = arg26_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_14: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_20, arg27_1, arg28_1);  arg27_1 = arg28_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_8: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_12, 2)\n",
            "    view_16: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_12, [sym_size_int_1, 2, 96, sym_size_int_8]);  conv1d_12 = None\n",
            "    transpose_21: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_16, 2, 3);  view_16 = None\n",
            "    sym_size_int_9: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_13, 2)\n",
            "    view_17: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_13, [sym_size_int_1, 2, 96, sym_size_int_9]);  conv1d_13 = None\n",
            "    transpose_22: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_17, 2, 3);  view_17 = None\n",
            "    view_18: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_14, [sym_size_int_1, 2, 96, sym_size_int_9]);  conv1d_14 = None\n",
            "    transpose_23: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_18, 2, 3);  view_18 = None\n",
            "    div_4: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_21, 9.797958971132712)\n",
            "    transpose_24: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_22, -2, -1);  transpose_22 = None\n",
            "    matmul_8: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_4, transpose_24);  div_4 = transpose_24 = None\n",
            "    eq_14: \"Sym(True)\" = sym_size_int_9 == sym_size_int_8;  eq_14 = None\n",
            "    sub_22: \"Sym(s12 - 5)\" = sym_size_int_9 - 5\n",
            "    sym_max_8: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_22, 0);  sub_22 = None\n",
            "    sub_23: \"Sym(5 - s12)\" = 5 - sym_size_int_9\n",
            "    sym_max_9: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_23, 0);  sub_23 = None\n",
            "    mul_29: \"Sym(2*s12)\" = 2 * sym_size_int_9\n",
            "    add_16: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_9 + mul_29;  mul_29 = None\n",
            "    sub_24: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_16 - 1;  add_16 = None\n",
            "    gt_4: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_8 > 0;  gt_4 = None\n",
            "    pad_16: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg21_1, [0, 0, sym_max_8, sym_max_8, 0, 0]);  arg21_1 = sym_max_8 = None\n",
            "    slice_15: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_16, 1, sym_max_9, sub_24);  pad_16 = sym_max_9 = sub_24 = None\n",
            "    div_5: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_21, 9.797958971132712);  transpose_21 = None\n",
            "    unsqueeze_9: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_15, 0);  slice_15 = None\n",
            "    transpose_25: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_9, -2, -1);  unsqueeze_9 = None\n",
            "    matmul_9: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_5, transpose_25);  div_5 = transpose_25 = None\n",
            "    pad_17: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_9, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_9 = None\n",
            "    mul_30: \"Sym(2*s12)\" = sym_size_int_8 * 2\n",
            "    mul_31: \"Sym(2*s12**2)\" = mul_30 * sym_size_int_8;  mul_30 = None\n",
            "    view_19: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_17, [sym_size_int_1, 2, mul_31]);  pad_17 = mul_31 = None\n",
            "    sub_25: \"Sym(s12 - 1)\" = sym_size_int_8 - 1\n",
            "    pad_18: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_19, [0, sub_25, 0, 0, 0, 0]);  view_19 = sub_25 = None\n",
            "    add_17: \"Sym(s12 + 1)\" = sym_size_int_8 + 1\n",
            "    mul_32: \"Sym(2*s12)\" = 2 * sym_size_int_8\n",
            "    sub_26: \"Sym(2*s12 - 1)\" = mul_32 - 1;  mul_32 = None\n",
            "    view_20: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_18, [sym_size_int_1, 2, add_17, sub_26]);  pad_18 = add_17 = sub_26 = None\n",
            "    sub_27: \"Sym(s12 - 1)\" = sym_size_int_8 - 1\n",
            "    slice_16: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_20, 2, None, sym_size_int_8);  view_20 = None\n",
            "    slice_17: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_16, 3, sub_27);  slice_16 = sub_27 = None\n",
            "    add_18: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_8, slice_17);  matmul_8 = slice_17 = None\n",
            "    eq_15: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill_2: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_18, eq_15, -10000.0);  add_18 = eq_15 = None\n",
            "    softmax_2: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_2, -1);  masked_fill_2 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_8: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_2, 0.1, False);  softmax_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_10: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_8, transpose_23);  transpose_23 = None\n",
            "    sub_28: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_19: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_8, [0, sub_28, 0, 0, 0, 0, 0, 0]);  dropout_8 = sub_28 = None\n",
            "    mul_33: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_29: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_34: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_29;  sub_29 = None\n",
            "    add_19: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_33 + mul_34;  mul_33 = mul_34 = None\n",
            "    view_21: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_19, [sym_size_int_5, 2, add_19]);  pad_19 = add_19 = None\n",
            "    pad_20: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_21, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_21 = None\n",
            "    mul_35: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_22: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_20, [sym_size_int_5, 2, sym_size_int_4, mul_35]);  pad_20 = mul_35 = None\n",
            "    le_4: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le_4 = None\n",
            "    slice_18: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_22, 0, 0, 9223372036854775807);  view_22 = None\n",
            "    le_5: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_5 = None\n",
            "    slice_19: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_18, 2, 0, 9223372036854775807);  slice_18 = None\n",
            "    slice_20: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_19, 3, 1, 9223372036854775807);  slice_19 = None\n",
            "    sub_30: \"Sym(s12 - 5)\" = sym_size_int_9 - 5\n",
            "    sym_max_10: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_30, 0);  sub_30 = None\n",
            "    sub_31: \"Sym(5 - s12)\" = 5 - sym_size_int_9\n",
            "    sym_max_11: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_31, 0);  sub_31 = None\n",
            "    mul_36: \"Sym(2*s12)\" = 2 * sym_size_int_9;  sym_size_int_9 = None\n",
            "    add_20: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_11 + mul_36;  mul_36 = None\n",
            "    sub_32: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_20 - 1;  add_20 = None\n",
            "    gt_5: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_10 > 0;  gt_5 = None\n",
            "    pad_21: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg22_1, [0, 0, sym_max_10, sym_max_10, 0, 0]);  arg22_1 = sym_max_10 = None\n",
            "    slice_21: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_21, 1, sym_max_11, sub_32);  pad_21 = sym_max_11 = sub_32 = None\n",
            "    unsqueeze_10: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_21, 0);  slice_21 = None\n",
            "    matmul_11: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_20, unsqueeze_10);  slice_20 = unsqueeze_10 = None\n",
            "    add_21: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_10, matmul_11);  matmul_10 = matmul_11 = None\n",
            "    transpose_26: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_21, 2, 3);  add_21 = None\n",
            "    contiguous_2: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_26);  transpose_26 = None\n",
            "    view_23: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_2, [sym_size_int_1, 192, sym_size_int_8]);  contiguous_2 = sym_size_int_8 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_15: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_23, arg29_1, arg30_1);  view_23 = arg29_1 = arg30_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_9: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_15, 0.1, False);  conv1d_15 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_22: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_20, dropout_9);  transpose_20 = dropout_9 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_27: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_22, 1, -1);  add_22 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_4: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_27, [192], arg65_1, arg66_1);  transpose_27 = arg65_1 = arg66_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_28: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_4, 1, -1);  layer_norm_4 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_37: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_28, type_as)\n",
            "    pad_22: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_37, [1, 1, 0, 0, 0, 0]);  mul_37 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_16: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_22, arg81_1, arg82_1);  pad_22 = arg81_1 = arg82_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_2: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_16);  conv1d_16 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_10: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_2, 0.1, False);  relu_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_38: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_10, type_as);  dropout_10 = None\n",
            "    pad_23: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_38, [1, 1, 0, 0, 0, 0]);  mul_38 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_17: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_23, arg83_1, arg84_1);  pad_23 = arg83_1 = arg84_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_39: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_17, type_as);  conv1d_17 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_11: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_39, 0.1, False);  mul_39 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_23: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_28, dropout_11);  transpose_28 = dropout_11 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_29: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_23, 1, -1);  add_23 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_5: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_29, [192], arg101_1, arg102_1);  transpose_29 = arg101_1 = arg102_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_30: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_5, 1, -1);  layer_norm_5 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_18: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_30, arg33_1, arg34_1);  arg33_1 = arg34_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_19: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_30, arg35_1, arg36_1);  arg35_1 = arg36_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_20: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_30, arg37_1, arg38_1);  arg37_1 = arg38_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_10: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_18, 2)\n",
            "    view_24: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_18, [sym_size_int_1, 2, 96, sym_size_int_10]);  conv1d_18 = None\n",
            "    transpose_31: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_24, 2, 3);  view_24 = None\n",
            "    sym_size_int_11: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_19, 2)\n",
            "    view_25: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_19, [sym_size_int_1, 2, 96, sym_size_int_11]);  conv1d_19 = None\n",
            "    transpose_32: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_25, 2, 3);  view_25 = None\n",
            "    view_26: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_20, [sym_size_int_1, 2, 96, sym_size_int_11]);  conv1d_20 = None\n",
            "    transpose_33: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_26, 2, 3);  view_26 = None\n",
            "    div_6: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_31, 9.797958971132712)\n",
            "    transpose_34: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_32, -2, -1);  transpose_32 = None\n",
            "    matmul_12: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_6, transpose_34);  div_6 = transpose_34 = None\n",
            "    eq_16: \"Sym(True)\" = sym_size_int_11 == sym_size_int_10;  eq_16 = None\n",
            "    sub_33: \"Sym(s12 - 5)\" = sym_size_int_11 - 5\n",
            "    sym_max_12: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_33, 0);  sub_33 = None\n",
            "    sub_34: \"Sym(5 - s12)\" = 5 - sym_size_int_11\n",
            "    sym_max_13: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_34, 0);  sub_34 = None\n",
            "    mul_40: \"Sym(2*s12)\" = 2 * sym_size_int_11\n",
            "    add_24: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_13 + mul_40;  mul_40 = None\n",
            "    sub_35: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_24 - 1;  add_24 = None\n",
            "    gt_6: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_12 > 0;  gt_6 = None\n",
            "    pad_24: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg31_1, [0, 0, sym_max_12, sym_max_12, 0, 0]);  arg31_1 = sym_max_12 = None\n",
            "    slice_22: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_24, 1, sym_max_13, sub_35);  pad_24 = sym_max_13 = sub_35 = None\n",
            "    div_7: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_31, 9.797958971132712);  transpose_31 = None\n",
            "    unsqueeze_11: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_22, 0);  slice_22 = None\n",
            "    transpose_35: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_11, -2, -1);  unsqueeze_11 = None\n",
            "    matmul_13: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_7, transpose_35);  div_7 = transpose_35 = None\n",
            "    pad_25: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_13, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_13 = None\n",
            "    mul_41: \"Sym(2*s12)\" = sym_size_int_10 * 2\n",
            "    mul_42: \"Sym(2*s12**2)\" = mul_41 * sym_size_int_10;  mul_41 = None\n",
            "    view_27: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_25, [sym_size_int_1, 2, mul_42]);  pad_25 = mul_42 = None\n",
            "    sub_36: \"Sym(s12 - 1)\" = sym_size_int_10 - 1\n",
            "    pad_26: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_27, [0, sub_36, 0, 0, 0, 0]);  view_27 = sub_36 = None\n",
            "    add_25: \"Sym(s12 + 1)\" = sym_size_int_10 + 1\n",
            "    mul_43: \"Sym(2*s12)\" = 2 * sym_size_int_10\n",
            "    sub_37: \"Sym(2*s12 - 1)\" = mul_43 - 1;  mul_43 = None\n",
            "    view_28: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_26, [sym_size_int_1, 2, add_25, sub_37]);  pad_26 = add_25 = sub_37 = None\n",
            "    sub_38: \"Sym(s12 - 1)\" = sym_size_int_10 - 1\n",
            "    slice_23: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_28, 2, None, sym_size_int_10);  view_28 = None\n",
            "    slice_24: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_23, 3, sub_38);  slice_23 = sub_38 = None\n",
            "    add_26: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_12, slice_24);  matmul_12 = slice_24 = None\n",
            "    eq_17: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill_3: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_26, eq_17, -10000.0);  add_26 = eq_17 = None\n",
            "    softmax_3: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_3, -1);  masked_fill_3 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_12: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_3, 0.1, False);  softmax_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_14: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_12, transpose_33);  transpose_33 = None\n",
            "    sub_39: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_27: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_12, [0, sub_39, 0, 0, 0, 0, 0, 0]);  dropout_12 = sub_39 = None\n",
            "    mul_44: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_40: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_45: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_40;  sub_40 = None\n",
            "    add_27: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_44 + mul_45;  mul_44 = mul_45 = None\n",
            "    view_29: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_27, [sym_size_int_5, 2, add_27]);  pad_27 = add_27 = None\n",
            "    pad_28: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_29, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_29 = None\n",
            "    mul_46: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_30: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_28, [sym_size_int_5, 2, sym_size_int_4, mul_46]);  pad_28 = mul_46 = None\n",
            "    le_6: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le_6 = None\n",
            "    slice_25: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_30, 0, 0, 9223372036854775807);  view_30 = None\n",
            "    le_7: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_7 = None\n",
            "    slice_26: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_25, 2, 0, 9223372036854775807);  slice_25 = None\n",
            "    slice_27: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_26, 3, 1, 9223372036854775807);  slice_26 = None\n",
            "    sub_41: \"Sym(s12 - 5)\" = sym_size_int_11 - 5\n",
            "    sym_max_14: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_41, 0);  sub_41 = None\n",
            "    sub_42: \"Sym(5 - s12)\" = 5 - sym_size_int_11\n",
            "    sym_max_15: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_42, 0);  sub_42 = None\n",
            "    mul_47: \"Sym(2*s12)\" = 2 * sym_size_int_11;  sym_size_int_11 = None\n",
            "    add_28: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_15 + mul_47;  mul_47 = None\n",
            "    sub_43: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_28 - 1;  add_28 = None\n",
            "    gt_7: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_14 > 0;  gt_7 = None\n",
            "    pad_29: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg32_1, [0, 0, sym_max_14, sym_max_14, 0, 0]);  arg32_1 = sym_max_14 = None\n",
            "    slice_28: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_29, 1, sym_max_15, sub_43);  pad_29 = sym_max_15 = sub_43 = None\n",
            "    unsqueeze_12: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_28, 0);  slice_28 = None\n",
            "    matmul_15: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_27, unsqueeze_12);  slice_27 = unsqueeze_12 = None\n",
            "    add_29: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_14, matmul_15);  matmul_14 = matmul_15 = None\n",
            "    transpose_36: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_29, 2, 3)\n",
            "    sym_numel_default_1: \"Sym(192*s12*s31)\" = torch.ops.aten.sym_numel.default(transpose_36)\n",
            "    eq_18: \"Sym(False)\" = sym_numel_default_1 == 0;  eq_18 = None\n",
            "    eq_19: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1;  eq_19 = None\n",
            "    eq_20: \"Sym(False)\" = sym_numel_default_1 == 0\n",
            "    eq_21: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1\n",
            "    or__5: \"Sym(Eq(s12, 1))\" = eq_21 | False;  eq_21 = None\n",
            "    and__4: \"Sym(Eq(s12, 1))\" = True & or__5;  or__5 = None\n",
            "    eq_22: \"Sym(Eq(s12, 1))\" = 1 == sym_size_int_4\n",
            "    or__6: \"Sym(Eq(s12, 1))\" = False | eq_22;  eq_22 = None\n",
            "    and__5: \"Sym(Eq(s12, 1))\" = and__4 & or__6;  and__4 = or__6 = None\n",
            "    mul_48: \"Sym(96*s12)\" = sym_size_int_4 * 96\n",
            "    sym_stride_int_2: \"Sym(96*s12)\" = torch.ops.aten.sym_stride.int(add_29, 1)\n",
            "    eq_23: \"Sym(True)\" = sym_stride_int_2 == mul_48;  sym_stride_int_2 = None\n",
            "    or__7: \"Sym(True)\" = False | eq_23;  eq_23 = None\n",
            "    and__6: \"Sym(Eq(s12, 1))\" = and__5 & or__7;  and__5 = or__7 = None\n",
            "    mul_49: \"Sym(192*s12)\" = mul_48 * 2;  mul_48 = None\n",
            "    eq_24: \"Sym(Eq(s31, 1))\" = sym_size_int_5 == 1\n",
            "    sym_stride_int_3: \"Sym(192*s12)\" = torch.ops.aten.sym_stride.int(add_29, 0);  add_29 = None\n",
            "    eq_25: \"Sym(True)\" = sym_stride_int_3 == mul_49;  sym_stride_int_3 = None\n",
            "    or__8: \"Sym(True)\" = eq_24 | eq_25;  eq_24 = eq_25 = None\n",
            "    and__7: \"Sym(Eq(s12, 1))\" = and__6 & or__8;  and__6 = or__8 = None\n",
            "    mul_50: \"Sym(192*s12*s31)\" = mul_49 * sym_size_int_5;  mul_49 = mul_50 = None\n",
            "    or__9: \"Sym(Eq(s12, 1))\" = and__7 | eq_20;  and__7 = eq_20 = or__9 = None\n",
            "    eq_26: \"Sym(False)\" = sym_numel_default_1 == 0;  sym_numel_default_1 = eq_26 = None\n",
            "    eq_27: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1;  eq_27 = None\n",
            "    contiguous_3: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_36);  transpose_36 = None\n",
            "    view_31: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_3, [sym_size_int_1, 192, sym_size_int_10]);  contiguous_3 = sym_size_int_10 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_21: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_31, arg39_1, arg40_1);  view_31 = arg39_1 = arg40_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_13: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_21, 0.1, False);  conv1d_21 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_30: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_30, dropout_13);  transpose_30 = dropout_13 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_37: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_30, 1, -1);  add_30 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_6: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_37, [192], arg67_1, arg68_1);  transpose_37 = arg67_1 = arg68_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_38: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_6, 1, -1);  layer_norm_6 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_51: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_38, type_as)\n",
            "    pad_30: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_51, [1, 1, 0, 0, 0, 0]);  mul_51 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_22: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_30, arg85_1, arg86_1);  pad_30 = arg85_1 = arg86_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_3: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_22);  conv1d_22 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_14: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_3, 0.1, False);  relu_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_52: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_14, type_as);  dropout_14 = None\n",
            "    pad_31: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_52, [1, 1, 0, 0, 0, 0]);  mul_52 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_23: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_31, arg87_1, arg88_1);  pad_31 = arg87_1 = arg88_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_53: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_23, type_as);  conv1d_23 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_15: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_53, 0.1, False);  mul_53 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_31: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_38, dropout_15);  transpose_38 = dropout_15 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_39: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_31, 1, -1);  add_31 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_7: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_39, [192], arg103_1, arg104_1);  transpose_39 = arg103_1 = arg104_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_40: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_7, 1, -1);  layer_norm_7 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_24: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_40, arg43_1, arg44_1);  arg43_1 = arg44_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_25: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_40, arg45_1, arg46_1);  arg45_1 = arg46_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_26: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_40, arg47_1, arg48_1);  arg47_1 = arg48_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_12: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_24, 2)\n",
            "    view_32: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_24, [sym_size_int_1, 2, 96, sym_size_int_12]);  conv1d_24 = None\n",
            "    transpose_41: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_32, 2, 3);  view_32 = None\n",
            "    sym_size_int_13: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_25, 2)\n",
            "    view_33: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_25, [sym_size_int_1, 2, 96, sym_size_int_13]);  conv1d_25 = None\n",
            "    transpose_42: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_33, 2, 3);  view_33 = None\n",
            "    view_34: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_26, [sym_size_int_1, 2, 96, sym_size_int_13]);  conv1d_26 = None\n",
            "    transpose_43: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_34, 2, 3);  view_34 = None\n",
            "    div_8: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_41, 9.797958971132712)\n",
            "    transpose_44: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_42, -2, -1);  transpose_42 = None\n",
            "    matmul_16: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_8, transpose_44);  div_8 = transpose_44 = None\n",
            "    eq_28: \"Sym(True)\" = sym_size_int_13 == sym_size_int_12;  eq_28 = None\n",
            "    sub_44: \"Sym(s12 - 5)\" = sym_size_int_13 - 5\n",
            "    sym_max_16: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_44, 0);  sub_44 = None\n",
            "    sub_45: \"Sym(5 - s12)\" = 5 - sym_size_int_13\n",
            "    sym_max_17: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_45, 0);  sub_45 = None\n",
            "    mul_54: \"Sym(2*s12)\" = 2 * sym_size_int_13\n",
            "    add_32: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_17 + mul_54;  mul_54 = None\n",
            "    sub_46: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_32 - 1;  add_32 = None\n",
            "    gt_8: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_16 > 0;  gt_8 = None\n",
            "    pad_32: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg41_1, [0, 0, sym_max_16, sym_max_16, 0, 0]);  arg41_1 = sym_max_16 = None\n",
            "    slice_29: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_32, 1, sym_max_17, sub_46);  pad_32 = sym_max_17 = sub_46 = None\n",
            "    div_9: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_41, 9.797958971132712);  transpose_41 = None\n",
            "    unsqueeze_13: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_29, 0);  slice_29 = None\n",
            "    transpose_45: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_13, -2, -1);  unsqueeze_13 = None\n",
            "    matmul_17: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_9, transpose_45);  div_9 = transpose_45 = None\n",
            "    pad_33: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_17, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_17 = None\n",
            "    mul_55: \"Sym(2*s12)\" = sym_size_int_12 * 2\n",
            "    mul_56: \"Sym(2*s12**2)\" = mul_55 * sym_size_int_12;  mul_55 = None\n",
            "    view_35: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_33, [sym_size_int_1, 2, mul_56]);  pad_33 = mul_56 = None\n",
            "    sub_47: \"Sym(s12 - 1)\" = sym_size_int_12 - 1\n",
            "    pad_34: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_35, [0, sub_47, 0, 0, 0, 0]);  view_35 = sub_47 = None\n",
            "    add_33: \"Sym(s12 + 1)\" = sym_size_int_12 + 1\n",
            "    mul_57: \"Sym(2*s12)\" = 2 * sym_size_int_12\n",
            "    sub_48: \"Sym(2*s12 - 1)\" = mul_57 - 1;  mul_57 = None\n",
            "    view_36: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_34, [sym_size_int_1, 2, add_33, sub_48]);  pad_34 = add_33 = sub_48 = None\n",
            "    sub_49: \"Sym(s12 - 1)\" = sym_size_int_12 - 1\n",
            "    slice_30: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_36, 2, None, sym_size_int_12);  view_36 = None\n",
            "    slice_31: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_30, 3, sub_49);  slice_30 = sub_49 = None\n",
            "    add_34: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_16, slice_31);  matmul_16 = slice_31 = None\n",
            "    eq_29: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill_4: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_34, eq_29, -10000.0);  add_34 = eq_29 = None\n",
            "    softmax_4: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_4, -1);  masked_fill_4 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_16: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_4, 0.1, False);  softmax_4 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_18: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_16, transpose_43);  transpose_43 = None\n",
            "    sub_50: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_35: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_16, [0, sub_50, 0, 0, 0, 0, 0, 0]);  dropout_16 = sub_50 = None\n",
            "    mul_58: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_51: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_59: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_51;  sub_51 = None\n",
            "    add_35: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_58 + mul_59;  mul_58 = mul_59 = None\n",
            "    view_37: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_35, [sym_size_int_5, 2, add_35]);  pad_35 = add_35 = None\n",
            "    pad_36: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_37, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_37 = None\n",
            "    mul_60: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_38: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_36, [sym_size_int_5, 2, sym_size_int_4, mul_60]);  pad_36 = mul_60 = None\n",
            "    le_8: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le_8 = None\n",
            "    slice_32: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_38, 0, 0, 9223372036854775807);  view_38 = None\n",
            "    le_9: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_9 = None\n",
            "    slice_33: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_32, 2, 0, 9223372036854775807);  slice_32 = None\n",
            "    slice_34: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_33, 3, 1, 9223372036854775807);  slice_33 = None\n",
            "    sub_52: \"Sym(s12 - 5)\" = sym_size_int_13 - 5\n",
            "    sym_max_18: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_52, 0);  sub_52 = None\n",
            "    sub_53: \"Sym(5 - s12)\" = 5 - sym_size_int_13\n",
            "    sym_max_19: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_53, 0);  sub_53 = None\n",
            "    mul_61: \"Sym(2*s12)\" = 2 * sym_size_int_13;  sym_size_int_13 = None\n",
            "    add_36: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_19 + mul_61;  mul_61 = None\n",
            "    sub_54: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_36 - 1;  add_36 = None\n",
            "    gt_9: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_18 > 0;  gt_9 = None\n",
            "    pad_37: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg42_1, [0, 0, sym_max_18, sym_max_18, 0, 0]);  arg42_1 = sym_max_18 = None\n",
            "    slice_35: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_37, 1, sym_max_19, sub_54);  pad_37 = sym_max_19 = sub_54 = None\n",
            "    unsqueeze_14: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_35, 0);  slice_35 = None\n",
            "    matmul_19: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_34, unsqueeze_14);  slice_34 = unsqueeze_14 = None\n",
            "    add_37: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_18, matmul_19);  matmul_18 = matmul_19 = None\n",
            "    transpose_46: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_37, 2, 3);  add_37 = None\n",
            "    contiguous_4: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_46);  transpose_46 = None\n",
            "    view_39: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_4, [sym_size_int_1, 192, sym_size_int_12]);  contiguous_4 = sym_size_int_12 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_27: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_39, arg49_1, arg50_1);  view_39 = arg49_1 = arg50_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_17: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_27, 0.1, False);  conv1d_27 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_38: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_40, dropout_17);  transpose_40 = dropout_17 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_47: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_38, 1, -1);  add_38 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_8: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_47, [192], arg69_1, arg70_1);  transpose_47 = arg69_1 = arg70_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_48: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_8, 1, -1);  layer_norm_8 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_62: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_48, type_as)\n",
            "    pad_38: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_62, [1, 1, 0, 0, 0, 0]);  mul_62 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_28: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_38, arg89_1, arg90_1);  pad_38 = arg89_1 = arg90_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_4: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_28);  conv1d_28 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_18: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_4, 0.1, False);  relu_4 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_63: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_18, type_as);  dropout_18 = None\n",
            "    pad_39: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_63, [1, 1, 0, 0, 0, 0]);  mul_63 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_29: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_39, arg91_1, arg92_1);  pad_39 = arg91_1 = arg92_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_64: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_29, type_as);  conv1d_29 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_19: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_64, 0.1, False);  mul_64 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_39: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_48, dropout_19);  transpose_48 = dropout_19 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_49: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_39, 1, -1);  add_39 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_9: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_49, [192], arg105_1, arg106_1);  transpose_49 = arg105_1 = arg106_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_50: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_9, 1, -1);  layer_norm_9 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_30: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_50, arg53_1, arg54_1);  arg53_1 = arg54_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_31: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_50, arg55_1, arg56_1);  arg55_1 = arg56_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_32: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_50, arg57_1, arg58_1);  arg57_1 = arg58_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_14: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_30, 2)\n",
            "    view_40: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_30, [sym_size_int_1, 2, 96, sym_size_int_14]);  conv1d_30 = None\n",
            "    transpose_51: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_40, 2, 3);  view_40 = None\n",
            "    sym_size_int_15: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_31, 2)\n",
            "    view_41: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_31, [sym_size_int_1, 2, 96, sym_size_int_15]);  conv1d_31 = None\n",
            "    transpose_52: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_41, 2, 3);  view_41 = None\n",
            "    view_42: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_32, [sym_size_int_1, 2, 96, sym_size_int_15]);  conv1d_32 = None\n",
            "    transpose_53: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_42, 2, 3);  view_42 = None\n",
            "    div_10: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_51, 9.797958971132712)\n",
            "    transpose_54: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_52, -2, -1);  transpose_52 = None\n",
            "    matmul_20: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_10, transpose_54);  div_10 = transpose_54 = None\n",
            "    eq_30: \"Sym(True)\" = sym_size_int_15 == sym_size_int_14;  eq_30 = None\n",
            "    sub_55: \"Sym(s12 - 5)\" = sym_size_int_15 - 5\n",
            "    sym_max_20: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_55, 0);  sub_55 = None\n",
            "    sub_56: \"Sym(5 - s12)\" = 5 - sym_size_int_15\n",
            "    sym_max_21: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_56, 0);  sub_56 = None\n",
            "    mul_65: \"Sym(2*s12)\" = 2 * sym_size_int_15\n",
            "    add_40: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_21 + mul_65;  mul_65 = None\n",
            "    sub_57: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_40 - 1;  add_40 = None\n",
            "    gt_10: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_20 > 0;  gt_10 = None\n",
            "    pad_40: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg51_1, [0, 0, sym_max_20, sym_max_20, 0, 0]);  arg51_1 = sym_max_20 = None\n",
            "    slice_36: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_40, 1, sym_max_21, sub_57);  pad_40 = sym_max_21 = sub_57 = None\n",
            "    div_11: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_51, 9.797958971132712);  transpose_51 = None\n",
            "    unsqueeze_15: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_36, 0);  slice_36 = None\n",
            "    transpose_55: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_15, -2, -1);  unsqueeze_15 = None\n",
            "    matmul_21: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_11, transpose_55);  div_11 = transpose_55 = None\n",
            "    pad_41: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_21, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_21 = None\n",
            "    mul_66: \"Sym(2*s12)\" = sym_size_int_14 * 2\n",
            "    mul_67: \"Sym(2*s12**2)\" = mul_66 * sym_size_int_14;  mul_66 = None\n",
            "    view_43: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_41, [sym_size_int_1, 2, mul_67]);  pad_41 = mul_67 = None\n",
            "    sub_58: \"Sym(s12 - 1)\" = sym_size_int_14 - 1\n",
            "    pad_42: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_43, [0, sub_58, 0, 0, 0, 0]);  view_43 = sub_58 = None\n",
            "    add_41: \"Sym(s12 + 1)\" = sym_size_int_14 + 1\n",
            "    mul_68: \"Sym(2*s12)\" = 2 * sym_size_int_14\n",
            "    sub_59: \"Sym(2*s12 - 1)\" = mul_68 - 1;  mul_68 = None\n",
            "    view_44: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_42, [sym_size_int_1, 2, add_41, sub_59]);  pad_42 = add_41 = sub_59 = None\n",
            "    sub_60: \"Sym(s12 - 1)\" = sym_size_int_14 - 1\n",
            "    slice_37: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_44, 2, None, sym_size_int_14);  view_44 = None\n",
            "    slice_38: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_37, 3, sub_60);  slice_37 = sub_60 = None\n",
            "    add_42: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_20, slice_38);  matmul_20 = slice_38 = None\n",
            "    eq_31: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0);  mul_2 = None\n",
            "    masked_fill_5: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_42, eq_31, -10000.0);  add_42 = eq_31 = None\n",
            "    softmax_5: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_5, -1);  masked_fill_5 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_20: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_5, 0.1, False);  softmax_5 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_22: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_20, transpose_53);  transpose_53 = None\n",
            "    sub_61: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_43: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_20, [0, sub_61, 0, 0, 0, 0, 0, 0]);  dropout_20 = sub_61 = None\n",
            "    mul_69: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_62: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_70: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_62;  sub_62 = None\n",
            "    add_43: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_69 + mul_70;  mul_69 = mul_70 = None\n",
            "    view_45: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_43, [sym_size_int_5, 2, add_43]);  pad_43 = add_43 = None\n",
            "    pad_44: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_45, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_45 = None\n",
            "    mul_71: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_46: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_44, [sym_size_int_5, 2, sym_size_int_4, mul_71]);  pad_44 = mul_71 = None\n",
            "    le_10: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  sym_size_int_5 = le_10 = None\n",
            "    slice_39: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_46, 0, 0, 9223372036854775807);  view_46 = None\n",
            "    le_11: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  sym_size_int_4 = le_11 = None\n",
            "    slice_40: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_39, 2, 0, 9223372036854775807);  slice_39 = None\n",
            "    slice_41: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_40, 3, 1, 9223372036854775807);  slice_40 = None\n",
            "    sub_63: \"Sym(s12 - 5)\" = sym_size_int_15 - 5\n",
            "    sym_max_22: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_63, 0);  sub_63 = None\n",
            "    sub_64: \"Sym(5 - s12)\" = 5 - sym_size_int_15\n",
            "    sym_max_23: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_64, 0);  sub_64 = None\n",
            "    mul_72: \"Sym(2*s12)\" = 2 * sym_size_int_15;  sym_size_int_15 = None\n",
            "    add_44: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_23 + mul_72;  mul_72 = None\n",
            "    sub_65: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_44 - 1;  add_44 = None\n",
            "    gt_11: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_22 > 0;  gt_11 = None\n",
            "    pad_45: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg52_1, [0, 0, sym_max_22, sym_max_22, 0, 0]);  arg52_1 = sym_max_22 = None\n",
            "    slice_42: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_45, 1, sym_max_23, sub_65);  pad_45 = sym_max_23 = sub_65 = None\n",
            "    unsqueeze_16: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_42, 0);  slice_42 = None\n",
            "    matmul_23: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_41, unsqueeze_16);  slice_41 = unsqueeze_16 = None\n",
            "    add_45: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_22, matmul_23);  matmul_22 = matmul_23 = None\n",
            "    transpose_56: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_45, 2, 3);  add_45 = None\n",
            "    contiguous_5: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_56);  transpose_56 = None\n",
            "    view_47: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_5, [sym_size_int_1, 192, sym_size_int_14]);  contiguous_5 = sym_size_int_14 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_33: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_47, arg59_1, arg60_1);  view_47 = arg59_1 = arg60_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_21: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_33, 0.1, False);  conv1d_33 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_46: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_50, dropout_21);  transpose_50 = dropout_21 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_57: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_46, 1, -1);  add_46 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_10: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_57, [192], arg71_1, arg72_1);  transpose_57 = arg71_1 = arg72_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_58: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_10, 1, -1);  layer_norm_10 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_73: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_58, type_as)\n",
            "    pad_46: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_73, [1, 1, 0, 0, 0, 0]);  mul_73 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_34: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_46, arg93_1, arg94_1);  pad_46 = arg93_1 = arg94_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_5: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_34);  conv1d_34 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_22: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_5, 0.1, False);  relu_5 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_74: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_22, type_as);  dropout_22 = None\n",
            "    pad_47: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_74, [1, 1, 0, 0, 0, 0]);  mul_74 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_35: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_47, arg95_1, arg96_1);  pad_47 = arg95_1 = arg96_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_75: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_35, type_as);  conv1d_35 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_23: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_75, 0.1, False);  mul_75 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_47: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_58, dropout_23);  transpose_58 = dropout_23 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_59: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_47, 1, -1);  add_47 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_11: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_59, [192], arg107_1, arg108_1);  transpose_59 = arg107_1 = arg108_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_60: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_11, 1, -1);  layer_norm_11 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:73 in forward, code: x = x * x_mask\n",
            "    mul_76: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_60, type_as);  transpose_60 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_36: \"f32[s31, 384, s12]\" = torch.ops.aten.conv1d.default(mul_76, arg109_1, arg110_1);  arg109_1 = arg110_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:206 in forward, code: stats = self.proj(x) * x_mask\n",
            "    mul_77: \"f32[s31, 384, s12]\" = torch.ops.aten.mul.Tensor(conv1d_36, type_as);  conv1d_36 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:208 in forward, code: m, logs = torch.split(stats, self.out_channels, dim=1)\n",
            "    split = torch.ops.aten.split.Tensor(mul_77, 192, 1);  mul_77 = None\n",
            "    getitem: \"f32[s31, 192, s12]\" = split[0];  getitem = None\n",
            "    getitem_1: \"f32[s31, 192, s12]\" = split[1];  split = getitem_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:64 in forward, code: x = torch.detach(x)\n",
            "    detach: \"f32[s31, 192, s12]\" = torch.ops.aten.detach.default(mul_76);  mul_76 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_37: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(detach, arg624_1, arg625_1);  detach = arg624_1 = arg625_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_78: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_37, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_38: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_78, arg628_1, arg629_1, [1], [1], [1], 192);  mul_78 = arg628_1 = arg629_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_61: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_38, 1, -1);  conv1d_38 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_12: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_61, [192], arg640_1, arg641_1);  transpose_61 = arg640_1 = arg641_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_62: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_12, 1, -1);  layer_norm_12 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_62);  transpose_62 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_39: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu, arg634_1, arg635_1);  gelu = arg634_1 = arg635_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_63: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_39, 1, -1);  conv1d_39 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_13: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_63, [192], arg646_1, arg647_1);  transpose_63 = arg646_1 = arg647_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_64: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_13, 1, -1);  layer_norm_13 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_1: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_64);  transpose_64 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_24: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_1, 0.5, False);  gelu_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_48: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(conv1d_37, dropout_24);  conv1d_37 = dropout_24 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_79: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_48, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_40: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_79, arg630_1, arg631_1, [1], [3], [3], 192);  mul_79 = arg630_1 = arg631_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_65: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_40, 1, -1);  conv1d_40 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_14: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_65, [192], arg642_1, arg643_1);  transpose_65 = arg642_1 = arg643_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_66: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_14, 1, -1);  layer_norm_14 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_2: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_66);  transpose_66 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_41: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_2, arg636_1, arg637_1);  gelu_2 = arg636_1 = arg637_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_67: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_41, 1, -1);  conv1d_41 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_15: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_67, [192], arg648_1, arg649_1);  transpose_67 = arg648_1 = arg649_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_68: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_15, 1, -1);  layer_norm_15 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_3: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_68);  transpose_68 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_25: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_3, 0.5, False);  gelu_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_49: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_48, dropout_25);  add_48 = dropout_25 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_80: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_49, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_42: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_80, arg632_1, arg633_1, [1], [9], [9], 192);  mul_80 = arg632_1 = arg633_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_69: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_42, 1, -1);  conv1d_42 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_16: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_69, [192], arg644_1, arg645_1);  transpose_69 = arg644_1 = arg645_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_70: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_16, 1, -1);  layer_norm_16 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_4: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_70);  transpose_70 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_43: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_4, arg638_1, arg639_1);  gelu_4 = arg638_1 = arg639_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_71: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_43, 1, -1);  conv1d_43 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_17: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_71, [192], arg650_1, arg651_1);  transpose_71 = arg650_1 = arg651_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_72: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_17, 1, -1);  layer_norm_17 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_5: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_72);  transpose_72 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_26: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_5, 0.5, False);  gelu_5 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_50: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_49, dropout_26);  add_49 = dropout_26 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:129 in forward, code: return x * x_mask\n",
            "    mul_81: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_50, type_as);  add_50 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_44: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_81, arg626_1, arg627_1);  mul_81 = arg626_1 = arg627_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:70 in forward, code: x = self.proj(x) * x_mask\n",
            "    mul_82: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_44, type_as)\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:111 in forward, code: z = torch.randn(x.size(0), 2, x.size(2)).type_as(x) * noise_scale\n",
            "    sym_size_int_16: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_44, 2);  conv1d_44 = None\n",
            "    randn: \"f32[s31, 2, s12]\" = torch.ops.aten.randn.default([sym_size_int_1, 2, sym_size_int_16], device = device(type='cpu'), pin_memory = False)\n",
            "    type_as_1: \"f32[s31, 2, s12]\" = torch.ops.aten.type_as.default(randn, mul_82);  randn = None\n",
            "    mul_83: \"f32[s31, 2, s12]\" = torch.ops.aten.mul.Tensor(type_as_1, select_2);  type_as_1 = select_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:386 in forward, code: x = torch.flip(x, [1])\n",
            "    flip: \"f32[s31, 2, s12]\" = torch.ops.aten.flip.default(mul_83, [1]);  mul_83 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:497 in forward, code: x0, x1 = torch.split(x, [self.half_channels] * 2, 1)\n",
            "    split_with_sizes = torch.ops.aten.split_with_sizes.default(flip, [1, 1], 1);  flip = None\n",
            "    getitem_2: \"f32[s31, 1, s12]\" = split_with_sizes[0]\n",
            "    getitem_3: \"f32[s31, 1, s12]\" = split_with_sizes[1];  split_with_sizes = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_45: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(getitem_2, arg454_1, arg455_1);  getitem_2 = arg454_1 = arg455_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:119 in forward, code: x = x + g\n",
            "    add_51: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(conv1d_45, mul_82);  conv1d_45 = mul_82 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_84: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_51, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_46: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_84, arg456_1, arg457_1, [1], [1], [1], 192);  mul_84 = arg456_1 = arg457_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_73: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_46, 1, -1);  conv1d_46 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_18: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_73, [192], arg468_1, arg469_1);  transpose_73 = arg468_1 = arg469_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_74: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_18, 1, -1);  layer_norm_18 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_6: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_74);  transpose_74 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_47: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_6, arg462_1, arg463_1);  gelu_6 = arg462_1 = arg463_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_75: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_47, 1, -1);  conv1d_47 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_19: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_75, [192], arg474_1, arg475_1);  transpose_75 = arg474_1 = arg475_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_76: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_19, 1, -1);  layer_norm_19 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_7: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_76);  transpose_76 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_27: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_7, 0.0, False);  gelu_7 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_52: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_51, dropout_27);  add_51 = dropout_27 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_85: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_52, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_48: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_85, arg458_1, arg459_1, [1], [3], [3], 192);  mul_85 = arg458_1 = arg459_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_77: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_48, 1, -1);  conv1d_48 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_20: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_77, [192], arg470_1, arg471_1);  transpose_77 = arg470_1 = arg471_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_78: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_20, 1, -1);  layer_norm_20 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_8: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_78);  transpose_78 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_49: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_8, arg464_1, arg465_1);  gelu_8 = arg464_1 = arg465_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_79: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_49, 1, -1);  conv1d_49 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_21: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_79, [192], arg476_1, arg477_1);  transpose_79 = arg476_1 = arg477_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_80: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_21, 1, -1);  layer_norm_21 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_9: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_80);  transpose_80 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_28: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_9, 0.0, False);  gelu_9 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_53: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_52, dropout_28);  add_52 = dropout_28 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_86: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_53, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_50: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_86, arg460_1, arg461_1, [1], [9], [9], 192);  mul_86 = arg460_1 = arg461_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_81: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_50, 1, -1);  conv1d_50 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_22: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_81, [192], arg472_1, arg473_1);  transpose_81 = arg472_1 = arg473_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_82: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_22, 1, -1);  layer_norm_22 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_10: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_82);  transpose_82 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_51: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_10, arg466_1, arg467_1);  gelu_10 = arg466_1 = arg467_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_83: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_51, 1, -1);  conv1d_51 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_23: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_83, [192], arg478_1, arg479_1);  transpose_83 = arg478_1 = arg479_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_84: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_23, 1, -1);  layer_norm_23 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_11: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_84);  transpose_84 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_29: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_11, 0.0, False);  gelu_11 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_54: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_53, dropout_29);  add_53 = dropout_29 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:129 in forward, code: return x * x_mask\n",
            "    mul_87: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_54, type_as);  add_54 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_52: \"f32[s31, 29, s12]\" = torch.ops.aten.conv1d.default(mul_87, arg480_1, arg481_1);  mul_87 = arg480_1 = arg481_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:500 in forward, code: h = self.proj(h) * x_mask\n",
            "    mul_88: \"f32[s31, 29, s12]\" = torch.ops.aten.mul.Tensor(conv1d_52, type_as);  conv1d_52 = type_as = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:503 in forward, code: h = h.reshape(b, c, -1, t).permute(0, 1, 3, 2)  # [b, cx?, t] -> [b, c, t, ?]\n",
            "    reshape: \"f32[s31, 1, 29, s12]\" = torch.ops.aten.reshape.default(mul_88, [sym_size_int_1, 1, -1, sym_size_int_16]);  mul_88 = sym_size_int_1 = sym_size_int_16 = None\n",
            "    permute: \"f32[s31, 1, s12, 29]\" = torch.ops.aten.permute.default(reshape, [0, 1, 3, 2]);  reshape = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:505 in forward, code: unnormalized_widths = h[..., : self.num_bins] / math.sqrt(self.filter_channels)\n",
            "    slice_43: \"f32[s31, 1, s12, 10]\" = torch.ops.aten.slice.Tensor(permute, 3, 0, 10)\n",
            "    div_12: \"f32[s31, 1, s12, 10]\" = torch.ops.aten.div.Tensor(slice_43, 13.856406460551018);  slice_43 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:506 in forward, code: unnormalized_heights = h[..., self.num_bins : 2 * self.num_bins] / math.sqrt(\n",
            "    slice_44: \"f32[s31, 1, s12, 10]\" = torch.ops.aten.slice.Tensor(permute, 3, 10, 20)\n",
            "    div_13: \"f32[s31, 1, s12, 10]\" = torch.ops.aten.div.Tensor(slice_44, 13.856406460551018);  slice_44 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:509 in forward, code: unnormalized_derivatives = h[..., 2 * self.num_bins :]\n",
            "    slice_45: \"f32[s31, 1, s12, 9]\" = torch.ops.aten.slice.Tensor(permute, 3, 20, 9223372036854775807);  permute = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:511 in forward, code: x1, logabsdet = piecewise_rational_quadratic_transform(\n",
            "    ge: \"b8[s31, 1, s12]\" = torch.ops.aten.ge.Scalar(getitem_3, -5.0)\n",
            "    le_12: \"b8[s31, 1, s12]\" = torch.ops.aten.le.Scalar(getitem_3, 5.0)\n",
            "    and_1: \"b8[s31, 1, s12]\" = torch.ops.aten.__and__.Tensor(ge, le_12);  ge = le_12 = None\n",
            "    bitwise_not: \"b8[s31, 1, s12]\" = torch.ops.aten.bitwise_not.default(and_1)\n",
            "    zeros_like: \"f32[s31, 1, s12]\" = torch.ops.aten.zeros_like.default(getitem_3, pin_memory = False)\n",
            "    zeros_like_1: \"f32[s31, 1, s12]\" = torch.ops.aten.zeros_like.default(getitem_3, pin_memory = False)\n",
            "    pad_48: \"f32[s31, 1, s12, 11]\" = torch.ops.aten.pad.default(slice_45, [1, 1]);  slice_45 = None\n",
            "    _tensor_constant0: \"f32[]\" = self._tensor_constant0\n",
            "    lift_fresh_copy: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None\n",
            "    select_3: \"f32[s31, 1, s12]\" = torch.ops.aten.select.int(pad_48, 3, 0)\n",
            "    fill_: \"f32[s31, 1, s12]\" = torch.ops.aten.fill_.Tensor(select_3, lift_fresh_copy);  select_3 = lift_fresh_copy = fill_ = None\n",
            "    _tensor_constant1: \"f32[]\" = self._tensor_constant1\n",
            "    lift_fresh_copy_1: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None\n",
            "    select_4: \"f32[s31, 1, s12]\" = torch.ops.aten.select.int(pad_48, 3, 10)\n",
            "    fill__1: \"f32[s31, 1, s12]\" = torch.ops.aten.fill_.Tensor(select_4, lift_fresh_copy_1);  select_4 = lift_fresh_copy_1 = fill__1 = None\n",
            "    index: \"f32[u0]\" = torch.ops.aten.index.Tensor(getitem_3, [bitwise_not])\n",
            "    sym_size_int_17: \"Sym(u0)\" = torch.ops.aten.sym_size.int(index, 0)\n",
            "    eq_32: \"Sym(True)\" = sym_size_int_17 == sym_size_int_17;  sym_size_int_17 = eq_32 = None\n",
            "    index_put_: \"f32[s31, 1, s12]\" = torch.ops.aten.index_put_.default(zeros_like, [bitwise_not], index);  zeros_like = index = index_put_ = None\n",
            "    _tensor_constant2: \"f32[]\" = self._tensor_constant2\n",
            "    lift_fresh_copy_2: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant2);  _tensor_constant2 = None\n",
            "    index_put__1: \"f32[s31, 1, s12]\" = torch.ops.aten.index_put_.default(zeros_like_1, [bitwise_not], lift_fresh_copy_2);  zeros_like_1 = bitwise_not = lift_fresh_copy_2 = index_put__1 = None\n",
            "    index_1: \"f32[u1]\" = torch.ops.aten.index.Tensor(getitem_3, [and_1]);  getitem_3 = None\n",
            "    index_2: \"f32[u1, 10]\" = torch.ops.aten.index.Tensor(div_12, [and_1]);  div_12 = None\n",
            "    index_3: \"f32[u1, 10]\" = torch.ops.aten.index.Tensor(div_13, [and_1]);  div_13 = None\n",
            "    index_4: \"f32[u1, 11]\" = torch.ops.aten.index.Tensor(pad_48, [and_1]);  pad_48 = and_1 = None\n",
            "    softmax_6: \"f32[u1, 10]\" = torch.ops.aten.softmax.int(index_2, -1);  index_2 = None\n",
            "    mul_89: \"f32[u1, 10]\" = torch.ops.aten.mul.Tensor(softmax_6, 0.99);  softmax_6 = None\n",
            "    add_55: \"f32[u1, 10]\" = torch.ops.aten.add.Tensor(mul_89, 0.001);  mul_89 = None\n",
            "    cumsum: \"f32[u1, 10]\" = torch.ops.aten.cumsum.default(add_55, -1);  add_55 = None\n",
            "    pad_49: \"f32[u1, 11]\" = torch.ops.aten.pad.default(cumsum, [1, 0], 'constant', 0.0);  cumsum = None\n",
            "    mul_90: \"f32[u1, 11]\" = torch.ops.aten.mul.Tensor(pad_49, 10.0);  pad_49 = None\n",
            "    add_56: \"f32[u1, 11]\" = torch.ops.aten.add.Tensor(mul_90, -5.0);  mul_90 = None\n",
            "    _tensor_constant3: \"f32[]\" = self._tensor_constant3\n",
            "    lift_fresh_copy_3: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant3);  _tensor_constant3 = None\n",
            "    select_5: \"f32[u1]\" = torch.ops.aten.select.int(add_56, 1, 0)\n",
            "    fill__2: \"f32[u1]\" = torch.ops.aten.fill_.Tensor(select_5, lift_fresh_copy_3);  select_5 = lift_fresh_copy_3 = fill__2 = None\n",
            "    _tensor_constant4: \"f32[]\" = self._tensor_constant4\n",
            "    lift_fresh_copy_4: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant4);  _tensor_constant4 = None\n",
            "    select_6: \"f32[u1]\" = torch.ops.aten.select.int(add_56, 1, 10)\n",
            "    fill__3: \"f32[u1]\" = torch.ops.aten.fill_.Tensor(select_6, lift_fresh_copy_4);  select_6 = lift_fresh_copy_4 = fill__3 = None\n",
            "    slice_46: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_56, 1, 1, 9223372036854775807)\n",
            "    slice_47: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_56, 1, 0, -1)\n",
            "    sub_66: \"f32[u1, 10]\" = torch.ops.aten.sub.Tensor(slice_46, slice_47);  slice_46 = slice_47 = None\n",
            "    softplus: \"f32[u1, 11]\" = torch.ops.aten.softplus.default(index_4);  index_4 = None\n",
            "    add_57: \"f32[u1, 11]\" = torch.ops.aten.add.Tensor(softplus, 0.001);  softplus = None\n",
            "    softmax_7: \"f32[u1, 10]\" = torch.ops.aten.softmax.int(index_3, -1);  index_3 = None\n",
            "    mul_91: \"f32[u1, 10]\" = torch.ops.aten.mul.Tensor(softmax_7, 0.99);  softmax_7 = None\n",
            "    add_58: \"f32[u1, 10]\" = torch.ops.aten.add.Tensor(mul_91, 0.001);  mul_91 = None\n",
            "    cumsum_1: \"f32[u1, 10]\" = torch.ops.aten.cumsum.default(add_58, -1);  add_58 = None\n",
            "    pad_50: \"f32[u1, 11]\" = torch.ops.aten.pad.default(cumsum_1, [1, 0], 'constant', 0.0);  cumsum_1 = None\n",
            "    mul_92: \"f32[u1, 11]\" = torch.ops.aten.mul.Tensor(pad_50, 10.0);  pad_50 = None\n",
            "    add_59: \"f32[u1, 11]\" = torch.ops.aten.add.Tensor(mul_92, -5.0);  mul_92 = None\n",
            "    _tensor_constant5: \"f32[]\" = self._tensor_constant5\n",
            "    lift_fresh_copy_5: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant5);  _tensor_constant5 = None\n",
            "    select_7: \"f32[u1]\" = torch.ops.aten.select.int(add_59, 1, 0)\n",
            "    fill__4: \"f32[u1]\" = torch.ops.aten.fill_.Tensor(select_7, lift_fresh_copy_5);  select_7 = lift_fresh_copy_5 = fill__4 = None\n",
            "    _tensor_constant6: \"f32[]\" = self._tensor_constant6\n",
            "    lift_fresh_copy_6: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant6);  _tensor_constant6 = None\n",
            "    select_8: \"f32[u1]\" = torch.ops.aten.select.int(add_59, 1, 10)\n",
            "    fill__5: \"f32[u1]\" = torch.ops.aten.fill_.Tensor(select_8, lift_fresh_copy_6);  select_8 = lift_fresh_copy_6 = fill__5 = None\n",
            "    slice_48: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_59, 1, 1, 9223372036854775807)\n",
            "    slice_49: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_59, 1, 0, -1)\n",
            "    sub_67: \"f32[u1, 10]\" = torch.ops.aten.sub.Tensor(slice_48, slice_49);  slice_48 = slice_49 = None\n",
            "    select_9: \"f32[u1]\" = torch.ops.aten.select.int(add_59, 1, 10)\n",
            "    add_: \"f32[u1]\" = torch.ops.aten.add_.Tensor(select_9, 1e-06);  select_9 = None\n",
            "    select_10: \"f32[u1]\" = torch.ops.aten.select.int(add_59, 1, 10)\n",
            "    sym_size_int_18: \"Sym(u1)\" = torch.ops.aten.sym_size.int(index_1, 0)\n",
            "    eq_33: \"Sym(True)\" = sym_size_int_18 == sym_size_int_18;  sym_size_int_18 = eq_33 = None\n",
            "    copy_: \"f32[u1]\" = torch.ops.aten.copy_.default(select_10, add_);  select_10 = add_ = copy_ = None\n",
            "    unsqueeze_17: \"f32[u1, 1]\" = torch.ops.aten.unsqueeze.default(index_1, 1)\n",
            "    ge_1: \"b8[u1, 11]\" = torch.ops.aten.ge.Tensor(unsqueeze_17, add_59);  unsqueeze_17 = None\n",
            "    sum_1: \"i64[u1]\" = torch.ops.aten.sum.dim_IntList(ge_1, [-1]);  ge_1 = None\n",
            "    sub_68: \"i64[u1]\" = torch.ops.aten.sub.Tensor(sum_1, 1);  sum_1 = None\n",
            "    unsqueeze_18: \"i64[u1, 1]\" = torch.ops.aten.unsqueeze.default(sub_68, 1);  sub_68 = None\n",
            "    gather: \"f32[u1, 1]\" = torch.ops.aten.gather.default(add_56, -1, unsqueeze_18);  add_56 = None\n",
            "    select_11: \"f32[u1]\" = torch.ops.aten.select.int(gather, 1, 0);  gather = select_11 = None\n",
            "    gather_1: \"f32[u1, 1]\" = torch.ops.aten.gather.default(sub_66, -1, unsqueeze_18)\n",
            "    select_12: \"f32[u1]\" = torch.ops.aten.select.int(gather_1, 1, 0);  gather_1 = select_12 = None\n",
            "    gather_2: \"f32[u1, 1]\" = torch.ops.aten.gather.default(add_59, -1, unsqueeze_18);  add_59 = None\n",
            "    select_13: \"f32[u1]\" = torch.ops.aten.select.int(gather_2, 1, 0);  gather_2 = None\n",
            "    div_14: \"f32[u1, 10]\" = torch.ops.aten.div.Tensor(sub_67, sub_66);  sub_66 = None\n",
            "    gather_3: \"f32[u1, 1]\" = torch.ops.aten.gather.default(div_14, -1, unsqueeze_18);  div_14 = None\n",
            "    select_14: \"f32[u1]\" = torch.ops.aten.select.int(gather_3, 1, 0);  gather_3 = None\n",
            "    gather_4: \"f32[u1, 1]\" = torch.ops.aten.gather.default(add_57, -1, unsqueeze_18)\n",
            "    select_15: \"f32[u1]\" = torch.ops.aten.select.int(gather_4, 1, 0);  gather_4 = None\n",
            "    slice_50: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_57, 1, 1, 9223372036854775807);  add_57 = None\n",
            "    gather_5: \"f32[u1, 1]\" = torch.ops.aten.gather.default(slice_50, -1, unsqueeze_18);  slice_50 = None\n",
            "    select_16: \"f32[u1]\" = torch.ops.aten.select.int(gather_5, 1, 0);  gather_5 = None\n",
            "    gather_6: \"f32[u1, 1]\" = torch.ops.aten.gather.default(sub_67, -1, unsqueeze_18);  sub_67 = unsqueeze_18 = None\n",
            "    select_17: \"f32[u1]\" = torch.ops.aten.select.int(gather_6, 1, 0);  gather_6 = None\n",
            "    sub_69: \"f32[u1]\" = torch.ops.aten.sub.Tensor(index_1, select_13)\n",
            "    add_60: \"f32[u1]\" = torch.ops.aten.add.Tensor(select_15, select_16)\n",
            "    mul_93: \"f32[u1]\" = torch.ops.aten.mul.Tensor(select_14, 2)\n",
            "    sub_70: \"f32[u1]\" = torch.ops.aten.sub.Tensor(add_60, mul_93);  add_60 = mul_93 = None\n",
            "    mul_94: \"f32[u1]\" = torch.ops.aten.mul.Tensor(sub_69, sub_70);  sub_69 = sub_70 = None\n",
            "    sub_71: \"f32[u1]\" = torch.ops.aten.sub.Tensor(select_14, select_15)\n",
            "    mul_95: \"f32[u1]\" = torch.ops.aten.mul.Tensor(select_17, sub_71);  sub_71 = None\n",
            "    add_61: \"f32[u1]\" = torch.ops.aten.add.Tensor(mul_94, mul_95);  mul_94 = mul_95 = None\n",
            "    mul_96: \"f32[u1]\" = torch.ops.aten.mul.Tensor(select_17, select_15);  select_17 = None\n",
            "    sub_72: \"f32[u1]\" = torch.ops.aten.sub.Tensor(index_1, select_13)\n",
            "    add_62: \"f32[u1]\" = torch.ops.aten.add.Tensor(select_15, select_16);  select_15 = select_16 = None\n",
            "    mul_97: \"f32[u1]\" = torch.ops.aten.mul.Tensor(select_14, 2)\n",
            "    sub_73: \"f32[u1]\" = torch.ops.aten.sub.Tensor(add_62, mul_97);  add_62 = mul_97 = None\n",
            "    mul_98: \"f32[u1]\" = torch.ops.aten.mul.Tensor(sub_72, sub_73);  sub_72 = sub_73 = None\n",
            "    sub_74: \"f32[u1]\" = torch.ops.aten.sub.Tensor(mul_96, mul_98);  mul_96 = mul_98 = None\n",
            "    neg: \"f32[u1]\" = torch.ops.aten.neg.default(select_14);  select_14 = None\n",
            "    sub_75: \"f32[u1]\" = torch.ops.aten.sub.Tensor(index_1, select_13);  index_1 = select_13 = None\n",
            "    mul_99: \"f32[u1]\" = torch.ops.aten.mul.Tensor(neg, sub_75);  neg = sub_75 = None\n",
            "    pow_1: \"f32[u1]\" = torch.ops.aten.pow.Tensor_Scalar(sub_74, 2);  sub_74 = None\n",
            "    mul_100: \"f32[u1]\" = torch.ops.aten.mul.Tensor(add_61, 4);  add_61 = None\n",
            "    mul_101: \"f32[u1]\" = torch.ops.aten.mul.Tensor(mul_100, mul_99);  mul_100 = mul_99 = None\n",
            "    sub_76: \"f32[u1]\" = torch.ops.aten.sub.Tensor(pow_1, mul_101);  pow_1 = mul_101 = None\n",
            "    ge_2: \"b8[u1]\" = torch.ops.aten.ge.Scalar(sub_76, 0);  sub_76 = None\n",
            "    all_1: \"b8[]\" = torch.ops.aten.all.default(ge_2);  ge_2 = None\n",
            "    ne: \"b8[]\" = torch.ops.aten.ne.Scalar(all_1, 0);  all_1 = None\n",
            "    item: \"Sym(Eq(u2, 1))\" = torch.ops.aten.item.default(ne);  ne = item = None\n",
            "    \n",
            "\n",
            "\n",
            "\n",
            "def forward(self, arg0_1: \"f32[256, 192]\", arg1_1: \"f32[1, 9, 96]\", arg2_1: \"f32[1, 9, 96]\", arg3_1: \"f32[192, 192, 1]\", arg4_1: \"f32[192]\", arg5_1: \"f32[192, 192, 1]\", arg6_1: \"f32[192]\", arg7_1: \"f32[192, 192, 1]\", arg8_1: \"f32[192]\", arg9_1: \"f32[192, 192, 1]\", arg10_1: \"f32[192]\", arg11_1: \"f32[1, 9, 96]\", arg12_1: \"f32[1, 9, 96]\", arg13_1: \"f32[192, 192, 1]\", arg14_1: \"f32[192]\", arg15_1: \"f32[192, 192, 1]\", arg16_1: \"f32[192]\", arg17_1: \"f32[192, 192, 1]\", arg18_1: \"f32[192]\", arg19_1: \"f32[192, 192, 1]\", arg20_1: \"f32[192]\", arg21_1: \"f32[1, 9, 96]\", arg22_1: \"f32[1, 9, 96]\", arg23_1: \"f32[192, 192, 1]\", arg24_1: \"f32[192]\", arg25_1: \"f32[192, 192, 1]\", arg26_1: \"f32[192]\", arg27_1: \"f32[192, 192, 1]\", arg28_1: \"f32[192]\", arg29_1: \"f32[192, 192, 1]\", arg30_1: \"f32[192]\", arg31_1: \"f32[1, 9, 96]\", arg32_1: \"f32[1, 9, 96]\", arg33_1: \"f32[192, 192, 1]\", arg34_1: \"f32[192]\", arg35_1: \"f32[192, 192, 1]\", arg36_1: \"f32[192]\", arg37_1: \"f32[192, 192, 1]\", arg38_1: \"f32[192]\", arg39_1: \"f32[192, 192, 1]\", arg40_1: \"f32[192]\", arg41_1: \"f32[1, 9, 96]\", arg42_1: \"f32[1, 9, 96]\", arg43_1: \"f32[192, 192, 1]\", arg44_1: \"f32[192]\", arg45_1: \"f32[192, 192, 1]\", arg46_1: \"f32[192]\", arg47_1: \"f32[192, 192, 1]\", arg48_1: \"f32[192]\", arg49_1: \"f32[192, 192, 1]\", arg50_1: \"f32[192]\", arg51_1: \"f32[1, 9, 96]\", arg52_1: \"f32[1, 9, 96]\", arg53_1: \"f32[192, 192, 1]\", arg54_1: \"f32[192]\", arg55_1: \"f32[192, 192, 1]\", arg56_1: \"f32[192]\", arg57_1: \"f32[192, 192, 1]\", arg58_1: \"f32[192]\", arg59_1: \"f32[192, 192, 1]\", arg60_1: \"f32[192]\", arg61_1: \"f32[192]\", arg62_1: \"f32[192]\", arg63_1: \"f32[192]\", arg64_1: \"f32[192]\", arg65_1: \"f32[192]\", arg66_1: \"f32[192]\", arg67_1: \"f32[192]\", arg68_1: \"f32[192]\", arg69_1: \"f32[192]\", arg70_1: \"f32[192]\", arg71_1: \"f32[192]\", arg72_1: \"f32[192]\", arg73_1: \"f32[768, 192, 3]\", arg74_1: \"f32[768]\", arg75_1: \"f32[192, 768, 3]\", arg76_1: \"f32[192]\", arg77_1: \"f32[768, 192, 3]\", arg78_1: \"f32[768]\", arg79_1: \"f32[192, 768, 3]\", arg80_1: \"f32[192]\", arg81_1: \"f32[768, 192, 3]\", arg82_1: \"f32[768]\", arg83_1: \"f32[192, 768, 3]\", arg84_1: \"f32[192]\", arg85_1: \"f32[768, 192, 3]\", arg86_1: \"f32[768]\", arg87_1: \"f32[192, 768, 3]\", arg88_1: \"f32[192]\", arg89_1: \"f32[768, 192, 3]\", arg90_1: \"f32[768]\", arg91_1: \"f32[192, 768, 3]\", arg92_1: \"f32[192]\", arg93_1: \"f32[768, 192, 3]\", arg94_1: \"f32[768]\", arg95_1: \"f32[192, 768, 3]\", arg96_1: \"f32[192]\", arg97_1: \"f32[192]\", arg98_1: \"f32[192]\", arg99_1: \"f32[192]\", arg100_1: \"f32[192]\", arg101_1: \"f32[192]\", arg102_1: \"f32[192]\", arg103_1: \"f32[192]\", arg104_1: \"f32[192]\", arg105_1: \"f32[192]\", arg106_1: \"f32[192]\", arg107_1: \"f32[192]\", arg108_1: \"f32[192]\", arg109_1: \"f32[384, 192, 1]\", arg110_1: \"f32[384]\", arg111_1: \"f32[256, 192, 7]\", arg112_1: \"f32[256]\", arg113_1: \"f32[128]\", arg114_1: \"f32[256, 128, 16]\", arg115_1: \"f32[64]\", arg116_1: \"f32[128, 64, 16]\", arg117_1: \"f32[32]\", arg118_1: \"f32[64, 32, 8]\", arg119_1: \"f32[128]\", arg120_1: \"f32[128, 128, 3]\", arg121_1: \"f32[128]\", arg122_1: \"f32[128, 128, 3]\", arg123_1: \"f32[128]\", arg124_1: \"f32[128, 128, 5]\", arg125_1: \"f32[128]\", arg126_1: \"f32[128, 128, 5]\", arg127_1: \"f32[128]\", arg128_1: \"f32[128, 128, 7]\", arg129_1: \"f32[128]\", arg130_1: \"f32[128, 128, 7]\", arg131_1: \"f32[64]\", arg132_1: \"f32[64, 64, 3]\", arg133_1: \"f32[64]\", arg134_1: \"f32[64, 64, 3]\", arg135_1: \"f32[64]\", arg136_1: \"f32[64, 64, 5]\", arg137_1: \"f32[64]\", arg138_1: \"f32[64, 64, 5]\", arg139_1: \"f32[64]\", arg140_1: \"f32[64, 64, 7]\", arg141_1: \"f32[64]\", arg142_1: \"f32[64, 64, 7]\", arg143_1: \"f32[32]\", arg144_1: \"f32[32, 32, 3]\", arg145_1: \"f32[32]\", arg146_1: \"f32[32, 32, 3]\", arg147_1: \"f32[32]\", arg148_1: \"f32[32, 32, 5]\", arg149_1: \"f32[32]\", arg150_1: \"f32[32, 32, 5]\", arg151_1: \"f32[32]\", arg152_1: \"f32[32, 32, 7]\", arg153_1: \"f32[32]\", arg154_1: \"f32[32, 32, 7]\", arg155_1: \"f32[1, 32, 7]\", arg156_1: \"f32[192, 513, 1]\", arg157_1: \"f32[192]\", arg158_1: \"f32[384]\", arg159_1: \"f32[384, 1, 1]\", arg160_1: \"f32[384, 192, 5]\", arg161_1: \"f32[384]\", arg162_1: \"f32[384, 1, 1]\", arg163_1: \"f32[384, 192, 5]\", arg164_1: \"f32[384]\", arg165_1: \"f32[384, 1, 1]\", arg166_1: \"f32[384, 192, 5]\", arg167_1: \"f32[384]\", arg168_1: \"f32[384, 1, 1]\", arg169_1: \"f32[384, 192, 5]\", arg170_1: \"f32[384]\", arg171_1: \"f32[384, 1, 1]\", arg172_1: \"f32[384, 192, 5]\", arg173_1: \"f32[384]\", arg174_1: \"f32[384, 1, 1]\", arg175_1: \"f32[384, 192, 5]\", arg176_1: \"f32[384]\", arg177_1: \"f32[384, 1, 1]\", arg178_1: \"f32[384, 192, 5]\", arg179_1: \"f32[384]\", arg180_1: \"f32[384, 1, 1]\", arg181_1: \"f32[384, 192, 5]\", arg182_1: \"f32[384]\", arg183_1: \"f32[384, 1, 1]\", arg184_1: \"f32[384, 192, 5]\", arg185_1: \"f32[384]\", arg186_1: \"f32[384, 1, 1]\", arg187_1: \"f32[384, 192, 5]\", arg188_1: \"f32[384]\", arg189_1: \"f32[384, 1, 1]\", arg190_1: \"f32[384, 192, 5]\", arg191_1: \"f32[384]\", arg192_1: \"f32[384, 1, 1]\", arg193_1: \"f32[384, 192, 5]\", arg194_1: \"f32[384]\", arg195_1: \"f32[384, 1, 1]\", arg196_1: \"f32[384, 192, 5]\", arg197_1: \"f32[384]\", arg198_1: \"f32[384, 1, 1]\", arg199_1: \"f32[384, 192, 5]\", arg200_1: \"f32[384]\", arg201_1: \"f32[384, 1, 1]\", arg202_1: \"f32[384, 192, 5]\", arg203_1: \"f32[384]\", arg204_1: \"f32[384, 1, 1]\", arg205_1: \"f32[384, 192, 5]\", arg206_1: \"f32[384]\", arg207_1: \"f32[384, 1, 1]\", arg208_1: \"f32[384, 192, 1]\", arg209_1: \"f32[384]\", arg210_1: \"f32[384, 1, 1]\", arg211_1: \"f32[384, 192, 1]\", arg212_1: \"f32[384]\", arg213_1: \"f32[384, 1, 1]\", arg214_1: \"f32[384, 192, 1]\", arg215_1: \"f32[384]\", arg216_1: \"f32[384, 1, 1]\", arg217_1: \"f32[384, 192, 1]\", arg218_1: \"f32[384]\", arg219_1: \"f32[384, 1, 1]\", arg220_1: \"f32[384, 192, 1]\", arg221_1: \"f32[384]\", arg222_1: \"f32[384, 1, 1]\", arg223_1: \"f32[384, 192, 1]\", arg224_1: \"f32[384]\", arg225_1: \"f32[384, 1, 1]\", arg226_1: \"f32[384, 192, 1]\", arg227_1: \"f32[384]\", arg228_1: \"f32[384, 1, 1]\", arg229_1: \"f32[384, 192, 1]\", arg230_1: \"f32[384]\", arg231_1: \"f32[384, 1, 1]\", arg232_1: \"f32[384, 192, 1]\", arg233_1: \"f32[384]\", arg234_1: \"f32[384, 1, 1]\", arg235_1: \"f32[384, 192, 1]\", arg236_1: \"f32[384]\", arg237_1: \"f32[384, 1, 1]\", arg238_1: \"f32[384, 192, 1]\", arg239_1: \"f32[384]\", arg240_1: \"f32[384, 1, 1]\", arg241_1: \"f32[384, 192, 1]\", arg242_1: \"f32[384]\", arg243_1: \"f32[384, 1, 1]\", arg244_1: \"f32[384, 192, 1]\", arg245_1: \"f32[384]\", arg246_1: \"f32[384, 1, 1]\", arg247_1: \"f32[384, 192, 1]\", arg248_1: \"f32[384]\", arg249_1: \"f32[384, 1, 1]\", arg250_1: \"f32[384, 192, 1]\", arg251_1: \"f32[192]\", arg252_1: \"f32[192, 1, 1]\", arg253_1: \"f32[192, 192, 1]\", arg254_1: \"f32[384, 192, 1]\", arg255_1: \"f32[384]\", arg256_1: \"f32[192, 96, 1]\", arg257_1: \"f32[192]\", arg258_1: \"f32[384]\", arg259_1: \"f32[384, 1, 1]\", arg260_1: \"f32[384, 192, 5]\", arg261_1: \"f32[384]\", arg262_1: \"f32[384, 1, 1]\", arg263_1: \"f32[384, 192, 5]\", arg264_1: \"f32[384]\", arg265_1: \"f32[384, 1, 1]\", arg266_1: \"f32[384, 192, 5]\", arg267_1: \"f32[384]\", arg268_1: \"f32[384, 1, 1]\", arg269_1: \"f32[384, 192, 5]\", arg270_1: \"f32[384]\", arg271_1: \"f32[384, 1, 1]\", arg272_1: \"f32[384, 192, 1]\", arg273_1: \"f32[384]\", arg274_1: \"f32[384, 1, 1]\", arg275_1: \"f32[384, 192, 1]\", arg276_1: \"f32[384]\", arg277_1: \"f32[384, 1, 1]\", arg278_1: \"f32[384, 192, 1]\", arg279_1: \"f32[192]\", arg280_1: \"f32[192, 1, 1]\", arg281_1: \"f32[192, 192, 1]\", arg282_1: \"f32[96, 192, 1]\", arg283_1: \"f32[96]\", arg284_1: \"f32[192, 96, 1]\", arg285_1: \"f32[192]\", arg286_1: \"f32[384]\", arg287_1: \"f32[384, 1, 1]\", arg288_1: \"f32[384, 192, 5]\", arg289_1: \"f32[384]\", arg290_1: \"f32[384, 1, 1]\", arg291_1: \"f32[384, 192, 5]\", arg292_1: \"f32[384]\", arg293_1: \"f32[384, 1, 1]\", arg294_1: \"f32[384, 192, 5]\", arg295_1: \"f32[384]\", arg296_1: \"f32[384, 1, 1]\", arg297_1: \"f32[384, 192, 5]\", arg298_1: \"f32[384]\", arg299_1: \"f32[384, 1, 1]\", arg300_1: \"f32[384, 192, 1]\", arg301_1: \"f32[384]\", arg302_1: \"f32[384, 1, 1]\", arg303_1: \"f32[384, 192, 1]\", arg304_1: \"f32[384]\", arg305_1: \"f32[384, 1, 1]\", arg306_1: \"f32[384, 192, 1]\", arg307_1: \"f32[192]\", arg308_1: \"f32[192, 1, 1]\", arg309_1: \"f32[192, 192, 1]\", arg310_1: \"f32[96, 192, 1]\", arg311_1: \"f32[96]\", arg312_1: \"f32[192, 96, 1]\", arg313_1: \"f32[192]\", arg314_1: \"f32[384]\", arg315_1: \"f32[384, 1, 1]\", arg316_1: \"f32[384, 192, 5]\", arg317_1: \"f32[384]\", arg318_1: \"f32[384, 1, 1]\", arg319_1: \"f32[384, 192, 5]\", arg320_1: \"f32[384]\", arg321_1: \"f32[384, 1, 1]\", arg322_1: \"f32[384, 192, 5]\", arg323_1: \"f32[384]\", arg324_1: \"f32[384, 1, 1]\", arg325_1: \"f32[384, 192, 5]\", arg326_1: \"f32[384]\", arg327_1: \"f32[384, 1, 1]\", arg328_1: \"f32[384, 192, 1]\", arg329_1: \"f32[384]\", arg330_1: \"f32[384, 1, 1]\", arg331_1: \"f32[384, 192, 1]\", arg332_1: \"f32[384]\", arg333_1: \"f32[384, 1, 1]\", arg334_1: \"f32[384, 192, 1]\", arg335_1: \"f32[192]\", arg336_1: \"f32[192, 1, 1]\", arg337_1: \"f32[192, 192, 1]\", arg338_1: \"f32[96, 192, 1]\", arg339_1: \"f32[96]\", arg340_1: \"f32[192, 96, 1]\", arg341_1: \"f32[192]\", arg342_1: \"f32[384]\", arg343_1: \"f32[384, 1, 1]\", arg344_1: \"f32[384, 192, 5]\", arg345_1: \"f32[384]\", arg346_1: \"f32[384, 1, 1]\", arg347_1: \"f32[384, 192, 5]\", arg348_1: \"f32[384]\", arg349_1: \"f32[384, 1, 1]\", arg350_1: \"f32[384, 192, 5]\", arg351_1: \"f32[384]\", arg352_1: \"f32[384, 1, 1]\", arg353_1: \"f32[384, 192, 5]\", arg354_1: \"f32[384]\", arg355_1: \"f32[384, 1, 1]\", arg356_1: \"f32[384, 192, 1]\", arg357_1: \"f32[384]\", arg358_1: \"f32[384, 1, 1]\", arg359_1: \"f32[384, 192, 1]\", arg360_1: \"f32[384]\", arg361_1: \"f32[384, 1, 1]\", arg362_1: \"f32[384, 192, 1]\", arg363_1: \"f32[192]\", arg364_1: \"f32[192, 1, 1]\", arg365_1: \"f32[192, 192, 1]\", arg366_1: \"f32[96, 192, 1]\", arg367_1: \"f32[96]\", arg368_1: \"f32[2, 1]\", arg369_1: \"f32[2, 1]\", arg370_1: \"f32[192, 1, 1]\", arg371_1: \"f32[192]\", arg372_1: \"f32[192, 1, 3]\", arg373_1: \"f32[192]\", arg374_1: \"f32[192, 1, 3]\", arg375_1: \"f32[192]\", arg376_1: \"f32[192, 1, 3]\", arg377_1: \"f32[192]\", arg378_1: \"f32[192, 192, 1]\", arg379_1: \"f32[192]\", arg380_1: \"f32[192, 192, 1]\", arg381_1: \"f32[192]\", arg382_1: \"f32[192, 192, 1]\", arg383_1: \"f32[192]\", arg384_1: \"f32[192]\", arg385_1: \"f32[192]\", arg386_1: \"f32[192]\", arg387_1: \"f32[192]\", arg388_1: \"f32[192]\", arg389_1: \"f32[192]\", arg390_1: \"f32[192]\", arg391_1: \"f32[192]\", arg392_1: \"f32[192]\", arg393_1: \"f32[192]\", arg394_1: \"f32[192]\", arg395_1: \"f32[192]\", arg396_1: \"f32[29, 192, 1]\", arg397_1: \"f32[29]\", arg398_1: \"f32[192, 1, 1]\", arg399_1: \"f32[192]\", arg400_1: \"f32[192, 1, 3]\", arg401_1: \"f32[192]\", arg402_1: \"f32[192, 1, 3]\", arg403_1: \"f32[192]\", arg404_1: \"f32[192, 1, 3]\", arg405_1: \"f32[192]\", arg406_1: \"f32[192, 192, 1]\", arg407_1: \"f32[192]\", arg408_1: \"f32[192, 192, 1]\", arg409_1: \"f32[192]\", arg410_1: \"f32[192, 192, 1]\", arg411_1: \"f32[192]\", arg412_1: \"f32[192]\", arg413_1: \"f32[192]\", arg414_1: \"f32[192]\", arg415_1: \"f32[192]\", arg416_1: \"f32[192]\", arg417_1: \"f32[192]\", arg418_1: \"f32[192]\", arg419_1: \"f32[192]\", arg420_1: \"f32[192]\", arg421_1: \"f32[192]\", arg422_1: \"f32[192]\", arg423_1: \"f32[192]\", arg424_1: \"f32[29, 192, 1]\", arg425_1: \"f32[29]\", arg426_1: \"f32[192, 1, 1]\", arg427_1: \"f32[192]\", arg428_1: \"f32[192, 1, 3]\", arg429_1: \"f32[192]\", arg430_1: \"f32[192, 1, 3]\", arg431_1: \"f32[192]\", arg432_1: \"f32[192, 1, 3]\", arg433_1: \"f32[192]\", arg434_1: \"f32[192, 192, 1]\", arg435_1: \"f32[192]\", arg436_1: \"f32[192, 192, 1]\", arg437_1: \"f32[192]\", arg438_1: \"f32[192, 192, 1]\", arg439_1: \"f32[192]\", arg440_1: \"f32[192]\", arg441_1: \"f32[192]\", arg442_1: \"f32[192]\", arg443_1: \"f32[192]\", arg444_1: \"f32[192]\", arg445_1: \"f32[192]\", arg446_1: \"f32[192]\", arg447_1: \"f32[192]\", arg448_1: \"f32[192]\", arg449_1: \"f32[192]\", arg450_1: \"f32[192]\", arg451_1: \"f32[192]\", arg452_1: \"f32[29, 192, 1]\", arg453_1: \"f32[29]\", arg454_1: \"f32[192, 1, 1]\", arg455_1: \"f32[192]\", arg456_1: \"f32[192, 1, 3]\", arg457_1: \"f32[192]\", arg458_1: \"f32[192, 1, 3]\", arg459_1: \"f32[192]\", arg460_1: \"f32[192, 1, 3]\", arg461_1: \"f32[192]\", arg462_1: \"f32[192, 192, 1]\", arg463_1: \"f32[192]\", arg464_1: \"f32[192, 192, 1]\", arg465_1: \"f32[192]\", arg466_1: \"f32[192, 192, 1]\", arg467_1: \"f32[192]\", arg468_1: \"f32[192]\", arg469_1: \"f32[192]\", arg470_1: \"f32[192]\", arg471_1: \"f32[192]\", arg472_1: \"f32[192]\", arg473_1: \"f32[192]\", arg474_1: \"f32[192]\", arg475_1: \"f32[192]\", arg476_1: \"f32[192]\", arg477_1: \"f32[192]\", arg478_1: \"f32[192]\", arg479_1: \"f32[192]\", arg480_1: \"f32[29, 192, 1]\", arg481_1: \"f32[29]\", arg482_1: \"f32[192, 1, 1]\", arg483_1: \"f32[192]\", arg484_1: \"f32[192, 192, 1]\", arg485_1: \"f32[192]\", arg486_1: \"f32[192, 1, 3]\", arg487_1: \"f32[192]\", arg488_1: \"f32[192, 1, 3]\", arg489_1: \"f32[192]\", arg490_1: \"f32[192, 1, 3]\", arg491_1: \"f32[192]\", arg492_1: \"f32[192, 192, 1]\", arg493_1: \"f32[192]\", arg494_1: \"f32[192, 192, 1]\", arg495_1: \"f32[192]\", arg496_1: \"f32[192, 192, 1]\", arg497_1: \"f32[192]\", arg498_1: \"f32[192]\", arg499_1: \"f32[192]\", arg500_1: \"f32[192]\", arg501_1: \"f32[192]\", arg502_1: \"f32[192]\", arg503_1: \"f32[192]\", arg504_1: \"f32[192]\", arg505_1: \"f32[192]\", arg506_1: \"f32[192]\", arg507_1: \"f32[192]\", arg508_1: \"f32[192]\", arg509_1: \"f32[192]\", arg510_1: \"f32[2, 1]\", arg511_1: \"f32[2, 1]\", arg512_1: \"f32[192, 1, 1]\", arg513_1: \"f32[192]\", arg514_1: \"f32[192, 1, 3]\", arg515_1: \"f32[192]\", arg516_1: \"f32[192, 1, 3]\", arg517_1: \"f32[192]\", arg518_1: \"f32[192, 1, 3]\", arg519_1: \"f32[192]\", arg520_1: \"f32[192, 192, 1]\", arg521_1: \"f32[192]\", arg522_1: \"f32[192, 192, 1]\", arg523_1: \"f32[192]\", arg524_1: \"f32[192, 192, 1]\", arg525_1: \"f32[192]\", arg526_1: \"f32[192]\", arg527_1: \"f32[192]\", arg528_1: \"f32[192]\", arg529_1: \"f32[192]\", arg530_1: \"f32[192]\", arg531_1: \"f32[192]\", arg532_1: \"f32[192]\", arg533_1: \"f32[192]\", arg534_1: \"f32[192]\", arg535_1: \"f32[192]\", arg536_1: \"f32[192]\", arg537_1: \"f32[192]\", arg538_1: \"f32[29, 192, 1]\", arg539_1: \"f32[29]\", arg540_1: \"f32[192, 1, 1]\", arg541_1: \"f32[192]\", arg542_1: \"f32[192, 1, 3]\", arg543_1: \"f32[192]\", arg544_1: \"f32[192, 1, 3]\", arg545_1: \"f32[192]\", arg546_1: \"f32[192, 1, 3]\", arg547_1: \"f32[192]\", arg548_1: \"f32[192, 192, 1]\", arg549_1: \"f32[192]\", arg550_1: \"f32[192, 192, 1]\", arg551_1: \"f32[192]\", arg552_1: \"f32[192, 192, 1]\", arg553_1: \"f32[192]\", arg554_1: \"f32[192]\", arg555_1: \"f32[192]\", arg556_1: \"f32[192]\", arg557_1: \"f32[192]\", arg558_1: \"f32[192]\", arg559_1: \"f32[192]\", arg560_1: \"f32[192]\", arg561_1: \"f32[192]\", arg562_1: \"f32[192]\", arg563_1: \"f32[192]\", arg564_1: \"f32[192]\", arg565_1: \"f32[192]\", arg566_1: \"f32[29, 192, 1]\", arg567_1: \"f32[29]\", arg568_1: \"f32[192, 1, 1]\", arg569_1: \"f32[192]\", arg570_1: \"f32[192, 1, 3]\", arg571_1: \"f32[192]\", arg572_1: \"f32[192, 1, 3]\", arg573_1: \"f32[192]\", arg574_1: \"f32[192, 1, 3]\", arg575_1: \"f32[192]\", arg576_1: \"f32[192, 192, 1]\", arg577_1: \"f32[192]\", arg578_1: \"f32[192, 192, 1]\", arg579_1: \"f32[192]\", arg580_1: \"f32[192, 192, 1]\", arg581_1: \"f32[192]\", arg582_1: \"f32[192]\", arg583_1: \"f32[192]\", arg584_1: \"f32[192]\", arg585_1: \"f32[192]\", arg586_1: \"f32[192]\", arg587_1: \"f32[192]\", arg588_1: \"f32[192]\", arg589_1: \"f32[192]\", arg590_1: \"f32[192]\", arg591_1: \"f32[192]\", arg592_1: \"f32[192]\", arg593_1: \"f32[192]\", arg594_1: \"f32[29, 192, 1]\", arg595_1: \"f32[29]\", arg596_1: \"f32[192, 1, 1]\", arg597_1: \"f32[192]\", arg598_1: \"f32[192, 1, 3]\", arg599_1: \"f32[192]\", arg600_1: \"f32[192, 1, 3]\", arg601_1: \"f32[192]\", arg602_1: \"f32[192, 1, 3]\", arg603_1: \"f32[192]\", arg604_1: \"f32[192, 192, 1]\", arg605_1: \"f32[192]\", arg606_1: \"f32[192, 192, 1]\", arg607_1: \"f32[192]\", arg608_1: \"f32[192, 192, 1]\", arg609_1: \"f32[192]\", arg610_1: \"f32[192]\", arg611_1: \"f32[192]\", arg612_1: \"f32[192]\", arg613_1: \"f32[192]\", arg614_1: \"f32[192]\", arg615_1: \"f32[192]\", arg616_1: \"f32[192]\", arg617_1: \"f32[192]\", arg618_1: \"f32[192]\", arg619_1: \"f32[192]\", arg620_1: \"f32[192]\", arg621_1: \"f32[192]\", arg622_1: \"f32[29, 192, 1]\", arg623_1: \"f32[29]\", arg624_1: \"f32[192, 192, 1]\", arg625_1: \"f32[192]\", arg626_1: \"f32[192, 192, 1]\", arg627_1: \"f32[192]\", arg628_1: \"f32[192, 1, 3]\", arg629_1: \"f32[192]\", arg630_1: \"f32[192, 1, 3]\", arg631_1: \"f32[192]\", arg632_1: \"f32[192, 1, 3]\", arg633_1: \"f32[192]\", arg634_1: \"f32[192, 192, 1]\", arg635_1: \"f32[192]\", arg636_1: \"f32[192, 192, 1]\", arg637_1: \"f32[192]\", arg638_1: \"f32[192, 192, 1]\", arg639_1: \"f32[192]\", arg640_1: \"f32[192]\", arg641_1: \"f32[192]\", arg642_1: \"f32[192]\", arg643_1: \"f32[192]\", arg644_1: \"f32[192]\", arg645_1: \"f32[192]\", arg646_1: \"f32[192]\", arg647_1: \"f32[192]\", arg648_1: \"f32[192]\", arg649_1: \"f32[192]\", arg650_1: \"f32[192]\", arg651_1: \"f32[192]\", arg652_1: \"i64[s31, s12]\", arg653_1: \"i64[s31]\", arg654_1: \"f32[3]\", arg655_1):\n",
            "    # No stacktrace found for following nodes\n",
            "    select: \"f32[]\" = torch.ops.aten.select.int(arg654_1, 0, 0);  select = None\n",
            "    select_1: \"f32[]\" = torch.ops.aten.select.int(arg654_1, 0, 1);  select_1 = None\n",
            "    select_2: \"f32[]\" = torch.ops.aten.select.int(arg654_1, 0, 2);  arg654_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(\n",
            "    embedding: \"f32[s31, s12, 192]\" = torch.ops.aten.embedding.default(arg0_1, arg652_1);  arg0_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:199 in forward, code: x = self.emb(x) * math.sqrt(self.hidden_channels)  # [b, t, h]\n",
            "    mul: \"f32[s31, s12, 192]\" = torch.ops.aten.mul.Tensor(embedding, 13.856406460551018);  embedding = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:200 in forward, code: x = torch.transpose(x, 1, -1)  # [b, h, t]\n",
            "    transpose: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(mul, 1, -1);  mul = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:202 in forward, code: commons.sequence_mask(x_lengths, x.size(2)), 1\n",
            "    sym_size_int: \"Sym(s12)\" = torch.ops.aten.sym_size.int(arg652_1, 1)\n",
            "    arange: \"i64[s12]\" = torch.ops.aten.arange.default(sym_size_int, dtype = torch.int64, device = device(type='cpu'), pin_memory = False);  sym_size_int = None\n",
            "    unsqueeze: \"i64[1, s12]\" = torch.ops.aten.unsqueeze.default(arange, 0)\n",
            "    unsqueeze_1: \"i64[s31, 1]\" = torch.ops.aten.unsqueeze.default(arg653_1, 1)\n",
            "    lt: \"b8[s31, s12]\" = torch.ops.aten.lt.Tensor(unsqueeze, unsqueeze_1);  unsqueeze = unsqueeze_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:201 in forward, code: x_mask = torch.unsqueeze(\n",
            "    unsqueeze_2: \"b8[s31, 1, s12]\" = torch.ops.aten.unsqueeze.default(lt, 1);  lt = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:203 in forward, code: ).type_as(x)\n",
            "    type_as: \"f32[s31, 1, s12]\" = torch.ops.aten.type_as.default(unsqueeze_2, transpose);  unsqueeze_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:205 in forward, code: x = self.encoder(x * x_mask, x_mask)\n",
            "    mul_1: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose, type_as);  transpose = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:61 in forward, code: attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n",
            "    unsqueeze_3: \"f32[s31, 1, 1, s12]\" = torch.ops.aten.unsqueeze.default(type_as, 2)\n",
            "    unsqueeze_4: \"f32[s31, 1, s12, 1]\" = torch.ops.aten.unsqueeze.default(type_as, -1)\n",
            "    mul_2: \"f32[s31, 1, s12, s12]\" = torch.ops.aten.mul.Tensor(unsqueeze_3, unsqueeze_4);  unsqueeze_3 = unsqueeze_4 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:62 in forward, code: x = x * x_mask\n",
            "    mul_3: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(mul_1, type_as);  mul_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_3, arg3_1, arg4_1);  arg3_1 = arg4_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_1: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_3, arg5_1, arg6_1);  arg5_1 = arg6_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_2: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_3, arg7_1, arg8_1);  arg7_1 = arg8_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_1: \"Sym(s31)\" = torch.ops.aten.sym_size.int(arg652_1, 0);  arg652_1 = None\n",
            "    sym_size_int_2: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d, 2)\n",
            "    view: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d, [sym_size_int_1, 2, 96, sym_size_int_2]);  conv1d = None\n",
            "    transpose_1: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view, 2, 3);  view = None\n",
            "    sym_size_int_3: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_1, 2)\n",
            "    view_1: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_1, [sym_size_int_1, 2, 96, sym_size_int_3]);  conv1d_1 = None\n",
            "    transpose_2: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_1, 2, 3);  view_1 = None\n",
            "    view_2: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_2, [sym_size_int_1, 2, 96, sym_size_int_3]);  conv1d_2 = None\n",
            "    transpose_3: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_2, 2, 3);  view_2 = None\n",
            "    div: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_1, 9.797958971132712)\n",
            "    transpose_4: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_2, -2, -1);  transpose_2 = None\n",
            "    matmul: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div, transpose_4);  div = transpose_4 = None\n",
            "    eq: \"Sym(True)\" = sym_size_int_3 == sym_size_int_2;  eq = None\n",
            "    sub: \"Sym(s12 - 5)\" = sym_size_int_3 - 5\n",
            "    sym_max: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub, 0);  sub = None\n",
            "    sub_1: \"Sym(5 - s12)\" = 5 - sym_size_int_3\n",
            "    sym_max_1: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_1, 0);  sub_1 = None\n",
            "    mul_4: \"Sym(2*s12)\" = 2 * sym_size_int_3\n",
            "    add: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_1 + mul_4;  mul_4 = None\n",
            "    sub_2: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add - 1;  add = None\n",
            "    gt: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max > 0;  gt = None\n",
            "    pad: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg1_1, [0, 0, sym_max, sym_max, 0, 0]);  arg1_1 = sym_max = None\n",
            "    slice_1: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad, 1, sym_max_1, sub_2);  pad = sym_max_1 = sub_2 = None\n",
            "    div_1: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_1, 9.797958971132712);  transpose_1 = None\n",
            "    unsqueeze_5: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_1, 0);  slice_1 = None\n",
            "    transpose_5: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_5, -2, -1);  unsqueeze_5 = None\n",
            "    matmul_1: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_1, transpose_5);  div_1 = transpose_5 = None\n",
            "    pad_1: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_1, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_1 = None\n",
            "    mul_5: \"Sym(2*s12)\" = sym_size_int_2 * 2\n",
            "    mul_6: \"Sym(2*s12**2)\" = mul_5 * sym_size_int_2;  mul_5 = None\n",
            "    view_3: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_1, [sym_size_int_1, 2, mul_6]);  pad_1 = mul_6 = None\n",
            "    sub_3: \"Sym(s12 - 1)\" = sym_size_int_2 - 1\n",
            "    pad_2: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_3, [0, sub_3, 0, 0, 0, 0]);  view_3 = sub_3 = None\n",
            "    add_1: \"Sym(s12 + 1)\" = sym_size_int_2 + 1\n",
            "    mul_7: \"Sym(2*s12)\" = 2 * sym_size_int_2\n",
            "    sub_4: \"Sym(2*s12 - 1)\" = mul_7 - 1;  mul_7 = None\n",
            "    view_4: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_2, [sym_size_int_1, 2, add_1, sub_4]);  pad_2 = add_1 = sub_4 = None\n",
            "    sub_5: \"Sym(s12 - 1)\" = sym_size_int_2 - 1\n",
            "    slice_2: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_4, 2, None, sym_size_int_2);  view_4 = None\n",
            "    slice_3: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_2, 3, sub_5);  slice_2 = sub_5 = None\n",
            "    add_2: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul, slice_3);  matmul = slice_3 = None\n",
            "    eq_1: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_2, eq_1, -10000.0);  add_2 = eq_1 = None\n",
            "    softmax: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill, -1);  masked_fill = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax, 0.1, False);  softmax = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_2: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout, transpose_3);  transpose_3 = None\n",
            "    sym_size_int_4: \"Sym(s12)\" = torch.ops.aten.sym_size.int(arange, 0);  arange = None\n",
            "    sub_6: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_3: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout, [0, sub_6, 0, 0, 0, 0, 0, 0]);  dropout = sub_6 = None\n",
            "    mul_8: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_7: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_9: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_7;  sub_7 = None\n",
            "    add_3: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_8 + mul_9;  mul_8 = mul_9 = None\n",
            "    sym_size_int_5: \"Sym(s31)\" = torch.ops.aten.sym_size.int(arg653_1, 0);  arg653_1 = None\n",
            "    view_5: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_3, [sym_size_int_5, 2, add_3]);  pad_3 = add_3 = None\n",
            "    pad_4: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_5, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_5 = None\n",
            "    mul_10: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_6: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_4, [sym_size_int_5, 2, sym_size_int_4, mul_10]);  pad_4 = mul_10 = None\n",
            "    le: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le = None\n",
            "    slice_4: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_6, 0, 0, 9223372036854775807);  view_6 = None\n",
            "    le_1: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_1 = None\n",
            "    slice_5: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_4, 2, 0, 9223372036854775807);  slice_4 = None\n",
            "    slice_6: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_5, 3, 1, 9223372036854775807);  slice_5 = None\n",
            "    sub_8: \"Sym(s12 - 5)\" = sym_size_int_3 - 5\n",
            "    sym_max_2: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_8, 0);  sub_8 = None\n",
            "    sub_9: \"Sym(5 - s12)\" = 5 - sym_size_int_3\n",
            "    sym_max_3: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_9, 0);  sub_9 = None\n",
            "    mul_11: \"Sym(2*s12)\" = 2 * sym_size_int_3;  sym_size_int_3 = None\n",
            "    add_4: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_3 + mul_11;  mul_11 = None\n",
            "    sub_10: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_4 - 1;  add_4 = None\n",
            "    gt_1: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_2 > 0;  gt_1 = None\n",
            "    pad_5: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg2_1, [0, 0, sym_max_2, sym_max_2, 0, 0]);  arg2_1 = sym_max_2 = None\n",
            "    slice_7: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_5, 1, sym_max_3, sub_10);  pad_5 = sym_max_3 = sub_10 = None\n",
            "    unsqueeze_6: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_7, 0);  slice_7 = None\n",
            "    matmul_3: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_6, unsqueeze_6);  slice_6 = unsqueeze_6 = None\n",
            "    add_5: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_2, matmul_3);  matmul_2 = matmul_3 = None\n",
            "    transpose_6: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_5, 2, 3)\n",
            "    sym_numel_default: \"Sym(192*s12*s31)\" = torch.ops.aten.sym_numel.default(transpose_6)\n",
            "    eq_2: \"Sym(False)\" = sym_numel_default == 0;  eq_2 = None\n",
            "    eq_3: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1;  eq_3 = None\n",
            "    eq_4: \"Sym(False)\" = sym_numel_default == 0\n",
            "    eq_5: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1\n",
            "    or_: \"Sym(Eq(s12, 1))\" = eq_5 | False;  eq_5 = None\n",
            "    and_: \"Sym(Eq(s12, 1))\" = True & or_;  or_ = None\n",
            "    eq_6: \"Sym(Eq(s12, 1))\" = 1 == sym_size_int_4\n",
            "    or__1: \"Sym(Eq(s12, 1))\" = False | eq_6;  eq_6 = None\n",
            "    and__1: \"Sym(Eq(s12, 1))\" = and_ & or__1;  and_ = or__1 = None\n",
            "    mul_12: \"Sym(96*s12)\" = sym_size_int_4 * 96\n",
            "    sym_stride_int: \"Sym(96*s12)\" = torch.ops.aten.sym_stride.int(add_5, 1)\n",
            "    eq_7: \"Sym(True)\" = sym_stride_int == mul_12;  sym_stride_int = None\n",
            "    or__2: \"Sym(True)\" = False | eq_7;  eq_7 = None\n",
            "    and__2: \"Sym(Eq(s12, 1))\" = and__1 & or__2;  and__1 = or__2 = None\n",
            "    mul_13: \"Sym(192*s12)\" = mul_12 * 2;  mul_12 = None\n",
            "    eq_8: \"Sym(Eq(s31, 1))\" = sym_size_int_5 == 1\n",
            "    sym_stride_int_1: \"Sym(192*s12)\" = torch.ops.aten.sym_stride.int(add_5, 0);  add_5 = None\n",
            "    eq_9: \"Sym(True)\" = sym_stride_int_1 == mul_13;  sym_stride_int_1 = None\n",
            "    or__3: \"Sym(True)\" = eq_8 | eq_9;  eq_8 = eq_9 = None\n",
            "    and__3: \"Sym(Eq(s12, 1))\" = and__2 & or__3;  and__2 = or__3 = None\n",
            "    mul_14: \"Sym(192*s12*s31)\" = mul_13 * sym_size_int_5;  mul_13 = mul_14 = None\n",
            "    or__4: \"Sym(Eq(s12, 1))\" = and__3 | eq_4;  and__3 = eq_4 = or__4 = None\n",
            "    eq_10: \"Sym(False)\" = sym_numel_default == 0;  sym_numel_default = eq_10 = None\n",
            "    eq_11: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1;  eq_11 = None\n",
            "    contiguous: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_6);  transpose_6 = None\n",
            "    view_7: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous, [sym_size_int_1, 192, sym_size_int_2]);  contiguous = sym_size_int_2 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_3: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_7, arg9_1, arg10_1);  view_7 = arg9_1 = arg10_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_1: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_3, 0.1, False);  conv1d_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_6: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(mul_3, dropout_1);  mul_3 = dropout_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_7: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_6, 1, -1);  add_6 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_7, [192], arg61_1, arg62_1);  transpose_7 = arg61_1 = arg62_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_8: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm, 1, -1);  layer_norm = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_15: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_8, type_as)\n",
            "    pad_6: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_15, [1, 1, 0, 0, 0, 0]);  mul_15 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_4: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_6, arg73_1, arg74_1);  pad_6 = arg73_1 = arg74_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_4);  conv1d_4 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_2: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu, 0.1, False);  relu = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_16: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_2, type_as);  dropout_2 = None\n",
            "    pad_7: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_16, [1, 1, 0, 0, 0, 0]);  mul_16 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_5: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_7, arg75_1, arg76_1);  pad_7 = arg75_1 = arg76_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_17: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_5, type_as);  conv1d_5 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_3: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_17, 0.1, False);  mul_17 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_7: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_8, dropout_3);  transpose_8 = dropout_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_9: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_7, 1, -1);  add_7 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_1: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_9, [192], arg97_1, arg98_1);  transpose_9 = arg97_1 = arg98_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_10: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_1, 1, -1);  layer_norm_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_6: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_10, arg13_1, arg14_1);  arg13_1 = arg14_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_7: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_10, arg15_1, arg16_1);  arg15_1 = arg16_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_8: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_10, arg17_1, arg18_1);  arg17_1 = arg18_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_6: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_6, 2)\n",
            "    view_8: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_6, [sym_size_int_1, 2, 96, sym_size_int_6]);  conv1d_6 = None\n",
            "    transpose_11: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_8, 2, 3);  view_8 = None\n",
            "    sym_size_int_7: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_7, 2)\n",
            "    view_9: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_7, [sym_size_int_1, 2, 96, sym_size_int_7]);  conv1d_7 = None\n",
            "    transpose_12: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_9, 2, 3);  view_9 = None\n",
            "    view_10: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_8, [sym_size_int_1, 2, 96, sym_size_int_7]);  conv1d_8 = None\n",
            "    transpose_13: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_10, 2, 3);  view_10 = None\n",
            "    div_2: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_11, 9.797958971132712)\n",
            "    transpose_14: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_12, -2, -1);  transpose_12 = None\n",
            "    matmul_4: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_2, transpose_14);  div_2 = transpose_14 = None\n",
            "    eq_12: \"Sym(True)\" = sym_size_int_7 == sym_size_int_6;  eq_12 = None\n",
            "    sub_11: \"Sym(s12 - 5)\" = sym_size_int_7 - 5\n",
            "    sym_max_4: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_11, 0);  sub_11 = None\n",
            "    sub_12: \"Sym(5 - s12)\" = 5 - sym_size_int_7\n",
            "    sym_max_5: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_12, 0);  sub_12 = None\n",
            "    mul_18: \"Sym(2*s12)\" = 2 * sym_size_int_7\n",
            "    add_8: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_5 + mul_18;  mul_18 = None\n",
            "    sub_13: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_8 - 1;  add_8 = None\n",
            "    gt_2: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_4 > 0;  gt_2 = None\n",
            "    pad_8: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg11_1, [0, 0, sym_max_4, sym_max_4, 0, 0]);  arg11_1 = sym_max_4 = None\n",
            "    slice_8: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_8, 1, sym_max_5, sub_13);  pad_8 = sym_max_5 = sub_13 = None\n",
            "    div_3: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_11, 9.797958971132712);  transpose_11 = None\n",
            "    unsqueeze_7: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_8, 0);  slice_8 = None\n",
            "    transpose_15: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_7, -2, -1);  unsqueeze_7 = None\n",
            "    matmul_5: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_3, transpose_15);  div_3 = transpose_15 = None\n",
            "    pad_9: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_5, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_5 = None\n",
            "    mul_19: \"Sym(2*s12)\" = sym_size_int_6 * 2\n",
            "    mul_20: \"Sym(2*s12**2)\" = mul_19 * sym_size_int_6;  mul_19 = None\n",
            "    view_11: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_9, [sym_size_int_1, 2, mul_20]);  pad_9 = mul_20 = None\n",
            "    sub_14: \"Sym(s12 - 1)\" = sym_size_int_6 - 1\n",
            "    pad_10: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_11, [0, sub_14, 0, 0, 0, 0]);  view_11 = sub_14 = None\n",
            "    add_9: \"Sym(s12 + 1)\" = sym_size_int_6 + 1\n",
            "    mul_21: \"Sym(2*s12)\" = 2 * sym_size_int_6\n",
            "    sub_15: \"Sym(2*s12 - 1)\" = mul_21 - 1;  mul_21 = None\n",
            "    view_12: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_10, [sym_size_int_1, 2, add_9, sub_15]);  pad_10 = add_9 = sub_15 = None\n",
            "    sub_16: \"Sym(s12 - 1)\" = sym_size_int_6 - 1\n",
            "    slice_9: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_12, 2, None, sym_size_int_6);  view_12 = None\n",
            "    slice_10: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_9, 3, sub_16);  slice_9 = sub_16 = None\n",
            "    add_10: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_4, slice_10);  matmul_4 = slice_10 = None\n",
            "    eq_13: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill_1: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_10, eq_13, -10000.0);  add_10 = eq_13 = None\n",
            "    softmax_1: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_1, -1);  masked_fill_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_4: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_1, 0.1, False);  softmax_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_6: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_4, transpose_13);  transpose_13 = None\n",
            "    sub_17: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_11: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_4, [0, sub_17, 0, 0, 0, 0, 0, 0]);  dropout_4 = sub_17 = None\n",
            "    mul_22: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_18: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_23: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_18;  sub_18 = None\n",
            "    add_11: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_22 + mul_23;  mul_22 = mul_23 = None\n",
            "    view_13: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_11, [sym_size_int_5, 2, add_11]);  pad_11 = add_11 = None\n",
            "    pad_12: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_13, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_13 = None\n",
            "    mul_24: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_14: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_12, [sym_size_int_5, 2, sym_size_int_4, mul_24]);  pad_12 = mul_24 = None\n",
            "    le_2: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le_2 = None\n",
            "    slice_11: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_14, 0, 0, 9223372036854775807);  view_14 = None\n",
            "    le_3: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_3 = None\n",
            "    slice_12: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_11, 2, 0, 9223372036854775807);  slice_11 = None\n",
            "    slice_13: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_12, 3, 1, 9223372036854775807);  slice_12 = None\n",
            "    sub_19: \"Sym(s12 - 5)\" = sym_size_int_7 - 5\n",
            "    sym_max_6: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_19, 0);  sub_19 = None\n",
            "    sub_20: \"Sym(5 - s12)\" = 5 - sym_size_int_7\n",
            "    sym_max_7: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_20, 0);  sub_20 = None\n",
            "    mul_25: \"Sym(2*s12)\" = 2 * sym_size_int_7;  sym_size_int_7 = None\n",
            "    add_12: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_7 + mul_25;  mul_25 = None\n",
            "    sub_21: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_12 - 1;  add_12 = None\n",
            "    gt_3: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_6 > 0;  gt_3 = None\n",
            "    pad_13: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg12_1, [0, 0, sym_max_6, sym_max_6, 0, 0]);  arg12_1 = sym_max_6 = None\n",
            "    slice_14: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_13, 1, sym_max_7, sub_21);  pad_13 = sym_max_7 = sub_21 = None\n",
            "    unsqueeze_8: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_14, 0);  slice_14 = None\n",
            "    matmul_7: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_13, unsqueeze_8);  slice_13 = unsqueeze_8 = None\n",
            "    add_13: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_6, matmul_7);  matmul_6 = matmul_7 = None\n",
            "    transpose_16: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_13, 2, 3);  add_13 = None\n",
            "    contiguous_1: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_16);  transpose_16 = None\n",
            "    view_15: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_1, [sym_size_int_1, 192, sym_size_int_6]);  contiguous_1 = sym_size_int_6 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_9: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_15, arg19_1, arg20_1);  view_15 = arg19_1 = arg20_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_5: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_9, 0.1, False);  conv1d_9 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_14: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_10, dropout_5);  transpose_10 = dropout_5 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_17: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_14, 1, -1);  add_14 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_2: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_17, [192], arg63_1, arg64_1);  transpose_17 = arg63_1 = arg64_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_18: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_2, 1, -1);  layer_norm_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_26: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_18, type_as)\n",
            "    pad_14: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_26, [1, 1, 0, 0, 0, 0]);  mul_26 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_10: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_14, arg77_1, arg78_1);  pad_14 = arg77_1 = arg78_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_1: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_10);  conv1d_10 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_6: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_1, 0.1, False);  relu_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_27: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_6, type_as);  dropout_6 = None\n",
            "    pad_15: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_27, [1, 1, 0, 0, 0, 0]);  mul_27 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_11: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_15, arg79_1, arg80_1);  pad_15 = arg79_1 = arg80_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_28: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_11, type_as);  conv1d_11 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_7: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_28, 0.1, False);  mul_28 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_15: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_18, dropout_7);  transpose_18 = dropout_7 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_19: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_15, 1, -1);  add_15 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_3: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_19, [192], arg99_1, arg100_1);  transpose_19 = arg99_1 = arg100_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_20: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_3, 1, -1);  layer_norm_3 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_12: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_20, arg23_1, arg24_1);  arg23_1 = arg24_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_13: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_20, arg25_1, arg26_1);  arg25_1 = arg26_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_14: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_20, arg27_1, arg28_1);  arg27_1 = arg28_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_8: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_12, 2)\n",
            "    view_16: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_12, [sym_size_int_1, 2, 96, sym_size_int_8]);  conv1d_12 = None\n",
            "    transpose_21: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_16, 2, 3);  view_16 = None\n",
            "    sym_size_int_9: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_13, 2)\n",
            "    view_17: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_13, [sym_size_int_1, 2, 96, sym_size_int_9]);  conv1d_13 = None\n",
            "    transpose_22: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_17, 2, 3);  view_17 = None\n",
            "    view_18: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_14, [sym_size_int_1, 2, 96, sym_size_int_9]);  conv1d_14 = None\n",
            "    transpose_23: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_18, 2, 3);  view_18 = None\n",
            "    div_4: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_21, 9.797958971132712)\n",
            "    transpose_24: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_22, -2, -1);  transpose_22 = None\n",
            "    matmul_8: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_4, transpose_24);  div_4 = transpose_24 = None\n",
            "    eq_14: \"Sym(True)\" = sym_size_int_9 == sym_size_int_8;  eq_14 = None\n",
            "    sub_22: \"Sym(s12 - 5)\" = sym_size_int_9 - 5\n",
            "    sym_max_8: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_22, 0);  sub_22 = None\n",
            "    sub_23: \"Sym(5 - s12)\" = 5 - sym_size_int_9\n",
            "    sym_max_9: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_23, 0);  sub_23 = None\n",
            "    mul_29: \"Sym(2*s12)\" = 2 * sym_size_int_9\n",
            "    add_16: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_9 + mul_29;  mul_29 = None\n",
            "    sub_24: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_16 - 1;  add_16 = None\n",
            "    gt_4: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_8 > 0;  gt_4 = None\n",
            "    pad_16: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg21_1, [0, 0, sym_max_8, sym_max_8, 0, 0]);  arg21_1 = sym_max_8 = None\n",
            "    slice_15: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_16, 1, sym_max_9, sub_24);  pad_16 = sym_max_9 = sub_24 = None\n",
            "    div_5: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_21, 9.797958971132712);  transpose_21 = None\n",
            "    unsqueeze_9: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_15, 0);  slice_15 = None\n",
            "    transpose_25: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_9, -2, -1);  unsqueeze_9 = None\n",
            "    matmul_9: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_5, transpose_25);  div_5 = transpose_25 = None\n",
            "    pad_17: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_9, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_9 = None\n",
            "    mul_30: \"Sym(2*s12)\" = sym_size_int_8 * 2\n",
            "    mul_31: \"Sym(2*s12**2)\" = mul_30 * sym_size_int_8;  mul_30 = None\n",
            "    view_19: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_17, [sym_size_int_1, 2, mul_31]);  pad_17 = mul_31 = None\n",
            "    sub_25: \"Sym(s12 - 1)\" = sym_size_int_8 - 1\n",
            "    pad_18: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_19, [0, sub_25, 0, 0, 0, 0]);  view_19 = sub_25 = None\n",
            "    add_17: \"Sym(s12 + 1)\" = sym_size_int_8 + 1\n",
            "    mul_32: \"Sym(2*s12)\" = 2 * sym_size_int_8\n",
            "    sub_26: \"Sym(2*s12 - 1)\" = mul_32 - 1;  mul_32 = None\n",
            "    view_20: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_18, [sym_size_int_1, 2, add_17, sub_26]);  pad_18 = add_17 = sub_26 = None\n",
            "    sub_27: \"Sym(s12 - 1)\" = sym_size_int_8 - 1\n",
            "    slice_16: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_20, 2, None, sym_size_int_8);  view_20 = None\n",
            "    slice_17: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_16, 3, sub_27);  slice_16 = sub_27 = None\n",
            "    add_18: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_8, slice_17);  matmul_8 = slice_17 = None\n",
            "    eq_15: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill_2: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_18, eq_15, -10000.0);  add_18 = eq_15 = None\n",
            "    softmax_2: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_2, -1);  masked_fill_2 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_8: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_2, 0.1, False);  softmax_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_10: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_8, transpose_23);  transpose_23 = None\n",
            "    sub_28: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_19: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_8, [0, sub_28, 0, 0, 0, 0, 0, 0]);  dropout_8 = sub_28 = None\n",
            "    mul_33: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_29: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_34: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_29;  sub_29 = None\n",
            "    add_19: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_33 + mul_34;  mul_33 = mul_34 = None\n",
            "    view_21: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_19, [sym_size_int_5, 2, add_19]);  pad_19 = add_19 = None\n",
            "    pad_20: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_21, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_21 = None\n",
            "    mul_35: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_22: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_20, [sym_size_int_5, 2, sym_size_int_4, mul_35]);  pad_20 = mul_35 = None\n",
            "    le_4: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le_4 = None\n",
            "    slice_18: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_22, 0, 0, 9223372036854775807);  view_22 = None\n",
            "    le_5: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_5 = None\n",
            "    slice_19: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_18, 2, 0, 9223372036854775807);  slice_18 = None\n",
            "    slice_20: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_19, 3, 1, 9223372036854775807);  slice_19 = None\n",
            "    sub_30: \"Sym(s12 - 5)\" = sym_size_int_9 - 5\n",
            "    sym_max_10: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_30, 0);  sub_30 = None\n",
            "    sub_31: \"Sym(5 - s12)\" = 5 - sym_size_int_9\n",
            "    sym_max_11: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_31, 0);  sub_31 = None\n",
            "    mul_36: \"Sym(2*s12)\" = 2 * sym_size_int_9;  sym_size_int_9 = None\n",
            "    add_20: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_11 + mul_36;  mul_36 = None\n",
            "    sub_32: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_20 - 1;  add_20 = None\n",
            "    gt_5: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_10 > 0;  gt_5 = None\n",
            "    pad_21: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg22_1, [0, 0, sym_max_10, sym_max_10, 0, 0]);  arg22_1 = sym_max_10 = None\n",
            "    slice_21: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_21, 1, sym_max_11, sub_32);  pad_21 = sym_max_11 = sub_32 = None\n",
            "    unsqueeze_10: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_21, 0);  slice_21 = None\n",
            "    matmul_11: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_20, unsqueeze_10);  slice_20 = unsqueeze_10 = None\n",
            "    add_21: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_10, matmul_11);  matmul_10 = matmul_11 = None\n",
            "    transpose_26: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_21, 2, 3);  add_21 = None\n",
            "    contiguous_2: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_26);  transpose_26 = None\n",
            "    view_23: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_2, [sym_size_int_1, 192, sym_size_int_8]);  contiguous_2 = sym_size_int_8 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_15: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_23, arg29_1, arg30_1);  view_23 = arg29_1 = arg30_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_9: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_15, 0.1, False);  conv1d_15 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_22: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_20, dropout_9);  transpose_20 = dropout_9 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_27: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_22, 1, -1);  add_22 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_4: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_27, [192], arg65_1, arg66_1);  transpose_27 = arg65_1 = arg66_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_28: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_4, 1, -1);  layer_norm_4 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_37: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_28, type_as)\n",
            "    pad_22: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_37, [1, 1, 0, 0, 0, 0]);  mul_37 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_16: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_22, arg81_1, arg82_1);  pad_22 = arg81_1 = arg82_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_2: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_16);  conv1d_16 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_10: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_2, 0.1, False);  relu_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_38: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_10, type_as);  dropout_10 = None\n",
            "    pad_23: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_38, [1, 1, 0, 0, 0, 0]);  mul_38 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_17: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_23, arg83_1, arg84_1);  pad_23 = arg83_1 = arg84_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_39: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_17, type_as);  conv1d_17 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_11: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_39, 0.1, False);  mul_39 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_23: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_28, dropout_11);  transpose_28 = dropout_11 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_29: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_23, 1, -1);  add_23 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_5: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_29, [192], arg101_1, arg102_1);  transpose_29 = arg101_1 = arg102_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_30: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_5, 1, -1);  layer_norm_5 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_18: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_30, arg33_1, arg34_1);  arg33_1 = arg34_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_19: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_30, arg35_1, arg36_1);  arg35_1 = arg36_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_20: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_30, arg37_1, arg38_1);  arg37_1 = arg38_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_10: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_18, 2)\n",
            "    view_24: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_18, [sym_size_int_1, 2, 96, sym_size_int_10]);  conv1d_18 = None\n",
            "    transpose_31: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_24, 2, 3);  view_24 = None\n",
            "    sym_size_int_11: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_19, 2)\n",
            "    view_25: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_19, [sym_size_int_1, 2, 96, sym_size_int_11]);  conv1d_19 = None\n",
            "    transpose_32: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_25, 2, 3);  view_25 = None\n",
            "    view_26: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_20, [sym_size_int_1, 2, 96, sym_size_int_11]);  conv1d_20 = None\n",
            "    transpose_33: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_26, 2, 3);  view_26 = None\n",
            "    div_6: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_31, 9.797958971132712)\n",
            "    transpose_34: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_32, -2, -1);  transpose_32 = None\n",
            "    matmul_12: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_6, transpose_34);  div_6 = transpose_34 = None\n",
            "    eq_16: \"Sym(True)\" = sym_size_int_11 == sym_size_int_10;  eq_16 = None\n",
            "    sub_33: \"Sym(s12 - 5)\" = sym_size_int_11 - 5\n",
            "    sym_max_12: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_33, 0);  sub_33 = None\n",
            "    sub_34: \"Sym(5 - s12)\" = 5 - sym_size_int_11\n",
            "    sym_max_13: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_34, 0);  sub_34 = None\n",
            "    mul_40: \"Sym(2*s12)\" = 2 * sym_size_int_11\n",
            "    add_24: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_13 + mul_40;  mul_40 = None\n",
            "    sub_35: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_24 - 1;  add_24 = None\n",
            "    gt_6: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_12 > 0;  gt_6 = None\n",
            "    pad_24: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg31_1, [0, 0, sym_max_12, sym_max_12, 0, 0]);  arg31_1 = sym_max_12 = None\n",
            "    slice_22: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_24, 1, sym_max_13, sub_35);  pad_24 = sym_max_13 = sub_35 = None\n",
            "    div_7: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_31, 9.797958971132712);  transpose_31 = None\n",
            "    unsqueeze_11: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_22, 0);  slice_22 = None\n",
            "    transpose_35: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_11, -2, -1);  unsqueeze_11 = None\n",
            "    matmul_13: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_7, transpose_35);  div_7 = transpose_35 = None\n",
            "    pad_25: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_13, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_13 = None\n",
            "    mul_41: \"Sym(2*s12)\" = sym_size_int_10 * 2\n",
            "    mul_42: \"Sym(2*s12**2)\" = mul_41 * sym_size_int_10;  mul_41 = None\n",
            "    view_27: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_25, [sym_size_int_1, 2, mul_42]);  pad_25 = mul_42 = None\n",
            "    sub_36: \"Sym(s12 - 1)\" = sym_size_int_10 - 1\n",
            "    pad_26: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_27, [0, sub_36, 0, 0, 0, 0]);  view_27 = sub_36 = None\n",
            "    add_25: \"Sym(s12 + 1)\" = sym_size_int_10 + 1\n",
            "    mul_43: \"Sym(2*s12)\" = 2 * sym_size_int_10\n",
            "    sub_37: \"Sym(2*s12 - 1)\" = mul_43 - 1;  mul_43 = None\n",
            "    view_28: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_26, [sym_size_int_1, 2, add_25, sub_37]);  pad_26 = add_25 = sub_37 = None\n",
            "    sub_38: \"Sym(s12 - 1)\" = sym_size_int_10 - 1\n",
            "    slice_23: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_28, 2, None, sym_size_int_10);  view_28 = None\n",
            "    slice_24: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_23, 3, sub_38);  slice_23 = sub_38 = None\n",
            "    add_26: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_12, slice_24);  matmul_12 = slice_24 = None\n",
            "    eq_17: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill_3: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_26, eq_17, -10000.0);  add_26 = eq_17 = None\n",
            "    softmax_3: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_3, -1);  masked_fill_3 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_12: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_3, 0.1, False);  softmax_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_14: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_12, transpose_33);  transpose_33 = None\n",
            "    sub_39: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_27: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_12, [0, sub_39, 0, 0, 0, 0, 0, 0]);  dropout_12 = sub_39 = None\n",
            "    mul_44: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_40: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_45: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_40;  sub_40 = None\n",
            "    add_27: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_44 + mul_45;  mul_44 = mul_45 = None\n",
            "    view_29: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_27, [sym_size_int_5, 2, add_27]);  pad_27 = add_27 = None\n",
            "    pad_28: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_29, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_29 = None\n",
            "    mul_46: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_30: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_28, [sym_size_int_5, 2, sym_size_int_4, mul_46]);  pad_28 = mul_46 = None\n",
            "    le_6: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le_6 = None\n",
            "    slice_25: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_30, 0, 0, 9223372036854775807);  view_30 = None\n",
            "    le_7: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_7 = None\n",
            "    slice_26: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_25, 2, 0, 9223372036854775807);  slice_25 = None\n",
            "    slice_27: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_26, 3, 1, 9223372036854775807);  slice_26 = None\n",
            "    sub_41: \"Sym(s12 - 5)\" = sym_size_int_11 - 5\n",
            "    sym_max_14: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_41, 0);  sub_41 = None\n",
            "    sub_42: \"Sym(5 - s12)\" = 5 - sym_size_int_11\n",
            "    sym_max_15: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_42, 0);  sub_42 = None\n",
            "    mul_47: \"Sym(2*s12)\" = 2 * sym_size_int_11;  sym_size_int_11 = None\n",
            "    add_28: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_15 + mul_47;  mul_47 = None\n",
            "    sub_43: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_28 - 1;  add_28 = None\n",
            "    gt_7: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_14 > 0;  gt_7 = None\n",
            "    pad_29: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg32_1, [0, 0, sym_max_14, sym_max_14, 0, 0]);  arg32_1 = sym_max_14 = None\n",
            "    slice_28: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_29, 1, sym_max_15, sub_43);  pad_29 = sym_max_15 = sub_43 = None\n",
            "    unsqueeze_12: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_28, 0);  slice_28 = None\n",
            "    matmul_15: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_27, unsqueeze_12);  slice_27 = unsqueeze_12 = None\n",
            "    add_29: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_14, matmul_15);  matmul_14 = matmul_15 = None\n",
            "    transpose_36: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_29, 2, 3)\n",
            "    sym_numel_default_1: \"Sym(192*s12*s31)\" = torch.ops.aten.sym_numel.default(transpose_36)\n",
            "    eq_18: \"Sym(False)\" = sym_numel_default_1 == 0;  eq_18 = None\n",
            "    eq_19: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1;  eq_19 = None\n",
            "    eq_20: \"Sym(False)\" = sym_numel_default_1 == 0\n",
            "    eq_21: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1\n",
            "    or__5: \"Sym(Eq(s12, 1))\" = eq_21 | False;  eq_21 = None\n",
            "    and__4: \"Sym(Eq(s12, 1))\" = True & or__5;  or__5 = None\n",
            "    eq_22: \"Sym(Eq(s12, 1))\" = 1 == sym_size_int_4\n",
            "    or__6: \"Sym(Eq(s12, 1))\" = False | eq_22;  eq_22 = None\n",
            "    and__5: \"Sym(Eq(s12, 1))\" = and__4 & or__6;  and__4 = or__6 = None\n",
            "    mul_48: \"Sym(96*s12)\" = sym_size_int_4 * 96\n",
            "    sym_stride_int_2: \"Sym(96*s12)\" = torch.ops.aten.sym_stride.int(add_29, 1)\n",
            "    eq_23: \"Sym(True)\" = sym_stride_int_2 == mul_48;  sym_stride_int_2 = None\n",
            "    or__7: \"Sym(True)\" = False | eq_23;  eq_23 = None\n",
            "    and__6: \"Sym(Eq(s12, 1))\" = and__5 & or__7;  and__5 = or__7 = None\n",
            "    mul_49: \"Sym(192*s12)\" = mul_48 * 2;  mul_48 = None\n",
            "    eq_24: \"Sym(Eq(s31, 1))\" = sym_size_int_5 == 1\n",
            "    sym_stride_int_3: \"Sym(192*s12)\" = torch.ops.aten.sym_stride.int(add_29, 0);  add_29 = None\n",
            "    eq_25: \"Sym(True)\" = sym_stride_int_3 == mul_49;  sym_stride_int_3 = None\n",
            "    or__8: \"Sym(True)\" = eq_24 | eq_25;  eq_24 = eq_25 = None\n",
            "    and__7: \"Sym(Eq(s12, 1))\" = and__6 & or__8;  and__6 = or__8 = None\n",
            "    mul_50: \"Sym(192*s12*s31)\" = mul_49 * sym_size_int_5;  mul_49 = mul_50 = None\n",
            "    or__9: \"Sym(Eq(s12, 1))\" = and__7 | eq_20;  and__7 = eq_20 = or__9 = None\n",
            "    eq_26: \"Sym(False)\" = sym_numel_default_1 == 0;  sym_numel_default_1 = eq_26 = None\n",
            "    eq_27: \"Sym(Eq(s12, 1))\" = sym_size_int_4 == 1;  eq_27 = None\n",
            "    contiguous_3: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_36);  transpose_36 = None\n",
            "    view_31: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_3, [sym_size_int_1, 192, sym_size_int_10]);  contiguous_3 = sym_size_int_10 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_21: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_31, arg39_1, arg40_1);  view_31 = arg39_1 = arg40_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_13: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_21, 0.1, False);  conv1d_21 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_30: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_30, dropout_13);  transpose_30 = dropout_13 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_37: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_30, 1, -1);  add_30 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_6: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_37, [192], arg67_1, arg68_1);  transpose_37 = arg67_1 = arg68_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_38: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_6, 1, -1);  layer_norm_6 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_51: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_38, type_as)\n",
            "    pad_30: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_51, [1, 1, 0, 0, 0, 0]);  mul_51 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_22: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_30, arg85_1, arg86_1);  pad_30 = arg85_1 = arg86_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_3: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_22);  conv1d_22 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_14: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_3, 0.1, False);  relu_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_52: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_14, type_as);  dropout_14 = None\n",
            "    pad_31: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_52, [1, 1, 0, 0, 0, 0]);  mul_52 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_23: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_31, arg87_1, arg88_1);  pad_31 = arg87_1 = arg88_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_53: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_23, type_as);  conv1d_23 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_15: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_53, 0.1, False);  mul_53 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_31: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_38, dropout_15);  transpose_38 = dropout_15 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_39: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_31, 1, -1);  add_31 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_7: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_39, [192], arg103_1, arg104_1);  transpose_39 = arg103_1 = arg104_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_40: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_7, 1, -1);  layer_norm_7 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_24: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_40, arg43_1, arg44_1);  arg43_1 = arg44_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_25: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_40, arg45_1, arg46_1);  arg45_1 = arg46_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_26: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_40, arg47_1, arg48_1);  arg47_1 = arg48_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_12: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_24, 2)\n",
            "    view_32: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_24, [sym_size_int_1, 2, 96, sym_size_int_12]);  conv1d_24 = None\n",
            "    transpose_41: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_32, 2, 3);  view_32 = None\n",
            "    sym_size_int_13: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_25, 2)\n",
            "    view_33: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_25, [sym_size_int_1, 2, 96, sym_size_int_13]);  conv1d_25 = None\n",
            "    transpose_42: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_33, 2, 3);  view_33 = None\n",
            "    view_34: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_26, [sym_size_int_1, 2, 96, sym_size_int_13]);  conv1d_26 = None\n",
            "    transpose_43: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_34, 2, 3);  view_34 = None\n",
            "    div_8: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_41, 9.797958971132712)\n",
            "    transpose_44: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_42, -2, -1);  transpose_42 = None\n",
            "    matmul_16: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_8, transpose_44);  div_8 = transpose_44 = None\n",
            "    eq_28: \"Sym(True)\" = sym_size_int_13 == sym_size_int_12;  eq_28 = None\n",
            "    sub_44: \"Sym(s12 - 5)\" = sym_size_int_13 - 5\n",
            "    sym_max_16: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_44, 0);  sub_44 = None\n",
            "    sub_45: \"Sym(5 - s12)\" = 5 - sym_size_int_13\n",
            "    sym_max_17: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_45, 0);  sub_45 = None\n",
            "    mul_54: \"Sym(2*s12)\" = 2 * sym_size_int_13\n",
            "    add_32: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_17 + mul_54;  mul_54 = None\n",
            "    sub_46: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_32 - 1;  add_32 = None\n",
            "    gt_8: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_16 > 0;  gt_8 = None\n",
            "    pad_32: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg41_1, [0, 0, sym_max_16, sym_max_16, 0, 0]);  arg41_1 = sym_max_16 = None\n",
            "    slice_29: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_32, 1, sym_max_17, sub_46);  pad_32 = sym_max_17 = sub_46 = None\n",
            "    div_9: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_41, 9.797958971132712);  transpose_41 = None\n",
            "    unsqueeze_13: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_29, 0);  slice_29 = None\n",
            "    transpose_45: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_13, -2, -1);  unsqueeze_13 = None\n",
            "    matmul_17: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_9, transpose_45);  div_9 = transpose_45 = None\n",
            "    pad_33: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_17, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_17 = None\n",
            "    mul_55: \"Sym(2*s12)\" = sym_size_int_12 * 2\n",
            "    mul_56: \"Sym(2*s12**2)\" = mul_55 * sym_size_int_12;  mul_55 = None\n",
            "    view_35: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_33, [sym_size_int_1, 2, mul_56]);  pad_33 = mul_56 = None\n",
            "    sub_47: \"Sym(s12 - 1)\" = sym_size_int_12 - 1\n",
            "    pad_34: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_35, [0, sub_47, 0, 0, 0, 0]);  view_35 = sub_47 = None\n",
            "    add_33: \"Sym(s12 + 1)\" = sym_size_int_12 + 1\n",
            "    mul_57: \"Sym(2*s12)\" = 2 * sym_size_int_12\n",
            "    sub_48: \"Sym(2*s12 - 1)\" = mul_57 - 1;  mul_57 = None\n",
            "    view_36: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_34, [sym_size_int_1, 2, add_33, sub_48]);  pad_34 = add_33 = sub_48 = None\n",
            "    sub_49: \"Sym(s12 - 1)\" = sym_size_int_12 - 1\n",
            "    slice_30: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_36, 2, None, sym_size_int_12);  view_36 = None\n",
            "    slice_31: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_30, 3, sub_49);  slice_30 = sub_49 = None\n",
            "    add_34: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_16, slice_31);  matmul_16 = slice_31 = None\n",
            "    eq_29: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0)\n",
            "    masked_fill_4: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_34, eq_29, -10000.0);  add_34 = eq_29 = None\n",
            "    softmax_4: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_4, -1);  masked_fill_4 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_16: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_4, 0.1, False);  softmax_4 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_18: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_16, transpose_43);  transpose_43 = None\n",
            "    sub_50: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_35: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_16, [0, sub_50, 0, 0, 0, 0, 0, 0]);  dropout_16 = sub_50 = None\n",
            "    mul_58: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_51: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_59: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_51;  sub_51 = None\n",
            "    add_35: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_58 + mul_59;  mul_58 = mul_59 = None\n",
            "    view_37: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_35, [sym_size_int_5, 2, add_35]);  pad_35 = add_35 = None\n",
            "    pad_36: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_37, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_37 = None\n",
            "    mul_60: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_38: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_36, [sym_size_int_5, 2, sym_size_int_4, mul_60]);  pad_36 = mul_60 = None\n",
            "    le_8: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  le_8 = None\n",
            "    slice_32: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_38, 0, 0, 9223372036854775807);  view_38 = None\n",
            "    le_9: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  le_9 = None\n",
            "    slice_33: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_32, 2, 0, 9223372036854775807);  slice_32 = None\n",
            "    slice_34: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_33, 3, 1, 9223372036854775807);  slice_33 = None\n",
            "    sub_52: \"Sym(s12 - 5)\" = sym_size_int_13 - 5\n",
            "    sym_max_18: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_52, 0);  sub_52 = None\n",
            "    sub_53: \"Sym(5 - s12)\" = 5 - sym_size_int_13\n",
            "    sym_max_19: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_53, 0);  sub_53 = None\n",
            "    mul_61: \"Sym(2*s12)\" = 2 * sym_size_int_13;  sym_size_int_13 = None\n",
            "    add_36: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_19 + mul_61;  mul_61 = None\n",
            "    sub_54: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_36 - 1;  add_36 = None\n",
            "    gt_9: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_18 > 0;  gt_9 = None\n",
            "    pad_37: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg42_1, [0, 0, sym_max_18, sym_max_18, 0, 0]);  arg42_1 = sym_max_18 = None\n",
            "    slice_35: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_37, 1, sym_max_19, sub_54);  pad_37 = sym_max_19 = sub_54 = None\n",
            "    unsqueeze_14: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_35, 0);  slice_35 = None\n",
            "    matmul_19: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_34, unsqueeze_14);  slice_34 = unsqueeze_14 = None\n",
            "    add_37: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_18, matmul_19);  matmul_18 = matmul_19 = None\n",
            "    transpose_46: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_37, 2, 3);  add_37 = None\n",
            "    contiguous_4: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_46);  transpose_46 = None\n",
            "    view_39: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_4, [sym_size_int_1, 192, sym_size_int_12]);  contiguous_4 = sym_size_int_12 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_27: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_39, arg49_1, arg50_1);  view_39 = arg49_1 = arg50_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_17: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_27, 0.1, False);  conv1d_27 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_38: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_40, dropout_17);  transpose_40 = dropout_17 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_47: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_38, 1, -1);  add_38 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_8: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_47, [192], arg69_1, arg70_1);  transpose_47 = arg69_1 = arg70_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_48: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_8, 1, -1);  layer_norm_8 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_62: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_48, type_as)\n",
            "    pad_38: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_62, [1, 1, 0, 0, 0, 0]);  mul_62 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_28: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_38, arg89_1, arg90_1);  pad_38 = arg89_1 = arg90_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_4: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_28);  conv1d_28 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_18: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_4, 0.1, False);  relu_4 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_63: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_18, type_as);  dropout_18 = None\n",
            "    pad_39: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_63, [1, 1, 0, 0, 0, 0]);  mul_63 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_29: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_39, arg91_1, arg92_1);  pad_39 = arg91_1 = arg92_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_64: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_29, type_as);  conv1d_29 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_19: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_64, 0.1, False);  mul_64 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_39: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_48, dropout_19);  transpose_48 = dropout_19 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_49: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_39, 1, -1);  add_39 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_9: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_49, [192], arg105_1, arg106_1);  transpose_49 = arg105_1 = arg106_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_50: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_9, 1, -1);  layer_norm_9 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_30: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_50, arg53_1, arg54_1);  arg53_1 = arg54_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_31: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_50, arg55_1, arg56_1);  arg55_1 = arg56_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_32: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(transpose_50, arg57_1, arg58_1);  arg57_1 = arg58_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    sym_size_int_14: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_30, 2)\n",
            "    view_40: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_30, [sym_size_int_1, 2, 96, sym_size_int_14]);  conv1d_30 = None\n",
            "    transpose_51: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_40, 2, 3);  view_40 = None\n",
            "    sym_size_int_15: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_31, 2)\n",
            "    view_41: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_31, [sym_size_int_1, 2, 96, sym_size_int_15]);  conv1d_31 = None\n",
            "    transpose_52: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_41, 2, 3);  view_41 = None\n",
            "    view_42: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.view.default(conv1d_32, [sym_size_int_1, 2, 96, sym_size_int_15]);  conv1d_32 = None\n",
            "    transpose_53: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.transpose.int(view_42, 2, 3);  view_42 = None\n",
            "    div_10: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_51, 9.797958971132712)\n",
            "    transpose_54: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(transpose_52, -2, -1);  transpose_52 = None\n",
            "    matmul_20: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.matmul.default(div_10, transpose_54);  div_10 = transpose_54 = None\n",
            "    eq_30: \"Sym(True)\" = sym_size_int_15 == sym_size_int_14;  eq_30 = None\n",
            "    sub_55: \"Sym(s12 - 5)\" = sym_size_int_15 - 5\n",
            "    sym_max_20: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_55, 0);  sub_55 = None\n",
            "    sub_56: \"Sym(5 - s12)\" = 5 - sym_size_int_15\n",
            "    sym_max_21: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_56, 0);  sub_56 = None\n",
            "    mul_65: \"Sym(2*s12)\" = 2 * sym_size_int_15\n",
            "    add_40: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_21 + mul_65;  mul_65 = None\n",
            "    sub_57: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_40 - 1;  add_40 = None\n",
            "    gt_10: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_20 > 0;  gt_10 = None\n",
            "    pad_40: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg51_1, [0, 0, sym_max_20, sym_max_20, 0, 0]);  arg51_1 = sym_max_20 = None\n",
            "    slice_36: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_40, 1, sym_max_21, sub_57);  pad_40 = sym_max_21 = sub_57 = None\n",
            "    div_11: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.div.Tensor(transpose_51, 9.797958971132712);  transpose_51 = None\n",
            "    unsqueeze_15: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_36, 0);  slice_36 = None\n",
            "    transpose_55: \"f32[1, 1, 96, 2*s12 - 1]\" = torch.ops.aten.transpose.int(unsqueeze_15, -2, -1);  unsqueeze_15 = None\n",
            "    matmul_21: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.matmul.default(div_11, transpose_55);  div_11 = transpose_55 = None\n",
            "    pad_41: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.pad.default(matmul_21, [0, 1, 0, 0, 0, 0, 0, 0]);  matmul_21 = None\n",
            "    mul_66: \"Sym(2*s12)\" = sym_size_int_14 * 2\n",
            "    mul_67: \"Sym(2*s12**2)\" = mul_66 * sym_size_int_14;  mul_66 = None\n",
            "    view_43: \"f32[s31, 2, 2*s12**2]\" = torch.ops.aten.view.default(pad_41, [sym_size_int_1, 2, mul_67]);  pad_41 = mul_67 = None\n",
            "    sub_58: \"Sym(s12 - 1)\" = sym_size_int_14 - 1\n",
            "    pad_42: \"f32[s31, 2, 2*s12**2 + s12 - 1]\" = torch.ops.aten.pad.default(view_43, [0, sub_58, 0, 0, 0, 0]);  view_43 = sub_58 = None\n",
            "    add_41: \"Sym(s12 + 1)\" = sym_size_int_14 + 1\n",
            "    mul_68: \"Sym(2*s12)\" = 2 * sym_size_int_14\n",
            "    sub_59: \"Sym(2*s12 - 1)\" = mul_68 - 1;  mul_68 = None\n",
            "    view_44: \"f32[s31, 2, s12 + 1, 2*s12 - 1]\" = torch.ops.aten.view.default(pad_42, [sym_size_int_1, 2, add_41, sub_59]);  pad_42 = add_41 = sub_59 = None\n",
            "    sub_60: \"Sym(s12 - 1)\" = sym_size_int_14 - 1\n",
            "    slice_37: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(view_44, 2, None, sym_size_int_14);  view_44 = None\n",
            "    slice_38: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.slice.Tensor(slice_37, 3, sub_60);  slice_37 = sub_60 = None\n",
            "    add_42: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.add.Tensor(matmul_20, slice_38);  matmul_20 = slice_38 = None\n",
            "    eq_31: \"b8[s31, 1, s12, s12]\" = torch.ops.aten.eq.Scalar(mul_2, 0);  mul_2 = None\n",
            "    masked_fill_5: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.masked_fill.Scalar(add_42, eq_31, -10000.0);  add_42 = eq_31 = None\n",
            "    softmax_5: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.softmax.int(masked_fill_5, -1);  masked_fill_5 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_20: \"f32[s31, 2, s12, s12]\" = torch.ops.aten.dropout.default(softmax_5, 0.1, False);  softmax_5 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:220 in forward, code: x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
            "    matmul_22: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(dropout_20, transpose_53);  transpose_53 = None\n",
            "    sub_61: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    pad_43: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.pad.default(dropout_20, [0, sub_61, 0, 0, 0, 0, 0, 0]);  dropout_20 = sub_61 = None\n",
            "    mul_69: \"Sym(s12**2)\" = sym_size_int_4 * sym_size_int_4\n",
            "    sub_62: \"Sym(s12 - 1)\" = sym_size_int_4 - 1\n",
            "    mul_70: \"Sym(s12*(s12 - 1))\" = sym_size_int_4 * sub_62;  sub_62 = None\n",
            "    add_43: \"Sym(s12**2 + s12*(s12 - 1))\" = mul_69 + mul_70;  mul_69 = mul_70 = None\n",
            "    view_45: \"f32[s31, 2, s12**2 + s12*(s12 - 1)]\" = torch.ops.aten.view.default(pad_43, [sym_size_int_5, 2, add_43]);  pad_43 = add_43 = None\n",
            "    pad_44: \"f32[s31, 2, s12**2 + s12*(s12 - 1) + s12]\" = torch.ops.aten.pad.default(view_45, [sym_size_int_4, 0, 0, 0, 0, 0]);  view_45 = None\n",
            "    mul_71: \"Sym(2*s12)\" = 2 * sym_size_int_4\n",
            "    view_46: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.view.default(pad_44, [sym_size_int_5, 2, sym_size_int_4, mul_71]);  pad_44 = mul_71 = None\n",
            "    le_10: \"Sym(s31 <= 9223372036854775807)\" = sym_size_int_5 <= 9223372036854775807;  sym_size_int_5 = le_10 = None\n",
            "    slice_39: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(view_46, 0, 0, 9223372036854775807);  view_46 = None\n",
            "    le_11: \"Sym(s12 <= 9223372036854775807)\" = sym_size_int_4 <= 9223372036854775807;  sym_size_int_4 = le_11 = None\n",
            "    slice_40: \"f32[s31, 2, s12, 2*s12]\" = torch.ops.aten.slice.Tensor(slice_39, 2, 0, 9223372036854775807);  slice_39 = None\n",
            "    slice_41: \"f32[s31, 2, s12, 2*s12 - 1]\" = torch.ops.aten.slice.Tensor(slice_40, 3, 1, 9223372036854775807);  slice_40 = None\n",
            "    sub_63: \"Sym(s12 - 5)\" = sym_size_int_15 - 5\n",
            "    sym_max_22: \"Sym(Max(0, s12 - 5))\" = torch.sym_max(sub_63, 0);  sub_63 = None\n",
            "    sub_64: \"Sym(5 - s12)\" = 5 - sym_size_int_15\n",
            "    sym_max_23: \"Sym(Max(0, 5 - s12))\" = torch.sym_max(sub_64, 0);  sub_64 = None\n",
            "    mul_72: \"Sym(2*s12)\" = 2 * sym_size_int_15;  sym_size_int_15 = None\n",
            "    add_44: \"Sym(2*s12 + Max(0, 5 - s12))\" = sym_max_23 + mul_72;  mul_72 = None\n",
            "    sub_65: \"Sym(2*s12 + Max(0, 5 - s12) - 1)\" = add_44 - 1;  add_44 = None\n",
            "    gt_11: \"Sym(Max(0, s12 - 5) > 0)\" = sym_max_22 > 0;  gt_11 = None\n",
            "    pad_45: \"f32[1, 2*Max(0, s12 - 5) + 9, 96]\" = torch.ops.aten.pad.default(arg52_1, [0, 0, sym_max_22, sym_max_22, 0, 0]);  arg52_1 = sym_max_22 = None\n",
            "    slice_42: \"f32[1, 2*s12 - 1, 96]\" = torch.ops.aten.slice.Tensor(pad_45, 1, sym_max_23, sub_65);  pad_45 = sym_max_23 = sub_65 = None\n",
            "    unsqueeze_16: \"f32[1, 1, 2*s12 - 1, 96]\" = torch.ops.aten.unsqueeze.default(slice_42, 0);  slice_42 = None\n",
            "    matmul_23: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.matmul.default(slice_41, unsqueeze_16);  slice_41 = unsqueeze_16 = None\n",
            "    add_45: \"f32[s31, 2, s12, 96]\" = torch.ops.aten.add.Tensor(matmul_22, matmul_23);  matmul_22 = matmul_23 = None\n",
            "    transpose_56: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.transpose.int(add_45, 2, 3);  add_45 = None\n",
            "    contiguous_5: \"f32[s31, 2, 96, s12]\" = torch.ops.aten.contiguous.default(transpose_56);  transpose_56 = None\n",
            "    view_47: \"f32[s31, 192, s12]\" = torch.ops.aten.view.default(contiguous_5, [sym_size_int_1, 192, sym_size_int_14]);  contiguous_5 = sym_size_int_14 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_33: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(view_47, arg59_1, arg60_1);  view_47 = arg59_1 = arg60_1 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_21: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(conv1d_33, 0.1, False);  conv1d_33 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:68 in forward, code: x = norm_layer_1(x + y)\n",
            "    add_46: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_50, dropout_21);  transpose_50 = dropout_21 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_57: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_46, 1, -1);  add_46 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_10: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_57, [192], arg71_1, arg72_1);  transpose_57 = arg71_1 = arg72_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_58: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_10, 1, -1);  layer_norm_10 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:390 in forward, code: padding1 = self._same_padding(x * x_mask)\n",
            "    mul_73: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_58, type_as)\n",
            "    pad_46: \"f32[s31, 192, s12 + 2]\" = torch.ops.aten.pad.default(mul_73, [1, 1, 0, 0, 0, 0]);  mul_73 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_34: \"f32[s31, 768, s12]\" = torch.ops.aten.conv1d.default(pad_46, arg93_1, arg94_1);  pad_46 = arg93_1 = arg94_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:397 in forward, code: x = torch.relu(x)\n",
            "    relu_5: \"f32[s31, 768, s12]\" = torch.ops.aten.relu.default(conv1d_34);  conv1d_34 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_22: \"f32[s31, 768, s12]\" = torch.ops.aten.dropout.default(relu_5, 0.1, False);  relu_5 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:403 in forward, code: padding2 = self._same_padding(x * x_mask)\n",
            "    mul_74: \"f32[s31, 768, s12]\" = torch.ops.aten.mul.Tensor(dropout_22, type_as);  dropout_22 = None\n",
            "    pad_47: \"f32[s31, 768, s12 + 2]\" = torch.ops.aten.pad.default(mul_74, [1, 1, 0, 0, 0, 0]);  mul_74 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_35: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(pad_47, arg95_1, arg96_1);  pad_47 = arg95_1 = arg96_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:407 in forward, code: return x * x_mask\n",
            "    mul_75: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_35, type_as);  conv1d_35 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_23: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(mul_75, 0.1, False);  mul_75 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:72 in forward, code: x = norm_layer_2(x + y)\n",
            "    add_47: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(transpose_58, dropout_23);  transpose_58 = dropout_23 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_59: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(add_47, 1, -1);  add_47 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_11: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_59, [192], arg107_1, arg108_1);  transpose_59 = arg107_1 = arg108_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_60: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_11, 1, -1);  layer_norm_11 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/attentions.py:73 in forward, code: x = x * x_mask\n",
            "    mul_76: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(transpose_60, type_as);  transpose_60 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_36: \"f32[s31, 384, s12]\" = torch.ops.aten.conv1d.default(mul_76, arg109_1, arg110_1);  arg109_1 = arg110_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:206 in forward, code: stats = self.proj(x) * x_mask\n",
            "    mul_77: \"f32[s31, 384, s12]\" = torch.ops.aten.mul.Tensor(conv1d_36, type_as);  conv1d_36 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:208 in forward, code: m, logs = torch.split(stats, self.out_channels, dim=1)\n",
            "    split = torch.ops.aten.split.Tensor(mul_77, 192, 1);  mul_77 = None\n",
            "    getitem: \"f32[s31, 192, s12]\" = split[0];  getitem = None\n",
            "    getitem_1: \"f32[s31, 192, s12]\" = split[1];  split = getitem_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:64 in forward, code: x = torch.detach(x)\n",
            "    detach: \"f32[s31, 192, s12]\" = torch.ops.aten.detach.default(mul_76);  mul_76 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_37: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(detach, arg624_1, arg625_1);  detach = arg624_1 = arg625_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_78: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_37, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_38: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_78, arg628_1, arg629_1, [1], [1], [1], 192);  mul_78 = arg628_1 = arg629_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_61: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_38, 1, -1);  conv1d_38 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_12: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_61, [192], arg640_1, arg641_1);  transpose_61 = arg640_1 = arg641_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_62: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_12, 1, -1);  layer_norm_12 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_62);  transpose_62 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_39: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu, arg634_1, arg635_1);  gelu = arg634_1 = arg635_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_63: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_39, 1, -1);  conv1d_39 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_13: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_63, [192], arg646_1, arg647_1);  transpose_63 = arg646_1 = arg647_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_64: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_13, 1, -1);  layer_norm_13 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_1: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_64);  transpose_64 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_24: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_1, 0.5, False);  gelu_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_48: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(conv1d_37, dropout_24);  conv1d_37 = dropout_24 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_79: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_48, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_40: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_79, arg630_1, arg631_1, [1], [3], [3], 192);  mul_79 = arg630_1 = arg631_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_65: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_40, 1, -1);  conv1d_40 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_14: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_65, [192], arg642_1, arg643_1);  transpose_65 = arg642_1 = arg643_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_66: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_14, 1, -1);  layer_norm_14 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_2: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_66);  transpose_66 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_41: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_2, arg636_1, arg637_1);  gelu_2 = arg636_1 = arg637_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_67: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_41, 1, -1);  conv1d_41 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_15: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_67, [192], arg648_1, arg649_1);  transpose_67 = arg648_1 = arg649_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_68: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_15, 1, -1);  layer_norm_15 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_3: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_68);  transpose_68 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_25: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_3, 0.5, False);  gelu_3 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_49: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_48, dropout_25);  add_48 = dropout_25 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_80: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_49, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_42: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_80, arg632_1, arg633_1, [1], [9], [9], 192);  mul_80 = arg632_1 = arg633_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_69: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_42, 1, -1);  conv1d_42 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_16: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_69, [192], arg644_1, arg645_1);  transpose_69 = arg644_1 = arg645_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_70: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_16, 1, -1);  layer_norm_16 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_4: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_70);  transpose_70 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_43: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_4, arg638_1, arg639_1);  gelu_4 = arg638_1 = arg639_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_71: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_43, 1, -1);  conv1d_43 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_17: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_71, [192], arg650_1, arg651_1);  transpose_71 = arg650_1 = arg651_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_72: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_17, 1, -1);  layer_norm_17 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_5: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_72);  transpose_72 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_26: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_5, 0.5, False);  gelu_5 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_50: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_49, dropout_26);  add_49 = dropout_26 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:129 in forward, code: return x * x_mask\n",
            "    mul_81: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_50, type_as);  add_50 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_44: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_81, arg626_1, arg627_1);  mul_81 = arg626_1 = arg627_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:70 in forward, code: x = self.proj(x) * x_mask\n",
            "    mul_82: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(conv1d_44, type_as)\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/models.py:111 in forward, code: z = torch.randn(x.size(0), 2, x.size(2)).type_as(x) * noise_scale\n",
            "    sym_size_int_16: \"Sym(s12)\" = torch.ops.aten.sym_size.int(conv1d_44, 2);  conv1d_44 = None\n",
            "    randn: \"f32[s31, 2, s12]\" = torch.ops.aten.randn.default([sym_size_int_1, 2, sym_size_int_16], device = device(type='cpu'), pin_memory = False)\n",
            "    type_as_1: \"f32[s31, 2, s12]\" = torch.ops.aten.type_as.default(randn, mul_82);  randn = None\n",
            "    mul_83: \"f32[s31, 2, s12]\" = torch.ops.aten.mul.Tensor(type_as_1, select_2);  type_as_1 = select_2 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:386 in forward, code: x = torch.flip(x, [1])\n",
            "    flip: \"f32[s31, 2, s12]\" = torch.ops.aten.flip.default(mul_83, [1]);  mul_83 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:497 in forward, code: x0, x1 = torch.split(x, [self.half_channels] * 2, 1)\n",
            "    split_with_sizes = torch.ops.aten.split_with_sizes.default(flip, [1, 1], 1);  flip = None\n",
            "    getitem_2: \"f32[s31, 1, s12]\" = split_with_sizes[0]\n",
            "    getitem_3: \"f32[s31, 1, s12]\" = split_with_sizes[1];  split_with_sizes = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_45: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(getitem_2, arg454_1, arg455_1);  getitem_2 = arg454_1 = arg455_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:119 in forward, code: x = x + g\n",
            "    add_51: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(conv1d_45, mul_82);  conv1d_45 = mul_82 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_84: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_51, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_46: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_84, arg456_1, arg457_1, [1], [1], [1], 192);  mul_84 = arg456_1 = arg457_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_73: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_46, 1, -1);  conv1d_46 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_18: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_73, [192], arg468_1, arg469_1);  transpose_73 = arg468_1 = arg469_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_74: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_18, 1, -1);  layer_norm_18 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_6: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_74);  transpose_74 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_47: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_6, arg462_1, arg463_1);  gelu_6 = arg462_1 = arg463_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_75: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_47, 1, -1);  conv1d_47 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_19: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_75, [192], arg474_1, arg475_1);  transpose_75 = arg474_1 = arg475_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_76: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_19, 1, -1);  layer_norm_19 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_7: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_76);  transpose_76 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_27: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_7, 0.0, False);  gelu_7 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_52: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_51, dropout_27);  add_51 = dropout_27 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_85: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_52, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_48: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_85, arg458_1, arg459_1, [1], [3], [3], 192);  mul_85 = arg458_1 = arg459_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_77: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_48, 1, -1);  conv1d_48 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_20: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_77, [192], arg470_1, arg471_1);  transpose_77 = arg470_1 = arg471_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_78: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_20, 1, -1);  layer_norm_20 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_8: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_78);  transpose_78 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_49: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_8, arg464_1, arg465_1);  gelu_8 = arg464_1 = arg465_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_79: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_49, 1, -1);  conv1d_49 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_21: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_79, [192], arg476_1, arg477_1);  transpose_79 = arg476_1 = arg477_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_80: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_21, 1, -1);  layer_norm_21 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_9: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_80);  transpose_80 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_28: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_9, 0.0, False);  gelu_9 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_53: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_52, dropout_28);  add_52 = dropout_28 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:121 in forward, code: y = self.convs_sep[i](x * x_mask)\n",
            "    mul_86: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_53, type_as)\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_50: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(mul_86, arg460_1, arg461_1, [1], [9], [9], 192);  mul_86 = arg460_1 = arg461_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_81: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_50, 1, -1);  conv1d_50 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_22: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_81, [192], arg472_1, arg473_1);  transpose_81 = arg472_1 = arg473_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_82: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_22, 1, -1);  layer_norm_22 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:123 in forward, code: y = F.gelu(y)\n",
            "    gelu_10: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_82);  transpose_82 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_51: \"f32[s31, 192, s12]\" = torch.ops.aten.conv1d.default(gelu_10, arg466_1, arg467_1);  gelu_10 = arg466_1 = arg467_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:24 in forward, code: x = x.transpose(1, -1)\n",
            "    transpose_83: \"f32[s31, s12, 192]\" = torch.ops.aten.transpose.int(conv1d_51, 1, -1);  conv1d_51 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:25 in forward, code: x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
            "    layer_norm_23: \"f32[s31, s12, 192]\" = torch.ops.aten.layer_norm.default(transpose_83, [192], arg478_1, arg479_1);  transpose_83 = arg478_1 = arg479_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:26 in forward, code: return x.transpose(1, -1)\n",
            "    transpose_84: \"f32[s31, 192, s12]\" = torch.ops.aten.transpose.int(layer_norm_23, 1, -1);  layer_norm_23 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:126 in forward, code: y = F.gelu(y)\n",
            "    gelu_11: \"f32[s31, 192, s12]\" = torch.ops.aten.gelu.default(transpose_84);  transpose_84 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
            "    dropout_29: \"f32[s31, 192, s12]\" = torch.ops.aten.dropout.default(gelu_11, 0.0, False);  gelu_11 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:128 in forward, code: x = x + y\n",
            "    add_54: \"f32[s31, 192, s12]\" = torch.ops.aten.add.Tensor(add_53, dropout_29);  add_53 = dropout_29 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:129 in forward, code: return x * x_mask\n",
            "    mul_87: \"f32[s31, 192, s12]\" = torch.ops.aten.mul.Tensor(add_54, type_as);  add_54 = None\n",
            "    \n",
            "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
            "    conv1d_52: \"f32[s31, 29, s12]\" = torch.ops.aten.conv1d.default(mul_87, arg480_1, arg481_1);  mul_87 = arg480_1 = arg481_1 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:500 in forward, code: h = self.proj(h) * x_mask\n",
            "    mul_88: \"f32[s31, 29, s12]\" = torch.ops.aten.mul.Tensor(conv1d_52, type_as);  conv1d_52 = type_as = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:503 in forward, code: h = h.reshape(b, c, -1, t).permute(0, 1, 3, 2)  # [b, cx?, t] -> [b, c, t, ?]\n",
            "    reshape: \"f32[s31, 1, 29, s12]\" = torch.ops.aten.reshape.default(mul_88, [sym_size_int_1, 1, -1, sym_size_int_16]);  mul_88 = sym_size_int_1 = sym_size_int_16 = None\n",
            "    permute: \"f32[s31, 1, s12, 29]\" = torch.ops.aten.permute.default(reshape, [0, 1, 3, 2]);  reshape = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:505 in forward, code: unnormalized_widths = h[..., : self.num_bins] / math.sqrt(self.filter_channels)\n",
            "    slice_43: \"f32[s31, 1, s12, 10]\" = torch.ops.aten.slice.Tensor(permute, 3, 0, 10)\n",
            "    div_12: \"f32[s31, 1, s12, 10]\" = torch.ops.aten.div.Tensor(slice_43, 13.856406460551018);  slice_43 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:506 in forward, code: unnormalized_heights = h[..., self.num_bins : 2 * self.num_bins] / math.sqrt(\n",
            "    slice_44: \"f32[s31, 1, s12, 10]\" = torch.ops.aten.slice.Tensor(permute, 3, 10, 20)\n",
            "    div_13: \"f32[s31, 1, s12, 10]\" = torch.ops.aten.div.Tensor(slice_44, 13.856406460551018);  slice_44 = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:509 in forward, code: unnormalized_derivatives = h[..., 2 * self.num_bins :]\n",
            "    slice_45: \"f32[s31, 1, s12, 9]\" = torch.ops.aten.slice.Tensor(permute, 3, 20, 9223372036854775807);  permute = None\n",
            "    \n",
            "     # File: /content/piper1-gpl/src/piper/train/vits/modules.py:511 in forward, code: x1, logabsdet = piecewise_rational_quadratic_transform(\n",
            "    ge: \"b8[s31, 1, s12]\" = torch.ops.aten.ge.Scalar(getitem_3, -5.0)\n",
            "    le_12: \"b8[s31, 1, s12]\" = torch.ops.aten.le.Scalar(getitem_3, 5.0)\n",
            "    and_1: \"b8[s31, 1, s12]\" = torch.ops.aten.__and__.Tensor(ge, le_12);  ge = le_12 = None\n",
            "    bitwise_not: \"b8[s31, 1, s12]\" = torch.ops.aten.bitwise_not.default(and_1)\n",
            "    zeros_like: \"f32[s31, 1, s12]\" = torch.ops.aten.zeros_like.default(getitem_3, pin_memory = False)\n",
            "    zeros_like_1: \"f32[s31, 1, s12]\" = torch.ops.aten.zeros_like.default(getitem_3, pin_memory = False)\n",
            "    pad_48: \"f32[s31, 1, s12, 11]\" = torch.ops.aten.pad.default(slice_45, [1, 1]);  slice_45 = None\n",
            "    _tensor_constant0: \"f32[]\" = self._tensor_constant0\n",
            "    lift_fresh_copy: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None\n",
            "    select_3: \"f32[s31, 1, s12]\" = torch.ops.aten.select.int(pad_48, 3, 0)\n",
            "    fill_: \"f32[s31, 1, s12]\" = torch.ops.aten.fill_.Tensor(select_3, lift_fresh_copy);  select_3 = lift_fresh_copy = fill_ = None\n",
            "    _tensor_constant1: \"f32[]\" = self._tensor_constant1\n",
            "    lift_fresh_copy_1: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None\n",
            "    select_4: \"f32[s31, 1, s12]\" = torch.ops.aten.select.int(pad_48, 3, 10)\n",
            "    fill__1: \"f32[s31, 1, s12]\" = torch.ops.aten.fill_.Tensor(select_4, lift_fresh_copy_1);  select_4 = lift_fresh_copy_1 = fill__1 = None\n",
            "    index: \"f32[u0]\" = torch.ops.aten.index.Tensor(getitem_3, [bitwise_not])\n",
            "    sym_size_int_17: \"Sym(u0)\" = torch.ops.aten.sym_size.int(index, 0)\n",
            "    eq_32: \"Sym(True)\" = sym_size_int_17 == sym_size_int_17;  sym_size_int_17 = eq_32 = None\n",
            "    index_put_: \"f32[s31, 1, s12]\" = torch.ops.aten.index_put_.default(zeros_like, [bitwise_not], index);  zeros_like = index = index_put_ = None\n",
            "    _tensor_constant2: \"f32[]\" = self._tensor_constant2\n",
            "    lift_fresh_copy_2: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant2);  _tensor_constant2 = None\n",
            "    index_put__1: \"f32[s31, 1, s12]\" = torch.ops.aten.index_put_.default(zeros_like_1, [bitwise_not], lift_fresh_copy_2);  zeros_like_1 = bitwise_not = lift_fresh_copy_2 = index_put__1 = None\n",
            "    index_1: \"f32[u1]\" = torch.ops.aten.index.Tensor(getitem_3, [and_1]);  getitem_3 = None\n",
            "    index_2: \"f32[u1, 10]\" = torch.ops.aten.index.Tensor(div_12, [and_1]);  div_12 = None\n",
            "    index_3: \"f32[u1, 10]\" = torch.ops.aten.index.Tensor(div_13, [and_1]);  div_13 = None\n",
            "    index_4: \"f32[u1, 11]\" = torch.ops.aten.index.Tensor(pad_48, [and_1]);  pad_48 = and_1 = None\n",
            "    softmax_6: \"f32[u1, 10]\" = torch.ops.aten.softmax.int(index_2, -1);  index_2 = None\n",
            "    mul_89: \"f32[u1, 10]\" = torch.ops.aten.mul.Tensor(softmax_6, 0.99);  softmax_6 = None\n",
            "    add_55: \"f32[u1, 10]\" = torch.ops.aten.add.Tensor(mul_89, 0.001);  mul_89 = None\n",
            "    cumsum: \"f32[u1, 10]\" = torch.ops.aten.cumsum.default(add_55, -1);  add_55 = None\n",
            "    pad_49: \"f32[u1, 11]\" = torch.ops.aten.pad.default(cumsum, [1, 0], 'constant', 0.0);  cumsum = None\n",
            "    mul_90: \"f32[u1, 11]\" = torch.ops.aten.mul.Tensor(pad_49, 10.0);  pad_49 = None\n",
            "    add_56: \"f32[u1, 11]\" = torch.ops.aten.add.Tensor(mul_90, -5.0);  mul_90 = None\n",
            "    _tensor_constant3: \"f32[]\" = self._tensor_constant3\n",
            "    lift_fresh_copy_3: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant3);  _tensor_constant3 = None\n",
            "    select_5: \"f32[u1]\" = torch.ops.aten.select.int(add_56, 1, 0)\n",
            "    fill__2: \"f32[u1]\" = torch.ops.aten.fill_.Tensor(select_5, lift_fresh_copy_3);  select_5 = lift_fresh_copy_3 = fill__2 = None\n",
            "    _tensor_constant4: \"f32[]\" = self._tensor_constant4\n",
            "    lift_fresh_copy_4: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant4);  _tensor_constant4 = None\n",
            "    select_6: \"f32[u1]\" = torch.ops.aten.select.int(add_56, 1, 10)\n",
            "    fill__3: \"f32[u1]\" = torch.ops.aten.fill_.Tensor(select_6, lift_fresh_copy_4);  select_6 = lift_fresh_copy_4 = fill__3 = None\n",
            "    slice_46: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_56, 1, 1, 9223372036854775807)\n",
            "    slice_47: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_56, 1, 0, -1)\n",
            "    sub_66: \"f32[u1, 10]\" = torch.ops.aten.sub.Tensor(slice_46, slice_47);  slice_46 = slice_47 = None\n",
            "    softplus: \"f32[u1, 11]\" = torch.ops.aten.softplus.default(index_4);  index_4 = None\n",
            "    add_57: \"f32[u1, 11]\" = torch.ops.aten.add.Tensor(softplus, 0.001);  softplus = None\n",
            "    softmax_7: \"f32[u1, 10]\" = torch.ops.aten.softmax.int(index_3, -1);  index_3 = None\n",
            "    mul_91: \"f32[u1, 10]\" = torch.ops.aten.mul.Tensor(softmax_7, 0.99);  softmax_7 = None\n",
            "    add_58: \"f32[u1, 10]\" = torch.ops.aten.add.Tensor(mul_91, 0.001);  mul_91 = None\n",
            "    cumsum_1: \"f32[u1, 10]\" = torch.ops.aten.cumsum.default(add_58, -1);  add_58 = None\n",
            "    pad_50: \"f32[u1, 11]\" = torch.ops.aten.pad.default(cumsum_1, [1, 0], 'constant', 0.0);  cumsum_1 = None\n",
            "    mul_92: \"f32[u1, 11]\" = torch.ops.aten.mul.Tensor(pad_50, 10.0);  pad_50 = None\n",
            "    add_59: \"f32[u1, 11]\" = torch.ops.aten.add.Tensor(mul_92, -5.0);  mul_92 = None\n",
            "    _tensor_constant5: \"f32[]\" = self._tensor_constant5\n",
            "    lift_fresh_copy_5: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant5);  _tensor_constant5 = None\n",
            "    select_7: \"f32[u1]\" = torch.ops.aten.select.int(add_59, 1, 0)\n",
            "    fill__4: \"f32[u1]\" = torch.ops.aten.fill_.Tensor(select_7, lift_fresh_copy_5);  select_7 = lift_fresh_copy_5 = fill__4 = None\n",
            "    _tensor_constant6: \"f32[]\" = self._tensor_constant6\n",
            "    lift_fresh_copy_6: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant6);  _tensor_constant6 = None\n",
            "    select_8: \"f32[u1]\" = torch.ops.aten.select.int(add_59, 1, 10)\n",
            "    fill__5: \"f32[u1]\" = torch.ops.aten.fill_.Tensor(select_8, lift_fresh_copy_6);  select_8 = lift_fresh_copy_6 = fill__5 = None\n",
            "    slice_48: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_59, 1, 1, 9223372036854775807)\n",
            "    slice_49: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_59, 1, 0, -1)\n",
            "    sub_67: \"f32[u1, 10]\" = torch.ops.aten.sub.Tensor(slice_48, slice_49);  slice_48 = slice_49 = None\n",
            "    select_9: \"f32[u1]\" = torch.ops.aten.select.int(add_59, 1, 10)\n",
            "    add_: \"f32[u1]\" = torch.ops.aten.add_.Tensor(select_9, 1e-06);  select_9 = None\n",
            "    select_10: \"f32[u1]\" = torch.ops.aten.select.int(add_59, 1, 10)\n",
            "    sym_size_int_18: \"Sym(u1)\" = torch.ops.aten.sym_size.int(index_1, 0)\n",
            "    eq_33: \"Sym(True)\" = sym_size_int_18 == sym_size_int_18;  sym_size_int_18 = eq_33 = None\n",
            "    copy_: \"f32[u1]\" = torch.ops.aten.copy_.default(select_10, add_);  select_10 = add_ = copy_ = None\n",
            "    unsqueeze_17: \"f32[u1, 1]\" = torch.ops.aten.unsqueeze.default(index_1, 1)\n",
            "    ge_1: \"b8[u1, 11]\" = torch.ops.aten.ge.Tensor(unsqueeze_17, add_59);  unsqueeze_17 = None\n",
            "    sum_1: \"i64[u1]\" = torch.ops.aten.sum.dim_IntList(ge_1, [-1]);  ge_1 = None\n",
            "    sub_68: \"i64[u1]\" = torch.ops.aten.sub.Tensor(sum_1, 1);  sum_1 = None\n",
            "    unsqueeze_18: \"i64[u1, 1]\" = torch.ops.aten.unsqueeze.default(sub_68, 1);  sub_68 = None\n",
            "    gather: \"f32[u1, 1]\" = torch.ops.aten.gather.default(add_56, -1, unsqueeze_18);  add_56 = None\n",
            "    select_11: \"f32[u1]\" = torch.ops.aten.select.int(gather, 1, 0);  gather = select_11 = None\n",
            "    gather_1: \"f32[u1, 1]\" = torch.ops.aten.gather.default(sub_66, -1, unsqueeze_18)\n",
            "    select_12: \"f32[u1]\" = torch.ops.aten.select.int(gather_1, 1, 0);  gather_1 = select_12 = None\n",
            "    gather_2: \"f32[u1, 1]\" = torch.ops.aten.gather.default(add_59, -1, unsqueeze_18);  add_59 = None\n",
            "    select_13: \"f32[u1]\" = torch.ops.aten.select.int(gather_2, 1, 0);  gather_2 = None\n",
            "    div_14: \"f32[u1, 10]\" = torch.ops.aten.div.Tensor(sub_67, sub_66);  sub_66 = None\n",
            "    gather_3: \"f32[u1, 1]\" = torch.ops.aten.gather.default(div_14, -1, unsqueeze_18);  div_14 = None\n",
            "    select_14: \"f32[u1]\" = torch.ops.aten.select.int(gather_3, 1, 0);  gather_3 = None\n",
            "    gather_4: \"f32[u1, 1]\" = torch.ops.aten.gather.default(add_57, -1, unsqueeze_18)\n",
            "    select_15: \"f32[u1]\" = torch.ops.aten.select.int(gather_4, 1, 0);  gather_4 = None\n",
            "    slice_50: \"f32[u1, 10]\" = torch.ops.aten.slice.Tensor(add_57, 1, 1, 9223372036854775807);  add_57 = None\n",
            "    gather_5: \"f32[u1, 1]\" = torch.ops.aten.gather.default(slice_50, -1, unsqueeze_18);  slice_50 = None\n",
            "    select_16: \"f32[u1]\" = torch.ops.aten.select.int(gather_5, 1, 0);  gather_5 = None\n",
            "    gather_6: \"f32[u1, 1]\" = torch.ops.aten.gather.default(sub_67, -1, unsqueeze_18);  sub_67 = unsqueeze_18 = None\n",
            "    select_17: \"f32[u1]\" = torch.ops.aten.select.int(gather_6, 1, 0);  gather_6 = None\n",
            "    sub_69: \"f32[u1]\" = torch.ops.aten.sub.Tensor(index_1, select_13)\n",
            "    add_60: \"f32[u1]\" = torch.ops.aten.add.Tensor(select_15, select_16)\n",
            "    mul_93: \"f32[u1]\" = torch.ops.aten.mul.Tensor(select_14, 2)\n",
            "    sub_70: \"f32[u1]\" = torch.ops.aten.sub.Tensor(add_60, mul_93);  add_60 = mul_93 = None\n",
            "    mul_94: \"f32[u1]\" = torch.ops.aten.mul.Tensor(sub_69, sub_70);  sub_69 = sub_70 = None\n",
            "    sub_71: \"f32[u1]\" = torch.ops.aten.sub.Tensor(select_14, select_15)\n",
            "    mul_95: \"f32[u1]\" = torch.ops.aten.mul.Tensor(select_17, sub_71);  sub_71 = None\n",
            "    add_61: \"f32[u1]\" = torch.ops.aten.add.Tensor(mul_94, mul_95);  mul_94 = mul_95 = None\n",
            "    mul_96: \"f32[u1]\" = torch.ops.aten.mul.Tensor(select_17, select_15);  select_17 = None\n",
            "    sub_72: \"f32[u1]\" = torch.ops.aten.sub.Tensor(index_1, select_13)\n",
            "    add_62: \"f32[u1]\" = torch.ops.aten.add.Tensor(select_15, select_16);  select_15 = select_16 = None\n",
            "    mul_97: \"f32[u1]\" = torch.ops.aten.mul.Tensor(select_14, 2)\n",
            "    sub_73: \"f32[u1]\" = torch.ops.aten.sub.Tensor(add_62, mul_97);  add_62 = mul_97 = None\n",
            "    mul_98: \"f32[u1]\" = torch.ops.aten.mul.Tensor(sub_72, sub_73);  sub_72 = sub_73 = None\n",
            "    sub_74: \"f32[u1]\" = torch.ops.aten.sub.Tensor(mul_96, mul_98);  mul_96 = mul_98 = None\n",
            "    neg: \"f32[u1]\" = torch.ops.aten.neg.default(select_14);  select_14 = None\n",
            "    sub_75: \"f32[u1]\" = torch.ops.aten.sub.Tensor(index_1, select_13);  index_1 = select_13 = None\n",
            "    mul_99: \"f32[u1]\" = torch.ops.aten.mul.Tensor(neg, sub_75);  neg = sub_75 = None\n",
            "    pow_1: \"f32[u1]\" = torch.ops.aten.pow.Tensor_Scalar(sub_74, 2);  sub_74 = None\n",
            "    mul_100: \"f32[u1]\" = torch.ops.aten.mul.Tensor(add_61, 4);  add_61 = None\n",
            "    mul_101: \"f32[u1]\" = torch.ops.aten.mul.Tensor(mul_100, mul_99);  mul_100 = mul_99 = None\n",
            "    sub_76: \"f32[u1]\" = torch.ops.aten.sub.Tensor(pow_1, mul_101);  pow_1 = mul_101 = None\n",
            "    ge_2: \"b8[u1]\" = torch.ops.aten.ge.Scalar(sub_76, 0);  sub_76 = None\n",
            "    all_1: \"b8[]\" = torch.ops.aten.all.default(ge_2);  ge_2 = None\n",
            "    ne: \"b8[]\" = torch.ops.aten.ne.Scalar(all_1, 0);  all_1 = None\n",
            "    item: \"Sym(Eq(u2, 1))\" = torch.ops.aten.item.default(ne);  ne = item = None\n",
            "    \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/exporter/_capture_strategies.py\", line 118, in __call__\n",
            "    exported_program = self._capture(model, args, kwargs, dynamic_shapes)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/exporter/_capture_strategies.py\", line 210, in _capture\n",
            "    return torch.export.export(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/__init__.py\", line 311, in export\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/__init__.py\", line 277, in export\n",
            "    return _export(\n",
            "           ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\", line 1163, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\", line 1129, in wrapper\n",
            "    ep = fn(*args, **kwargs)\n",
            "         ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/exported_program.py\", line 124, in wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\", line 2255, in _export\n",
            "    ep = _export_for_training(\n",
            "         ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\", line 1163, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\", line 1129, in wrapper\n",
            "    ep = fn(*args, **kwargs)\n",
            "         ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/exported_program.py\", line 124, in wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\", line 2071, in _export_for_training\n",
            "    export_artifact = export_func(\n",
            "                      ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\", line 2002, in _non_strict_export\n",
            "    aten_export_artifact = _to_aten_func(  # type: ignore[operator]\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\", line 1793, in _export_to_aten_ir_make_fx\n",
            "    gm, graph_signature = transform(_make_fx_helper)(\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\", line 1922, in _aot_export_non_strict\n",
            "    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\", line 1706, in _make_fx_helper\n",
            "    gm = make_fx(\n",
            "         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 2429, in wrapped\n",
            "    return make_fx_tracer.trace(f, *args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 2356, in trace\n",
            "    return self._trace_inner(f, *args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 2318, in _trace_inner\n",
            "    t = dispatch_trace(\n",
            "        ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_compile.py\", line 53, in inner\n",
            "    return disable_fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1303, in dispatch_trace\n",
            "    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1908, in trace\n",
            "    res = super().trace(root, concrete_args)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/_symbolic_trace.py\", line 868, in trace\n",
            "    (self.create_arg(fn(*args)),),\n",
            "                     ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1361, in wrapped\n",
            "    out = f(*tensors)  # type:ignore[call-arg]\n",
            "          ^^^^^^^^^^^\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\", line 1593, in wrapped_fn\n",
            "    return tuple(flat_fn(*args))\n",
            "                 ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py\", line 187, in flat_fn\n",
            "    tree_out = fn(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/graph_capture_wrappers.py\", line 1354, in functional_call\n",
            "    out = mod(*args[params_len:], **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/_symbolic_trace.py\", line 843, in module_call_wrapper\n",
            "    return self.call_module(mod, forward, args, kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1997, in call_module\n",
            "    return Tracer.call_module(self, m, forward, args, kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/_symbolic_trace.py\", line 560, in call_module\n",
            "    ret_val = forward(*args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/_symbolic_trace.py\", line 836, in forward\n",
            "    return _orig_module_call(mod, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\", line 1906, in forward\n",
            "    tree_out = mod(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/_symbolic_trace.py\", line 843, in module_call_wrapper\n",
            "    return self.call_module(mod, forward, args, kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1997, in call_module\n",
            "    return Tracer.call_module(self, m, forward, args, kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/_symbolic_trace.py\", line 560, in call_module\n",
            "    ret_val = forward(*args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/_symbolic_trace.py\", line 836, in forward\n",
            "    return _orig_module_call(mod, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/piper1-gpl/src/piper/train/export_onnx.py\", line 61, in infer_forward\n",
            "    audio = model_g.infer(\n",
            "            ^^^^^^^^^^^^^^\n",
            "  File \"/content/piper1-gpl/src/piper/train/vits/models.py\", line 701, in infer\n",
            "    logw = self.dp(x, x_mask, g=g, reverse=True, noise_scale=noise_scale_w)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/_symbolic_trace.py\", line 843, in module_call_wrapper\n",
            "    return self.call_module(mod, forward, args, kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1997, in call_module\n",
            "    return Tracer.call_module(self, m, forward, args, kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/_symbolic_trace.py\", line 560, in call_module\n",
            "    ret_val = forward(*args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/_symbolic_trace.py\", line 836, in forward\n",
            "    return _orig_module_call(mod, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/piper1-gpl/src/piper/train/vits/models.py\", line 114, in forward\n",
            "    z = flow(z, x_mask, g=x, reverse=reverse)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/_symbolic_trace.py\", line 843, in module_call_wrapper\n",
            "    return self.call_module(mod, forward, args, kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1997, in call_module\n",
            "    return Tracer.call_module(self, m, forward, args, kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/_symbolic_trace.py\", line 560, in call_module\n",
            "    ret_val = forward(*args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/_symbolic_trace.py\", line 836, in forward\n",
            "    return _orig_module_call(mod, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/piper1-gpl/src/piper/train/vits/modules.py\", line 511, in forward\n",
            "    x1, logabsdet = piecewise_rational_quadratic_transform(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/piper1-gpl/src/piper/train/vits/transforms.py\", line 30, in piecewise_rational_quadratic_transform\n",
            "    outputs, logabsdet = spline_fn(\n",
            "                         ^^^^^^^^^^\n",
            "  File \"/content/piper1-gpl/src/piper/train/vits/transforms.py\", line 83, in unconstrained_rational_quadratic_spline\n",
            "    ) = rational_quadratic_spline(\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/piper1-gpl/src/piper/train/vits/transforms.py\", line 174, in rational_quadratic_spline\n",
            "    assert (discriminant >= 0).all(), discriminant\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1409, in __torch_function__\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/proxy_tensor.py\", line 1479, in __torch_function__\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_export/non_strict_utils.py\", line 1066, in __torch_function__\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/sym_node.py\", line 538, in guard_bool\n",
            "    r = self.evaluate()\n",
            "        ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/sym_node.py\", line 512, in evaluate\n",
            "    return self.shape_env.evaluate_sym_node(self, size_oblivious)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 7233, in evaluate_sym_node\n",
            "    return self.evaluate_expr(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 7333, in evaluate_expr\n",
            "    return self._inner_evaluate_expr(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/recording.py\", line 272, in wrapper\n",
            "    return retlog(fn(*args, **kwargs))\n",
            "                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 7356, in _inner_evaluate_expr\n",
            "    return self._evaluate_expr(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 7574, in _evaluate_expr\n",
            "    raise self._make_data_dependent_error(\n",
            "torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression Eq(u2, 1) (unhinted: Eq(u2, 1)).  (Size-like symbols: none)\n",
            "\n",
            "consider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (_export/non_strict_utils.py:1066 in __torch_function__)\n",
            "For more information, run with TORCH_LOGS=\"dynamic\"\n",
            "For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u2\"\n",
            "If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n",
            "For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n",
            "\n",
            "For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n",
            "\n",
            "The following call raised this error:\n",
            "  File \"/content/piper1-gpl/src/piper/train/vits/transforms.py\", line 174, in rational_quadratic_spline\n",
            "    assert (discriminant >= 0).all(), discriminant\n",
            "\n",
            "\n",
            "The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/piper1-gpl/src/piper/train/export_onnx.py\", line 112, in <module>\n",
            "    main()\n",
            "  File \"/content/piper1-gpl/src/piper/train/export_onnx.py\", line 92, in main\n",
            "    torch.onnx.export(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/onnx/__init__.py\", line 296, in export\n",
            "    return _compat.export_compat(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/exporter/_compat.py\", line 143, in export_compat\n",
            "    onnx_program = _core.export(\n",
            "                   ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/exporter/_flags.py\", line 23, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/exporter/_core.py\", line 1385, in export\n",
            "    raise _errors.TorchExportError(\n",
            "torch.onnx._internal.exporter._errors.TorchExportError: Failed to export the model with torch.export. \u001b[96mThis is step 1/3\u001b[0m of exporting the model to ONNX. Next steps:\n",
            "- Modify the model code for `torch.export.export` to succeed. Refer to https://pytorch.org/docs/stable/generated/exportdb/index.html for more information.\n",
            "- Debug `torch.export.export` and submit a PR to PyTorch.\n",
            "- Create an issue in the PyTorch GitHub repository against the \u001b[96m*torch.export*\u001b[0m component and attach the full error stack as well as reproduction scripts.\n",
            "\n",
            "## Exception summary\n",
            "\n",
            "<class 'torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode'>: Could not guard on data-dependent expression Eq(u2, 1) (unhinted: Eq(u2, 1)).  (Size-like symbols: none)\n",
            "\n",
            "consider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (_export/non_strict_utils.py:1066 in __torch_function__)\n",
            "For more information, run with TORCH_LOGS=\"dynamic\"\n",
            "For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u2\"\n",
            "If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n",
            "For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n",
            "\n",
            "For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n",
            "\n",
            "The following call raised this error:\n",
            "  File \"/content/piper1-gpl/src/piper/train/vits/transforms.py\", line 174, in rational_quadratic_spline\n",
            "    assert (discriminant >= 0).all(), discriminant\n",
            "\n",
            "\n",
            "The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.\n",
            "\n",
            "(Refer to the full stack trace above for more information.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/tts_data/myvoice/my_colab_voice.json /content/model.onnx.json"
      ],
      "metadata": {
        "id": "pOqvuxqM8UrN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}